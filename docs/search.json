[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 378: Linear Regression Analysis",
    "section": "",
    "text": "Preface\n\nI never felt such a glow of loyalty and respect towards the sovereignty and magnificent sway of mathematical analysis as when his answer reached me confirming, by purely mathematical reasoning, my various and laborious statistical conclusions.\n\nRegression towards Mediocrity in Hereditary Stature\nSir Francis Galton, FRS (1886)\nThis collection of lecture notes is an updated version of my original lecture ntoes from 2017. They are now typeset in Quarto thanks to the suggestion of my former PhD student, Dr. Katie L Burak, who is currently an assistant professor of teaching at the University of British Columbia.\nThese revised and enhanced notes have some nice additions. Most notably, there is embedded R code and datasets within the text, which is (perhaps) the main point of using Quarto over my old LaTex notes. I have also taken it upon myself to add new sections and expand on those I find most interesting. This mostly occurs at the end of the notes with additional sections on advanced topics like logisitic regression, LASSO, and some Bayesian regression models.\nThe material in these notes is now too much for a single semester course in linear regression. Of note, I personally plan to skip the sections on influential points in regression models as the formulae are both tedious and ad-hoc. Nevertheless, I did copy these bits into the new version of my notes for completeness sake.\nAdam B Kashlak\nEdmonton, Canada\nAugust 2025\nThe following are lecture notes originally produced for an upper level undergraduate course on linear regression at the University of Alberta in the fall of 2017. Regression is one of the main, if not the primary, workhorses of statistical inference. Hence, I do hope you will find these notes useful in learning about regression.\nThe goal is to begin with the standard development of ordinary least squares in the multiple regression setting, then to move onto a discussion of model assumptions and issues that can arise in practice, and finally to discuss some specific instances of generalized linear models (GLMs) without delving into GLMs in full generality. Of course, what follows is by no means a unique exposition but is mostly derived from three main sources: the text, Linear Regression Analysis, by Montgomery, Peck, and Vining; the course notes of Dr. Linglong Kong who lectured this same course in 2013; whatever remains inside my brain from supervising (TAing) undergraduate statistics courses at the University of Cambridge during my PhD years.\nAdam B Kashlak\nEdmonton, Canada\nAugust 2017",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chpt1_ols.html",
    "href": "chpt1_ols.html",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "1.1 Introduction\nLinear regression begins with the simple but profound idea that some observed output or {response} variable, \\(Y\\in\\mathbb{R}\\), is a function of \\(p\\) input or regressor variables \\(x_1,\\ldots,x_p\\) with the addition of some unknown noise variable \\(\\varepsilon\\). Namely, \\[\n  Y = f(x_1,\\ldots,x_p) + \\varepsilon\n\\] where the noise is generally assumed to have mean zero and finite variance. The function \\(f\\) is unknown and relates the inputs \\(x_i\\) to the output \\(Y\\). Our goal is to ascertain what \\(f\\) could be.\nIn this setting, \\(Y\\) is usually considered to be a random variable while the \\(x_i\\) are considered fixed. Hence, the expected value of \\(Y\\) is in terms of the unknown function \\(f\\) and the regressors: \\[\n  \\mathrm{E}\\left(Y\\middle|x_1,\\ldots,x_p\\right) = f(x_1,\\ldots,x_p).\n\\] While \\(f\\) can be considered to be in some very general classes of functions, we begin with the standard linear setting. Let \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\in\\mathbb{R}\\). Then, the multiple regression model is \\[\n  Y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p\n    = {\\beta}^\\mathrm{T} X\n\\] where \\(\\beta = {(\\beta_0,\\ldots,\\beta_p)}^\\mathrm{T}\\) and \\(X = {(1,x_1,\\ldots,x_p)}^\\mathrm{T}\\). The simple regression model is a submodel of the above where \\(p=1\\), which is \\[\n  Y = \\beta_0 + \\beta_1 x_1 + \\varepsilon,\n\\] and will be treated concurrently with multiple regression.\nIn the statistics setting, the parameter vector \\(\\beta\\in\\mathbb{R}^p\\) is unknown. The analyst observes multiple replications of regressor and response pairs, \\((X_1,Y_1),\\ldots,(X_n,Y_n)\\) where \\(n\\) is the sample size, and wishes to choose a ``best’’ estimate for \\(\\beta\\) based on these \\(n\\) observations. This setup can be concisely written in a vector-matrix form as \\[\n  Y = X\\beta + \\varepsilon\n\\tag{1.1}\\] where \\[\n  Y =\n  \\begin{pmatrix}\n    Y_1 \\\\ \\vdots \\\\ Y_n\n  \\end{pmatrix},~~~~\n  X =\n  \\begin{pmatrix}\n    1 & x_{1,1} & \\ldots & x_{1,p} \\\\\n    1 & x_{2,1} & \\ldots & x_{2,p} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{n,1} & \\ldots & x_{n,p}\n  \\end{pmatrix},~~~~\n  \\beta =\n  \\begin{pmatrix}\n    \\beta_0 \\\\ \\vdots \\\\ \\beta_p\n  \\end{pmatrix},~~~~\n  \\varepsilon=\n  \\begin{pmatrix}\n    \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n\n  \\end{pmatrix}.\n\\] Note that \\(Y,\\varepsilon\\in\\mathbb{R}^n\\), \\(\\beta\\in\\mathbb{R}^{p+1}\\), and \\(X\\in\\mathbb{R}^{n\\times {p+1}}\\).\nAs \\(Y\\) is a random variable, we can compute its mean vector and covariance matrix as follows: \\[\n  \\mathrm{E}Y = \\mathrm{E}\\left( X\\beta +\\varepsilon\\right) = X\\beta\n\\] and \\[\n  \\mathrm{Var}\\left(Y\\right)\n  = \\mathrm{E}\\left( (Y-X\\beta){(Y-X\\beta)}^\\mathrm{T} \\right)\n  = \\mathrm{E}\\left( \\varepsilon{\\varepsilon}^\\mathrm{T} \\right)\n  = \\mathrm{Var}\\left( \\varepsilon\\right)\n  = \\sigma^2I_n.\n\\]\nAn example of a linear regression is this following study from the New England Journal of Medicine1 can be found in the code below. This study highlights the correlation between chocolate consumption and Nobel prizes received in 16 different countries.\n# Read in Table\ndat = read.table(\"data/chocoTable.r\");\n\n# Plot data\nplot( \n  dat$choco, dat$nobel,\n  xlab=\"Chocolate Consumption (kg per capita)\",\n  ylab=\"Nobel prizes per 10 million\",\n  las=1,#xlim=c(0,max(dat$choco)+1),\n  ylim=c(-1,max(dat$nobel))\n);\n\n# Label data\ntext(\n  x=dat$choco, y=dat$nobel,\n  labels=dat$abbrev, \n  pos=1,cex=0.7\n)\n\n# Construct a linear model\nlmDat = lm( nobel ~ choco, data = dat );\n\n# plot regression line\nabline(lmDat,col='blue',lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt1_ols.html#introduction",
    "href": "chpt1_ols.html#introduction",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "1.1.1 Definitions\nBefore continuing, we require the following collection of terminology.\nThe response \\(Y\\) and the regressors \\(X\\) were already introduced above. These elements comprise the observed data in our regression. The noise or error variable is \\(\\varepsilon\\). The entries in this vector are usually considered to be independent and identically distributed (iid) random variables with mean zero and finite variance \\(\\sigma^2 &lt; \\infty\\). Very often, this vector will be assumed to have a multivariate normal distribution: \\(\\varepsilon\\sim\\mathcal{N}\\left({ 0},\\sigma^2 I_n\\right)\\) where \\(I_n\\) is the \\(n\\times n\\) identity matrix. The variance \\(\\sigma^2\\) is also generally considered to be unknown to the analyst.\nThe unknown vector \\(\\beta\\) is our parameter vector. Eventually, we will construct an estimator \\(\\hat{\\beta}\\) from the observed data. Given such an estimator, the fitted values are \\(\\hat{Y} := X\\hat{\\beta}\\). These values are what the model believes are the expected values at each regressor.\nGiven the fitted values, the residuals are \\(r = Y-\\hat{Y}\\) which is a vector with entries \\(r_i = Y_i - \\hat{Y}_i\\). This is the difference between the observed response and the expected response of our model. The residuals are of critical importance to testing how good our model is and will reappear in most subsequent sections.\n\nThe four variables in the linear regression model of Equation 1.1 split between whether they are fixed or random variables and between whether or not the analyst knows their value.\n\n\n\nKnown\nUnknown\n\n\n\n\nFixed\n\\(X\\)\n\\(\\beta\\)\n\n\nRandom\n\\(Y\\)\n\\(\\varepsilon\\)\n\n\n\nLastly, there is the concept of sum of squares. Letting \\(\\bar{Y} = n^{-1}\\sum_{i=1}^nY_i\\) be the sample mean for \\(Y\\), the total sum of squares is \\(SS_\\text{tot} = \\sum_{i=1}^n(Y_i-\\bar{Y})^2\\), which can be thought of as the total variation of the responses. This can be decomposed into a sum of the explained sum of squares and the residual sum of squares as follows: \\[\n  SS_\\text{tot} = SS_\\text{exp} + SS_\\text{res}\n  = \\sum_{i=1}^n (\\hat{Y}_i-\\bar{Y})^2\n  + \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2.\n\\] The explained sum of squares can be thought of as the amount of variation explained by the model while the residual sum of squares can be thought of as a measure of the variation that is not yet contained in the model. The sum of squares gives us an expression for the so called coefficient of determination, \\(R^2 = SS_\\text{exp}/SS_\\text{tot} = 1-SS_\\text{res}/SS_\\text{tot}\\in[0,1]\\), which is treated as a measure of what percentage of the variation is explained by the given model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt1_ols.html#point-estimation",
    "href": "chpt1_ols.html#point-estimation",
    "title": "1  Ordinary Least Squares",
    "section": "1.2 Point Estimation",
    "text": "1.2 Point Estimation\nIn the ordinary least squares setting, the our choice of estimator is \\[\n  \\hat{\\beta} = \\underset{\\tilde{\\beta}\\in\\mathbb{R}^p}{\\arg\\min}\n  \\sum_{i=1}^n( Y_i - X_{i,\\cdot}\\cdot\\tilde{\\beta} )^2\n\\tag{1.2}\\] where \\(X_{i,\\cdot}\\) is the \\(i\\)th row of the matrix \\(X\\). In the simple regression setting, this reduces to \\[\n  (\\hat{\\beta}_0,\\hat{\\beta}_1) =\n  \\underset{(\\tilde{\\beta}_0,\\tilde{\\beta}_1)\\in\\mathbb{R}^2}{\\arg\\min}\n  \\sum_{i=1}^n( Y_i - (\\tilde{\\beta}_0 + \\tilde{\\beta}_1x_i) )^2.\n\\] Note that this is equivalent to choosing a \\(\\hat{\\beta}\\) to minimize the sum of the squared residuals.\nIt is perfectly reasonable to consider other criterion beyond minimizing the sum of squared residuals. However, this approach results in an estimator with many nice properties. Most notably is the Gauss-Markov theorem:\n\nTheorem 1.1 (Gauss-Markov Theorem) Given the regression setting from Equation 1.1 and that for the errors, \\(\\mathrm{E}\\varepsilon_i = 0\\) for \\(i=1,\\ldots,n\\), \\(\\mathrm{Var}\\left(\\varepsilon_i\\right) = \\sigma^2\\) for \\(i=1,\\ldots,n\\), and \\(\\mathrm{cov}\\left(\\varepsilon_i,\\varepsilon_j\\right) = 0\\) for \\(i\\ne j\\), then the least squares estimator results in the minimal variance over all linear unbiased estimators.\n(This is sometimes referred to as the ``Best Linear Unbiased Estimator’’ or BLUE)\n\nHence, it can be shown that the estimator is unbiased, \\(\\mathrm{E}\\hat{\\beta} = \\beta\\). Furthermore, as long as the model contains an intercept term \\(\\beta_0\\), the constructed least squares line passes through the centre of the data in the sense that the sum of the residuals is zero, \\(\\sum_{i=1}^n r_i = 0\\) and that \\(\\bar{Y} = \\hat{\\beta}\\bar{X}\\) where \\(\\bar{Y} = n^{-1}\\sum_{i=1}^n Y_i\\) is the sample mean of the \\(Y_i\\) and where \\(\\bar{X}\\) is the vector of column means of the matrix \\(X\\).\n\n1.2.1 Derivation of the OLS estimator\nThe goal is to derive an explicit solution to Equation 1.2. First, consider the following partial derivative: \\[\\begin{align*}\n  \\frac{\\partial}{\\partial \\hat{\\beta}_k}\n  \\sum_{i=1}^n(Y_i-X_{i,\\cdot}\\cdot\\hat{\\beta})^2\n  &= -2\\sum_{i=1}^n(Y_i-X_{i,\\cdot}\\cdot\\hat{\\beta})X_{i,k}\\\\\n  &= -2\\sum_{i=1}^n(\n    Y_i-{\\textstyle \\sum_{j=1}^{p+1} X_{i,j}\\hat{\\beta}_j}\n  )X_{i,k}\\\\\n  &= -2\\sum_{i=1}^n Y_iX_{i,k}  \n     +2\\sum_{i=1}^n\\sum_{j=1}^{p+1} X_{i,j}X_{i,k}\\hat{\\beta}_j\n\\end{align*}\\] The above is the \\(k\\)th entry in the vector \\(\\nabla \\sum_{i=1}^n(Y_i-X_{i,\\cdot}\\hat{\\beta})^2\\). Hence, \\[\n  \\nabla \\sum_{i=1}^n(Y_i-X_{i,\\cdot}\\hat{\\beta})^2\n  = -2 {X}^\\mathrm{T}Y + 2{X}^\\mathrm{T}X\\hat{\\beta}.\n\\] Setting this equal to zero results in a critical point at \\[\n  {X}^\\mathrm{T}Y = {X}^\\mathrm{T}X\\hat{\\beta}\n\\] or \\(\\hat{\\beta} = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\) assuming \\({X}^\\mathrm{T}X\\) is invertible. Revisiting the terminology in the above definitions sections gives the following table:\n\n\n\n\n\n\n\nObject\nFormula\n\n\n\n\nLeast Squares Estimator:\n\\(\\hat{\\beta} = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\)\n\n\nFitted Values:\n\\(\\hat{Y} = X\\hat{\\beta}=X({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\)\n\n\nResiduals:\n\\(r = Y-\\hat{Y} = (I_n - X({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T})Y\\)\n\n\n\nIn the case that \\(n&gt;p\\) and that the columns of \\(X\\) are linearly independent, the matrix \\(P_X:=X({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}\\) is a rank \\(p+1\\) projection matrix. Similarly, \\(I_n-P_X\\) is the complementary rank \\(n-p-1\\) projection matrix. Intuitively, this implies that the fitted values are the projection on the observed values onto a \\(p\\)-dimensional subspace while the residuals arise from a projection onto the orthogonal subspace. As a result, it can be shown that \\(\\mathrm{cov}\\left(\\hat{Y},r\\right) = 0\\).\nNow that we have an explicit expression for the least squares estimator \\(\\hat{\\beta}\\), we can show that it is unbiased. \\[\n  \\mathrm{E}\\hat{\\beta} = \\mathrm{E}\\left(({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\right)\n  = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}\\mathrm{E}Y\n  = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T} X\\beta = \\beta.\n\\] Following that, we can compute its variance. \\[\\begin{align*}\n  \\mathrm{Var}\\left(\\hat{\\beta}\\right)\n  &= \\mathrm{E}\\left( (\\hat{\\beta}-\\beta){(\\hat{\\beta}-\\beta)}^\\mathrm{T} \\right)\\\\\n  &= \\mathrm{E}\\left( \\hat{\\beta}{\\hat{\\beta}}^\\mathrm{T} \\right) - \\beta{\\beta}^\\mathrm{T}\\\\\n  &= \\mathrm{E}\\left( ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y{(({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y)}^\\mathrm{T} \\right)\n     - \\beta{\\beta}^\\mathrm{T}\\\\\n  &= ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}\\mathrm{E}\\left( Y{Y}^\\mathrm{T} \\right)X({X}^\\mathrm{T}X)^{-1}\n     - \\beta{\\beta}^\\mathrm{T}\\\\\n  &= ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T} (\n       \\sigma^2I_n + X\\beta{\\beta}^\\mathrm{T}{X}^\\mathrm{T}\n     ) X({X}^\\mathrm{T}X)^{-1}\n     - \\beta{\\beta}^\\mathrm{T}\\\\\n  &= \\sigma^2({X}^\\mathrm{T}X)^{-1}.\n\\end{align*}\\]\nThus far, we have only assumed that \\(\\varepsilon\\) is a random vector with iid entries with mean zero and variance \\(\\sigma^2\\). If in addition, we assumed that \\(\\varepsilon\\) has a normal or Gaussian distribution, then \\[\n  \\varepsilon\\sim\\mathcal{N}\\left(0,\\sigma^2I_n\\right),~~\n  Y\\sim\\mathcal{N}\\left(X\\beta,\\sigma^2I_n\\right),\\text{ and }\n  \\hat{\\beta}\\sim\\mathcal{N}\\left(\\beta,\\sigma^2({X}^\\mathrm{T}X)^{-1}\\right).\n\\] Furthermore, with a little work, one can show that for the fitted values and residuals also have normal distributions in this setting: \\[\n  \\hat{Y}\\sim\\mathcal{N}\\left(X\\hat{\\beta},\\sigma^2P_X\\right),~\\text{ and }~\n  r\\sim\\mathcal{N}\\left(0,\\sigma^2(I_n-P_X)\\right).\n\\] Notice that the two above covariance matrices are not generally of full rank. This assumption that the errors follow a normal distribution is a very common assumption to make in practice.\n\n\n1.2.2 Maximum likelihood estimate under normality\nIn the previous section, the OLS estimator is derived by minimizing the sum of the squared errors. Now, given the additional assumption that the errors have a normal distribution, we can compute an alternative estimator for \\(\\beta\\): the maximum likelihood estimate (MLE). We can also use this to simultaneously compute the MLE for \\(\\sigma^2\\).\nFrom above we have that \\(Y\\sim\\mathcal{N}\\left(X\\beta,\\sigma^2I_n\\right)\\), and hence the likelihood is \\[\\begin{align*}\n  L(\\beta,\\sigma^2; X,Y)\n  &= (2\\pi\\sigma^2)^{-n/2}\\exp\\left(\n    -\\frac{1}{2\\sigma^2}{(Y-X\\beta)}^\\mathrm{T}(Y-X\\beta)\n  \\right).\n\\end{align*}\\] The log likelihood is then \\[\\begin{multline*}\n\\ell(\\beta,\\sigma^2;X,Y) = \\log L(\\beta,\\sigma^2;X,Y)\n=\\\\= -\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2\n   -\\frac{1}{2\\sigma^2}{(Y-X\\beta)}^\\mathrm{T}(Y-X\\beta).\n\\end{multline*}\\] This implies that the MLE for \\(\\beta\\) comes from solving \\[\n  0 = \\frac{\\partial\\ell}{\\partial\\beta} =\n  \\frac{\\partial}{\\partial\\beta} {(Y-X\\beta)}^\\mathrm{T}(Y-X\\beta),\n\\] which is solved by the OLS estimator from above. Hence, the MLE under normality is the least squares estimator.\nFor the variance term \\(\\sigma^2\\), the MLE is similarly found by solving \\[\n  0= \\frac{\\partial\\ell}{\\partial\\sigma^2} =\n  -\\frac{n}{2} (\\sigma^{2})^{-1}\n  +\\frac{(\\sigma^2)^{-2}}{2}{(Y-X\\beta)}^\\mathrm{T}(Y-X\\beta).\n\\] This occurs for \\(\\hat{\\sigma}^2 = n^{-1}{(Y-X\\hat{\\beta})}^\\mathrm{T}(Y-X\\hat{\\beta})\\), which is just the average sum of squares of the residuals: \\(\\hat{\\sigma}^2 = n^{-1}\\sum_{i=1}^n r_i^2\\). However, this is a biased estimator of the variance as the residuals are not independent and have a degenerate covariance matrix of rank \\(n-p-1\\). Intuitively, this implies that the sum of squared residuals has \\(n-p-1\\) degrees of freedom resulting in \\[\n  \\frac{SS_\\text{res}}{\\sigma^2} = \\frac{1}{\\sigma^2}\\sum_{i=1}^n r_i^2\n  \\sim\\chi^2\\left(n-p-1\\right)\n\\] and the unbiased estimator of \\(\\sigma^2\\) being \\(SS_\\text{res}/(n-p-1) = \\hat{\\sigma}^2(n/(n-p-1))\\).\nFor a more precise explanation of where this comes from, see Cochran’s Theorem which is beyond the scope of this course.\n\n1.2.2.1 Chocolate-Nobel Data\nRunning a regression in R on the chocolate consumption vs Nobel prize data from above results in a fitted model \\[\n  (\\text{Nobel Prizes}) =  -0.991 + 1.3545(\\text{Chocolate})\n\\] This indicates that a 1 kg increase in chocolate consumption per capita corresponds to an expected increase in 1.35 Nobel prizes per 10 million people.\n\n# Read in Table\ndat = read.table(\"data/chocoTable.r\");\n# Construct a linear model\nlmDat = lm( nobel ~ choco, dat=dat );\n# print summary of model\nsummary(lmDat);\n\n\nCall:\nlm(formula = nobel ~ choco, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6876 -1.6504 -0.5288  0.1484 11.3922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.9910     2.1327  -0.465  0.64932   \nchoco         1.3545     0.3446   3.931  0.00151 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.666 on 14 degrees of freedom\nMultiple R-squared:  0.5246,    Adjusted R-squared:  0.4907 \nF-statistic: 15.45 on 1 and 14 DF,  p-value: 0.001508\n\n\n\n\n\n1.2.3 Proof of the Gauss-Markov Theorem\n\nProof. Any linear estimator can be written as \\(AY\\) for some non-random matrix \\(A\\in\\mathbb{R}^{(p+1)\\times n}\\). We can in turn write \\(A = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}+D\\) for some matrix \\(D\\in\\mathbb{R}^{(p+1)\\times n}\\). Then, as \\[\\begin{align*}\n    \\mathrm{E}\\left( AY \\right)  \n    &= AX\\beta \\\\\n    &= \\left[({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}+D\\right]X\\beta \\\\\n    &= \\beta + DX\\beta,\n  \\end{align*}\\] the unbiased condition implies that \\(DX\\beta=0\\) for any \\(\\beta\\in\\mathbb{R}^{p+1}\\) and hence that \\(DX=0\\).\nNext, we compute the variance of the arbitrary linear unbiased estimator to get \\[\\begin{align*}\n    \\mathrm{Var}\\left(AY\\right) &=\n    A\\mathrm{Var}\\left(Y\\right){A}^\\mathrm{T}\\\\\n    &=\\sigma^2\\left[\n      ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}+D\n    \\right]{\\left[\n      ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}+D\n    \\right]}^\\mathrm{T}\\\\\n    &= \\sigma^2\\left[\n      ({X}^\\mathrm{T}X)^{-1} +\n      ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}{D}^\\mathrm{T} +\n      DX({X}^\\mathrm{T}X)^{-1} +\n      D{D}^\\mathrm{T}\n    \\right] \\\\\n    &= \\sigma^2\\left[\n      ({X}^\\mathrm{T}X)^{-1} +\n      D{D}^\\mathrm{T}\n    \\right].\n  \\end{align*}\\] Hence, to minimize the variance, we must minimize \\(D{D}^\\mathrm{T}\\) as \\(D{D}^\\mathrm{T}\\) is necessarily a positive semi-definite matrix. This is achieved by setting \\(D=0\\) and arriving at \\(({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\) having minimial variance.\n\n\nRemark 1.1. Note that \\(D{D}^\\mathrm{T}\\) is positive semi-definite for any choice of \\(D\\) as for any \\(w\\in\\mathbb{R}^{p+1}\\), we have \\[\n    {w}^\\mathrm{T}(D{D}^\\mathrm{T}){w} =\n    {({D}^\\mathrm{T}w)}^\\mathrm{T}(Dw) = \\lVert Dw\\rVert_2 \\ge 0.\n  \\]\n\n\nRemark 1.2. While \\(\\hat{\\beta}\\) has minimal variance over all unbiased estimators, we can lessen the variance further if we allow for biased estimators. This is considered in many more advanced regression methods such as ridge regression and lasso.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt1_ols.html#hypothesis-testing",
    "href": "chpt1_ols.html#hypothesis-testing",
    "title": "1  Ordinary Least Squares",
    "section": "1.3 Hypothesis Testing",
    "text": "1.3 Hypothesis Testing\n\n1.3.1 Goodness of fit\nWe now have a model for our data, and in some sense, this model is optimal as it minimizes the squared errors. However, even being optimal, we are still interested in knowing whether or not this is a good model for our data. This is a question of goodness of fit.\nThe first question to ask is, do any of the regressors provide information about the response in the linear model framework? This can be written mathematically as \\[\nH_0: \\beta_1 = \\ldots = \\beta_p = 0, ~~~~~ H_1: \\exists i\\ge1~s.t.~\\beta_i\\ne0,\n\\tag{1.3}\\] which is asking is there at least one \\(\\beta_i\\) that we can claim is non-zero and hence implies that the regressor \\(x_i\\) has some nontrivial influence over \\(y\\).\nTo test this hypothesis, we revisit the explained and residual sums of squares introduced in the Definitions section. Specifically, we already have that \\(SS_\\text{res}/\\sigma^2 \\sim\\chi^2\\left(n-p-1\\right)\\) from above. Similarly, \\(SS_\\text{exp}/\\sigma^2 \\sim\\chi^2\\left(p\\right)\\) under the null hypothesis where \\(\\beta_1=\\ldots=\\beta_p=0\\), and hence any variation in those terms should be pure noise. Lastly, it can be demonstrated that \\(SS_\\text{res}\\) and \\(SS_\\text{exp}\\) are independent random variables, which intuitively follows from the orthogonality of the fitted values and the errors. Once again, this can be made precise via Cochran’s Theorem.\nThe usual test statistic for the hypothesis in Equation 1.3 is \\[\n  \\frac{SS_\\text{exp}/p}{SS_\\text{res}/(n-p-1)} \\sim\n  F\\left(p,n-p-1\\right),\n\\] which leads to an F test. If the test statistic is large, then the explained variation is larger than the noise resulting in a small p-value and a rejection of the null hypothesis.\n\n1.3.1.1 F test on Chocolate-Nobel data\nFrom the final line of the R output from the summary() command, we have a test statistic value of 15.45 with degrees of freedom 1 and 14. This results in a very small p-value of 0.001508.\n\nsummary(lmDat)\n\n\nCall:\nlm(formula = nobel ~ choco, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6876 -1.6504 -0.5288  0.1484 11.3922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.9910     2.1327  -0.465  0.64932   \nchoco         1.3545     0.3446   3.931  0.00151 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.666 on 14 degrees of freedom\nMultiple R-squared:  0.5246,    Adjusted R-squared:  0.4907 \nF-statistic: 15.45 on 1 and 14 DF,  p-value: 0.001508\n\n\nIf you were to run the regression in R without the intercept term, which is fixing \\(\\beta_0=0\\), then the result is \\(\\hat{\\beta}_1 = 1.22\\), a value for the test statistic for the F test of 44.24, now with degrees of freedom 1 and 15, and an even smaller p-value of \\(7.7\\times10^{-6}\\). Typically, regression models always include an intercept term. However, there are some situations where we wish the enforce that an input of zero returns an output of zero.\n\n# Construct a linear model\nlmDat0 = lm( nobel ~ choco - 1, dat=dat );\n# print summary of model\nsummary(lmDat0);\n\n\nCall:\nlm(formula = nobel ~ choco - 1, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4991 -1.7891 -1.3237 -0.1955 10.9910 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nchoco   1.2205     0.1835   6.651 7.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.543 on 15 degrees of freedom\nMultiple R-squared:  0.7468,    Adjusted R-squared:  0.7299 \nF-statistic: 44.24 on 1 and 15 DF,  p-value: 7.725e-06\n\n# Plot data\nplot( \n  dat$choco, dat$nobel,\n  xlab=\"Chocolate Consumption (kg per capita)\",\n  ylab=\"Nobel prizes per 10 million\",\n  las=1,#xlim=c(0,max(dat$choco)+1),\n  ylim=c(-1,max(dat$nobel))\n);\n# Label data\ntext(\n  x=dat$choco, y=dat$nobel,\n  labels=dat$abbrev, \n  pos=1,cex=0.7\n)\n\n# plot regression line\nabline(lmDat,col='blue',lwd=2,lty=1)\nabline(lmDat0,col='red',lwd=2,lty=2)\n# Add a legend\nlegend(\n  \"topleft\",legend = c(\"with intercept\",\"without intercept\"),\n  col = c(\"blue\",\"red\"),lwd=2,lty=1:2\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 Regression coefficients\nGiven that the previous F test results in a significant p-value, the subsequent question is to ask which of the \\(p\\) regressors are significant? Hence, we have the following hypotheses for \\(j=0,1,\\ldots,p\\). \\[\n  H_{0,j}: \\beta_j=0 ~~~~~\n  H_{1,j}: \\beta_j\\ne0.\n\\] Each individual \\(\\hat{\\beta}_j\\sim\\mathcal{N}\\left(\\beta_j,\\sigma^2({X}^\\mathrm{T}X)^{-1}_{j,j}\\right)\\) where \\(({X}^\\mathrm{T}X)^{-1}_{j,j}\\) is the \\(j\\)th entry in the diagonal of \\(({X}^\\mathrm{T}X)^{-1}_{j,j}\\).\n\nRemark 1.3. We will index the entries of the matrix from \\(0,1,\\ldots,p\\) to conform with the indexing of the \\(\\beta\\)’s. Note that this is a \\((p+1)\\times(p+1)\\) matrix.\n\nThus, under the null hypothesis that \\(\\beta_j=0\\), we have that \\[\n  \\hat{\\beta}_j/\\sqrt{ \\sigma^2({X}^\\mathrm{T}X)^{-1}_{j,j} } \\sim\\mathcal{N}\\left(0,1\\right).\n\\] However, we cannot perform a z test as \\(\\sigma^2\\) is unknown. To rectify this, the unbiased estimator for \\(\\sigma^2\\) is used in its place resulting in \\[\n  \\frac{\\hat{\\beta}_j}{\\sqrt{ ({X}^\\mathrm{T}X)^{-1}_{j,j}SS_\\text{res}/(n-p-1) }}\n  \\sim t\\left(n-p-1\\right),\n\\] and a t test can be performed. If the value of the test statistic is large, then there may be sufficient evidence to reject the null that \\(\\beta_j=0\\). The denominator is often referred to as the standard error. To simplify future formulae, this will be denoted as \\(\\text{se}(\\beta_j)\\).\nIt is worth noting that this test looks for significant influence of the \\(j\\)th regressor on the response given all of the other regressors. Hence, it quantifies the marginal as opposed to the absolute effect of that variable on the model. These ideas will be investigated further when discussing variable selection later in this book. However, as a quick word of caution, when \\(p\\) hypothesis tests are performed, the analyst needs to consider multiple testing corrections.\n\n1.3.2.1 t test on Chocolate-Nobel data\nThe R commands lm() and summary() will return a table of regression coefficients, t test statistics and p-values associated with each coefficient. For the Chocolate-Nobel prize data, the table looks like\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(\\(&gt;\\lvert t\\rvert\\))\n\n\n\n\n(Intercept)\n-0.9910\n2.1327\n-0.465\n0.64932\n\n\nchoco\n1.3545\n0.3446\n3.931\n0.00151\n\n\n\n\nsummary(lmDat)\n\n\nCall:\nlm(formula = nobel ~ choco, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6876 -1.6504 -0.5288  0.1484 11.3922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.9910     2.1327  -0.465  0.64932   \nchoco         1.3545     0.3446   3.931  0.00151 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.666 on 14 degrees of freedom\nMultiple R-squared:  0.5246,    Adjusted R-squared:  0.4907 \nF-statistic: 15.45 on 1 and 14 DF,  p-value: 0.001508\n\n\n\n\n\n1.3.3 Partial F-test\nIn the previous two sections, we first tested as to whether or not there exists at least one \\(\\beta_j\\), \\(j=1,\\ldots,p\\), that is non-zero. Then, we tested whether or not a specific \\(\\beta_j\\) is non-zero. The next logical question is whether or not some collection of \\(\\beta_j\\)’s of size strictly between \\(1\\) and \\(p\\) has a non-zero element. That is, for a fixed \\(q\\), \\[\n  H_0: \\beta_{p-q+1} = \\ldots = \\beta_p = 0 ~~~~~~~~\n  H_1: \\exists i\\ge p-q+1~s.t.~\\beta_i\\ne0.\n\\tag{1.4}\\] Here, we are comparing two different models, which are the partial and full models, respectively, \\[\n  Y = \\beta_{0:p-q}X+\\varepsilon,~~\\text{ and }~~Y = \\beta_{0:p-q}X+\\beta_{p-q+1:p}X+\\varepsilon,\n\\] and want to know whether the final \\(q\\) regressors add any significant explanation to our model given the other \\(p-q\\). For the above notation, \\[\n  \\beta_{i:j} = {(0,\\ldots,0,\\beta_i,\\beta_{i+1},\\ldots,\\beta_j,0,\\ldots,0)}^\\mathrm{T}.\n\\]\nTo run the hypothesis test in Equation 1.4, we would have to compute the least squares estimator in the partial model, \\(\\hat{\\beta}_{1:p-q}\\), and the standard least squares estimator in the full model, \\(\\hat{\\beta}\\). Then, we will have to compute the additional explained sum of squares gained from adding the \\(q\\) extra regressors to our model, which is \\[\n  SS_\\text{exp}(\\beta_{p-q+1:p}|\\beta_{1:p-q}) =\n  SS_\\text{exp}(\\beta) - SS_\\text{exp}(\\beta_{1:p-q}),\n\\] the explained sum of squares from the full model minus the explained sum of squares from the partial model.\nSimilarly to the full F-test from above, we have under the null hypothesis that \\(SS_\\text{exp}(\\beta_{p-q+1:p}|\\beta_{1:p-q})/\\sigma^2 \\sim\\chi^2\\left(q\\right).\\) Hence, \\[\n  \\frac{SS_\\text{exp}(\\beta_{p-q+1:p}|\\beta_{1:p-q})/q}{SS_\\text{res}/(n-p-1)} \\sim\n  F\\left(q,n-p-1\\right),\n\\] so if this test statistic is large, then we have evidence to suggestion that at least one of the additional \\(q\\) regressors adds some explanatory power to our model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt1_ols.html#interval-estimators",
    "href": "chpt1_ols.html#interval-estimators",
    "title": "1  Ordinary Least Squares",
    "section": "1.4 Interval Estimators",
    "text": "1.4 Interval Estimators\n\n1.4.1 Confidence Intervals\nConfidence intervals play a complementary role with hypothesis testing. From the development of the above test for an individual \\({\\beta}_j\\), we have that \\[\n  \\frac{\\hat{\\beta}_j-\\beta_j}{\\text{se}(\\beta_j)}\n  \\sim t\\left(n-p-1\\right),\n\\] Hence, a \\(1-\\alpha\\) confidence interval for the parameter \\(\\beta_j\\) is \\[\n  \\hat{\\beta}_j - t_{\\alpha/2,n-p-1}\\text{se}(\\beta_j)\n  \\le \\beta \\le\n  \\hat{\\beta}_j + t_{\\alpha/2,n-p-1}\\text{se}(\\beta_j)\n\\] where \\(t_{\\alpha/2,n-p-1}\\in\\mathbb{R}^+\\) is such that \\(\\mathrm{P}\\left(T \\le t_{\\alpha/2,n-p-1}\\right)=\\alpha/2\\) when \\(T\\sim t\\left(n-p-1\\right)\\).\nWhile the above can be used to produce a confidence interval for each individual parameter, combining these intervals will not result in a \\(1-\\alpha\\) confidence set for the entire parameter vector. To construct such a confidence region, a little more care is required. Also, we will construct a confidence set for the entire vector \\((\\beta_0,\\beta_1,\\ldots,\\beta_p)\\), which results in \\(p+1\\) degrees of freedom in what follows. As \\(\\hat{\\beta} \\sim\\mathcal{N}\\left(\\beta,\\sigma^2({X}^\\mathrm{T}X)^{-1}\\right)\\) we have that \\[\n  \\sigma^{-2}{(\\hat{\\beta}-\\beta)}^\\mathrm{T}{X}^\\mathrm{T}X(\\hat{\\beta}-\\beta)\n  \\sim\\chi^2\\left(p+1\\right).\n\\] From before, we have that \\(SS_\\text{res}/\\sigma^2 \\sim\\chi^2\\left(n-p-1\\right)\\). Hence \\[\n  \\frac{\n    {(\\hat{\\beta}-\\beta)}^\\mathrm{T}{X}^\\mathrm{T}X(\\hat{\\beta}-\\beta)/(p+1)\n  }{\n    SS_\\text{res}/(n-p-1)\n  } \\sim F\\left(p+1,n-p-1\\right).\n\\] Thus, a \\(1-\\alpha\\) confidence ellipsoid can be constructed as \\[\n  \\frac{\n    {(\\hat{\\beta}-\\beta)}^\\mathrm{T}{X}^\\mathrm{T}X(\\hat{\\beta}-\\beta)/(p+1)\n  }{\n    SS_\\text{res}/(n-p-1)\n  } \\le F_{\\alpha,p+1,n-p-1}.\n\\]\nA 95% and a 99% confidence ellipsoid for the Chocolate-Nobel prize data is displayed in the code below. Notice that both ellipses contain \\(\\hat{\\beta}_0=0\\) which had a t statistic p-value of 0.649. Meanwhile neither contain \\(\\hat{\\beta}_1=0\\) whose p-value was the very significant 0.0015. The confidence ellipses were plotted with help from the R library ellipse.\n\nlibrary(ellipse)\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\nplot(\n  ellipse(lmDat,level=.99),type='l',\n  ylim=c(0,3),col='blue',lwd=2,las=1\n);\nabline(h=0,v=0,col='darkgray');\nlines(ellipse(lmDat,level=.99),type='l',col='blue',lwd=2);\nlines(ellipse(lmDat,level=.95),type='l',col='green',lwd=2);\nlegend(\n  \"topright\",legend = c(\"95% confidence\",\"99% confidence\"),\n  col = c(\"green\",\"blue\"),lwd = 2\n)\n\n\n\n\n\n\n\n\n\n\n1.4.2 Prediction Intervals for an expected observation\nGiven the least squares model, the analyst may be interested in estimating the expected value of \\(Y\\) have some specific input \\(x=(1,x_1,\\ldots,x_p)\\). Our new random variable is \\(\\hat{Y}_0 = \\hat{\\beta}\\cdot X\\) where \\(X\\) is fixed and \\(\\hat{\\beta}\\) is random. Of course, the expected value is just \\[\n  \\mathrm{E}\\left(\\hat{Y}_0\\middle|X=x\\right) = \\mathrm{E}{\\hat{\\beta}}\\cdot x\n  = \\beta_0 + \\sum_{i=1}^p \\beta_ix_i.\n\\] To find a \\(1-\\alpha\\) interval estimate for \\(\\hat{Y}_0\\) at \\(X=x\\), recall once again that \\(\\hat{\\beta}\\sim\\mathcal{N}\\left(\\beta,\\sigma^2({X}^\\mathrm{T}X)^{-1}\\right)\\). Thus, \\[\n  \\hat{Y}_0|(X=x) \\sim\\mathcal{N}\\left(\\beta\\cdot x,\\sigma^2 {x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x\\right).\n\\] Hence, \\[\n  \\frac{\\hat{\\beta}\\cdot x - \\mathrm{E}\\left(\\hat{Y}_0\\middle|X=x\\right)}{\n    \\sqrt{ \\sigma^2 {x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x }\n  }\\sim\\mathcal{N}\\left(0,1\\right),\n\\] and \\[\n  \\frac{\\hat{\\beta}\\cdot x - \\mathrm{E}\\left(\\hat{Y}_0\\middle|X=x\\right)}{\n    \\sqrt{ (SS_\\text{res}/(n-p-1)){x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x }\n  }\\sim t\\left(n-p-1\\right),\n\\] which results in the following \\(1-\\alpha\\) confidence interval: \\[\\begin{multline*}\n  \\hat{\\beta}\\cdot x - t_{\\alpha/2,n-p-1}\\sqrt{\n    \\frac{SS_\\text{res}}{n-p-1}{x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x\n  }\n  \\le\\\\\\le \\mathrm{E}\\left(\\hat{Y}_0\\middle|X=x\\right)=\\beta\\cdot x \\le\\\\\\le\n  \\hat{\\beta}\\cdot x + t_{\\alpha/2,n-p-1}\\sqrt{\n    \\frac{SS_\\text{res}}{n-p-1}{x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x\n  }.\n\\end{multline*}\\]\n\n\n1.4.3 Prediction Intervals for a new observation\nIn the previous subsection, we asked for a confidence interval for the expected value of the response given a new vector of regressors, which was a confidence interval for \\(\\mathrm{E}\\left(\\hat{Y}_0\\middle|X=x\\right)=\\beta\\cdot x\\) based on \\(\\hat{\\beta}\\cdot x\\). Now, we want to determine a confidence interval for the future response given a vector of regressors. That is, we want an interval for \\(Y_0 = \\beta\\cdot x + \\varepsilon_0\\sim\\mathcal{N}\\left(\\beta\\cdot x,\\sigma^2\\right)\\), but, as usual, \\(\\beta\\) unknown.\nTo circumvent this, note that \\[\n  Y_0 - \\hat{Y}_0\n  = (\\beta\\cdot x + \\varepsilon_0) - \\hat{\\beta}\\cdot x\n  \\sim\\mathcal{N}\\left(0,\\sigma^2( 1 + {x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x )\\right),\n\\] because the variances of \\(\\varepsilon_0\\) and \\(\\hat{\\beta}\\cdot x\\) sum as these are independent random variables. Hence, applying the usual rearrangement of terms and replacement of \\(\\sigma^2\\) with \\(SS_\\text{res}/(n-p-1)\\) results in \\[\\begin{multline*}\n  \\hat{\\beta}\\cdot x - t_{\\alpha/2,n-p-1}\\sqrt{\n    \\frac{(1+{x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x)SS_\\text{res}}{n-p-1}\n  }\n  \\le Y_0 \\le \\\\ \\le\n  \\hat{\\beta}\\cdot x + t_{\\alpha/2,n-p-1}\\sqrt{\n    \\frac{(1+{x}^\\mathrm{T}({X}^\\mathrm{T}X)^{-1}x)SS_\\text{res}}{n-p-1}\n  }.\n\\end{multline*}\\]\nTo demonstrate these prediction intervals, we once again consider the Chocolate-Nobel prize data for both the expected mean and for a new observation.\n\n# Plot data\nplot( \n  dat$choco, dat$nobel,\n  xlab=\"Chocolate Consumption (kg per capita)\",\n  ylab=\"Nobel prizes per 10 million\",\n  las=1,#xlim=c(0,max(dat$choco)+1),\n  ylim=c(-1,max(dat$nobel))\n);\n# Label data\ntext(\n  x=dat$choco, y=dat$nobel,\n  labels=dat$abbrev, \n  pos=1,cex=0.7\n)\n# plot regression line\nabline(lmDat,col='blue',lwd=2)\n\n# plot 95% confidence and prediction intervals\ntt = seq(0,13,0.1)\nprDat  = predict(\n  lmDat,newdata=data.frame(choco=tt),\n  interval='confidence',level=0.95\n)\nprDat2 = predict(\n  lmDat,newdata=data.frame(choco=tt),\n  interval='prediction',level=0.95\n)\nlines(tt,prDat[,2],col='blue')\nlines(tt,prDat[,3],col='blue')\nlines(tt,prDat2[,2],col='red')\nlines(tt,prDat2[,3],col='red')\ntext(\n  c(4,4),c(1.5,14),\n  labels=c(\"Expected Observation\",\"New Observation\"),\n  col=c(\"blue\",\"red\"),pos=4\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt1_ols.html#indicator-variables-and-anova",
    "href": "chpt1_ols.html#indicator-variables-and-anova",
    "title": "1  Ordinary Least Squares",
    "section": "1.5 Indicator Variables and ANOVA",
    "text": "1.5 Indicator Variables and ANOVA\n\n1.5.1 Indicator variables\nThus far, we have considered models of the form \\[\n  y = \\beta_0 + \\beta_1x_1 +\\ldots+\\beta_px_p + \\varepsilon\n\\] where the regressors \\(x_1,\\ldots,x_p\\in\\mathbb{R}\\) can take on any real value. However, very often in practice, we have regressors that take on categorical values. For example, male vs female, employed vs unemployed, treatment vs placebo, Edmonton vs Calgary, etc. When there is a binary choice as in these examples, we can choose one category to correspond to zero and the other category to correspond to one.\nAs an example, consider \\[\n  y = \\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + \\varepsilon\n\\tag{1.5}\\] where \\(x_1\\in\\mathbb{R}\\) and \\(x_2\\in\\{0,1\\}\\). Then, we effectively have two models: \\[\\begin{align*}\n  y &= \\beta_0 + \\beta_1x_1 + 0 + \\varepsilon\\\\\n  y &= \\beta_0 + \\beta_1x_1 + \\beta_2 + \\varepsilon= (\\beta_0+\\beta_2) + \\beta_2x_1 +\\varepsilon.\n\\end{align*}\\] What we have is two models with the same slope \\(\\beta_1\\) but with two different intercepts \\(\\beta_0\\) and \\(\\beta_0+\\beta_2\\), which are two parallel lines.\n\nRemark 1.4. A first thought is to merely split the data and train two separate models. However, we want to use the entire dataset at once specifically to estimate the common slope \\(\\beta_1\\) with as much accuracy as possible.\n\nWhile the range of the regressors has changed, we will fit the least squares estimate to the model precisely as before. Now, considering the model in Equation 1.5, assume that we have \\(m\\) samples with \\(x_2=0\\) and \\(n\\) samples with \\(x_2=1\\). Our design matrix takes on a new form: \\[\n  Y =\n  \\begin{pmatrix}\n    Y_1 \\\\ \\vdots \\\\ Y_m \\\\ Y_{m+1} \\\\ \\vdots \\\\ Y_{m+n}\n  \\end{pmatrix},~~~~\n  X =\n  \\begin{pmatrix}\n    1 & x_{1} & 0 \\\\\n    \\vdots & \\vdots & \\vdots \\\\\n    1 & x_{m} & 0 \\\\\n    1 & x_{m+1} & 1 \\\\\n    \\vdots & \\vdots & \\vdots \\\\\n    1 & x_{m+n} & 1 \\\\\n  \\end{pmatrix},~~~~\n  \\beta =\n  \\begin{pmatrix}\n    \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\n  \\end{pmatrix},~~~~\n  \\varepsilon=\n  \\begin{pmatrix}\n    \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_{m+n}\n  \\end{pmatrix}.\n\\] However, the least squares estimate is computed as before as \\(\\hat{\\beta} = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\). Furthermore, we can perform hypothesis tests on the fitted model such as \\[\n  H_0: \\beta_2=0~~~~H_1:\\beta_2\\ne0,\n\\] which is equivalently asking whether or not the regression lines have the same intercept.\nModels can be expanded to include multiple indicator variables as long as the matrix \\({X}^\\mathrm{T}X\\) is still invertible. For example, let’s suppose we want to look at wages in Alberta with respect to age but partitioned for male vs female and for Edmonton vs Calgary. Then, the model would look like \\[\n  (\\text{wage}) = \\beta_0 + \\beta_1(\\text{age}) +\n  \\beta_2(\\text{Is male?}) + \\beta_3(\\text{Is from Edmonton?}).\n\\] In the silly case that our data only consisted of men from Calgary and women from Edmonton, then the final regressor is redundant and \\({X}^\\mathrm{T}X\\) will not be invertible. While this extreme case should not occur, it is possible to have an imbalance in the categories, which we will discuss later.\n\n\n1.5.2 ANOVA\nANOVA, or the Analysis of Variance, is a slightly overloaded term in statistics. We already considered ANOVA tables when comparing nested models in the hypothesis tests. However, ANOVA can also be used in the setting of the so-called One-Way Analysis of Variance. In this case, we want to compare \\(k\\) samples for equality of the means. For example, we take height measurements from randomly selected citizens from different countries and ask whether or not there is significant evidence to reject the claim that all nations have roughly the same height distribution.\nThe reason for discussing ANOVA in these notes is that it can be written in a linear regression context as follows. Imagine that we have \\(k\\) different groups of observations with sample sizes \\(n_j\\), \\(j=1,\\ldots,k\\) for each group. Let \\(y_{i,j}\\) be the \\(i\\)th observation from the \\(j\\)th group where \\(i\\in\\{1,\\ldots,n_j\\}\\) and \\(j\\in\\{1,\\ldots,k\\}\\). The model is \\[\n  y_{i,j} = \\mu_j + \\varepsilon_{i,j},\n\\] which is each observation is just some group mean, \\(\\mu_j\\), with the addition of random noise.\nFrom here, one can show that the fitted values as just \\(\\hat{y}_{i,j} = n_{j}^{-1}\\sum_{l=1}^{n_j} y_{l,j}\\), which is the \\(j\\)th sample mean. Then, an F-test can be performed similar to what we did above with F tests to test \\[\n  H_0: \\mu_1=\\ldots=\\mu_k~~~\n  H_1: \\exists j_1\\ne j_2~s.t.~\\mu_{j_1}\\ne\\mu_{j_2}.\n\\]\nTo reformulate the model to align with our F-test from before, we rewrite it as a linear regression with indicator variables for the regressors \\[\n  y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_{k-1}x_{k-1} + \\varepsilon_{i,j}\n\\] with \\(\\beta_0 = \\mu_k\\) and \\(\\beta_j={\\mu_j-\\mu_k}\\) for \\(j=1,\\ldots,k-1\\). Then, we can test for whether or not there exists at least one \\(\\beta_j\\ne0\\) for \\(j=1,\\ldots,k-1\\). Here, the degrees of freedom for the explained sum of squares is \\(k-1\\) and the degrees of freedom for the residual sum of squares is \\(N - (k-1) -1 = N-k\\) with \\(N = \\sum_{j=1}^k n_j\\).\nIf all of the \\(n_j\\) are equal, this reduces to \\(k(n-1)\\).\nIn this case, the vector \\(Y\\) and the design matrix \\(X\\) will take on the form \\[\n  Y = \\begin{pmatrix}\n    y_{1,1} \\\\ y_{2,1} \\\\ \\vdots \\\\ y_{n_1,1} \\\\ y_{1,2} \\\\ \\vdots \\\\\n    y_{n_k,k}\n  \\end{pmatrix},~~\n  X = \\begin{pmatrix}\n    1 & 1  & 0 & \\dots & 0  \\\\\n    1 & 1  & 0 & \\dots & 0  \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & 1  & 0 & \\dots & 0  \\\\\n    1 & 0  & 1 & \\dots & 0  \\\\\n    1 & 0  & 1 & \\dots & 0  \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & 0  & 0 & \\dots & 1  \\\\\n    1 & 0  & 0 & \\dots & 1  \\\\\n    1 & 0  & 0 & \\dots & 0  \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & 0  & 0 & \\dots & 0  \\\\\n    1 & 0  & 0 & \\dots & 0\n  \\end{pmatrix}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt1_ols.html#data-example-exam-grades",
    "href": "chpt1_ols.html#data-example-exam-grades",
    "title": "1  Ordinary Least Squares",
    "section": "1.6 Data Example: Exam Grades",
    "text": "1.6 Data Example: Exam Grades\nTo illustrate the topics discussed in this chapter, we will consider a dataset of course grades for a linear regression class. This dataset consists of \\(n=56\\) undergraduate students who are either in their 3rd year (22 students) or 4th year (33 students) of university studies. The year of study acts as a binary variable as discussed in the previous section. This dataset also has columns corresponding to the students overall mark on written homework assignments, online coding assignments, the midterm exam, and the final exam. In what follows, we will see what variables if any can be used to predict the final exam grade and, furthermore, if there is a difference between the final exam grades for the 3rd year and 4th year students.\n\n# Read in Data\nexams = read.csv(\"data/statGrades.csv\")\n\n# Tell R that \"year\" is a categorical variable\nexams$year &lt;- as.factor(exams$year)\n\n# Look at the data\nhead(exams)\n\n      year written    online  midterm   final\n1 4th Year  80.668  83.84127 70.00000 56.4948\n2 3rd Year  96.668 100.00000 50.00000 54.0000\n3 4th Year  90.000  93.73737 50.00000 43.5024\n4 3rd Year  98.000  98.57143 56.66667 62.9964\n5 3rd Year  46.000  48.72727 50.00000 43.5024\n6 4th Year  75.334  96.34921 93.33333 81.0000\n\n\nWhen comparing two independent groups, we can use the classic two sample t test using t.test() in R. The default is to assume that the variances between the two groups are not equal, which is the so-called Welch’s t-test. Otherwise, we can tell the function to treat the variances as equal by setting var.equal=T.\n\nt.test(\n  final ~ year,\n  data = exams\n)\n\n\n    Welch Two Sample t-test\n\ndata:  final by year\nt = -0.42107, df = 43.775, p-value = 0.6758\nalternative hypothesis: true difference in means between group 3rd Year and group 4th Year is not equal to 0\n95 percent confidence interval:\n -11.709616   7.662733\nsample estimates:\nmean in group 3rd Year mean in group 4th Year \n              65.67292               67.69636 \n\nt.test(\n  final ~ year,\n  data = exams,\n  var.equal=T\n)\n\n\n    Two Sample t-test\n\ndata:  final by year\nt = -0.43015, df = 54, p-value = 0.6688\nalternative hypothesis: true difference in means between group 3rd Year and group 4th Year is not equal to 0\n95 percent confidence interval:\n -11.454430   7.407547\nsample estimates:\nmean in group 3rd Year mean in group 4th Year \n              65.67292               67.69636 \n\n\nWe can also view a two sample t test as a linear regression of the response variable (final exam grade) on the dummy variable (student year). The regression equation becomes \\[\n  (\\text{final exam grade}) = \\beta_0 + \\beta_1(\\text{student year}) + \\varepsilon\n\\] What we can see from the output below is that this model is equivalent to the two sample t test assuming equal (homogeneous) variances among the two samples. In the end, there is no noticable difference between the performance of 3rd and 4th year students.\n\n# A two-sample test\nmd.year = lm(\n  final ~ year,\n  data = exams\n)\nsummary(md.year)\n\n\nCall:\nlm(formula = final ~ year, data = exams)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.700 -14.444   1.318  12.550  36.322 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    65.673      3.611   18.19   &lt;2e-16 ***\nyear4th Year    2.023      4.704    0.43    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.32 on 54 degrees of freedom\nMultiple R-squared:  0.003415,  Adjusted R-squared:  -0.01504 \nF-statistic: 0.185 on 1 and 54 DF,  p-value: 0.6688\n\n\nHowever, the lm() function allows us to fit more complex models than just a two sample comparison. Here, we use the students performance on the written homework, the online coding homework, and the midterm exam as predictors of performance on the final exam. The result below is that the midterm mark is a strongly significant predictor of the final exam mark with an estimated coefficient of 0.953 and that the written assignment mark is a weak but possibly still relevant predictor of the final exam mark.\n\n# Fit Linear Model\nmd.exams = lm(\n  final ~ year+written+online+midterm,\n  data = exams\n)\nsummary(md.exams)\n\n\nCall:\nlm(formula = final ~ year + written + online + midterm, data = exams)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.2987  -7.0874  -0.8767   6.4742  31.3874 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -14.3982    12.6807  -1.135   0.2615    \nyear4th Year   1.5523     3.0561   0.508   0.6137    \nwritten        0.3707     0.1962   1.889   0.0646 .  \nonline        -0.2024     0.2143  -0.944   0.3494    \nmidterm        0.9527     0.1113   8.558 1.97e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.24 on 51 degrees of freedom\nMultiple R-squared:  0.6032,    Adjusted R-squared:  0.5721 \nF-statistic: 19.39 on 4 and 51 DF,  p-value: 9.478e-10\n\n\nWe can also consider the simpler regression model that only takes the midterm mark into account as a predictor variable. The fitted model has a slope parameter of 0.938 and an intercept not significantly different from zero. This indicates that the student’s final exam mark was on average \\(0.938\\times(\\text{midterm mark})\\), or simply that students had slightly lower grades on average on the final exam compared to the midterm exam. The cyan coloured region indicates those who had a higher midterm mark than final exam mark.\n\nmd.exams0 = lm(\n  final ~ midterm,\n  data = exams\n)\nsummary(md.exams0)\n\n\nCall:\nlm(formula = final ~ midterm, data = exams)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.758  -8.683  -2.942   7.620  31.621 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.8376     8.1986   0.102    0.919    \nmidterm       0.9381     0.1144   8.201 4.68e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.58 on 54 degrees of freedom\nMultiple R-squared:  0.5547,    Adjusted R-squared:  0.5464 \nF-statistic: 67.25 on 1 and 54 DF,  p-value: 4.684e-11\n\nplot(\n  exams$midterm,exams$final,las=1,\n  xlab=\"midterm mark\",ylab=\"final mark\"\n)\npolygon(\n  c(0,200,200),c(0,200,0),col = 'lightcyan',\n  border = NA\n)\npolygon(\n  c(0,0,200),c(0,200,200),col = 'white',\n  border = NA\n)\nabline(md.exams0,col='red',lwd=2)\npoints(exams$midterm,exams$final)\n\n\n\n\n\n\n\n\nWe can now imagine a hypothetical student in their 4th year who got a 95% on the written assignemnts, 85% on the online assignemnts, and an 83% on the midterm exam. Using the predict() function in R, we get an expected final exam grade of 84.24%. Going further, a 95% prediction interval for a new value gives a very wide range from 60.3% to 108.17%. This is where the model believes the final exam grade will lie for our hypothetical student.\n\npredict(\n  md.exams,\n  newdata=data.frame(\n    year=\"4th Year\",written=95,online=85,midterm=83\n  ), interval=\"prediction\",level=0.95\n)\n\n       fit      lwr      upr\n1 84.24118 60.30866 108.1737\n\n\nWe can use the anova() function to compare nested models, which will perform a partial F-test. In this example, we can compare the model that only includes the year variable to a model that contains model year and midterm to a model that contains all four predictor variables.\nIn the ANOVA table below, we have that comparing model 1 to model 2 results is a very significant p-value indicating that including midterm as a predictor results in a large decrease in the residual sum of squares (column 2 in the table below). Secondly, we compare model 2 to model 3 and get a weak but possibly significant p-value of 0.0597. This gives some weak evidence that including written and online assignment marks may further improve the fit of the model.\nNote that the degrees of freedom for the partial F-tests performed can be found in columns 1 and 3 in the ANOVA table. These are the degrees of freedom for the residual sum of sequares (column 1) and the explained sum of squares (column 3).\n\nmd.exams1 = lm(\n  final ~ year+midterm,\n  data = exams\n)\nanova(md.year,md.exams1,md.exams)\n\nAnalysis of Variance Table\n\nModel 1: final ~ year\nModel 2: final ~ year + midterm\nModel 3: final ~ year + written + online + midterm\n  Res.Df     RSS Df Sum of Sq       F    Pr(&gt;F)    \n1     54 16195.2                                   \n2     53  7200.8  1    8994.4 71.1454 3.058e-11 ***\n3     51  6447.5  2     753.3  2.9792   0.05974 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf we incorrectly try to compare non-nested models, we get erroneous results in our ANOVA table.\n\nmd.exams2 = lm(\n  final ~ year+written+online,\n  data = exams\n)\nanova(md.exams1,md.exams2)\n\nAnalysis of Variance Table\n\nModel 1: final ~ year + midterm\nModel 2: final ~ year + written + online\n  Res.Df     RSS Df Sum of Sq F Pr(&gt;F)\n1     53  7200.8                      \n2     52 15706.6  1   -8505.8         \n\n\nThe general equation for a linear regression is \\(Y=X\\beta+\\varepsilon\\). To recover the design matrix \\(X\\), we can use the function model.matrix() in R.\nNotice that the column corresponding to year is now encoded as a binary variable with a 1 indicating 4th year and a 0 indicating 3rd year.\n\nmodel.matrix(md.exams)\n\n   (Intercept) year4th Year   written    online   midterm\n1            1            1  80.66800  83.84127  70.00000\n2            1            0  96.66800 100.00000  50.00000\n3            1            1  90.00000  93.73737  50.00000\n4            1            0  98.00000  98.57143  56.66667\n5            1            0  46.00000  48.72727  50.00000\n6            1            1  75.33400  96.34921  93.33333\n7            1            0  82.66600  89.77778 100.00000\n8            1            0  80.33400  96.34921  60.00000\n9            1            0  96.66600  96.34921  56.66667\n10           1            0  82.66600  87.46609  70.00000\n11           1            1  72.66667 100.00000  56.66667\n12           1            0  95.33600  98.57143  63.33333\n13           1            1  98.00000 100.00000  93.33333\n14           1            0  92.00000 100.00000  88.33333\n15           1            1  63.33200  74.66667  63.33333\n16           1            1  98.00200  98.57143  76.66667\n17           1            1  86.00000  85.73737  60.00000\n18           1            1 100.00000  97.77778  73.33333\n19           1            1  96.66600  98.57143  60.00000\n20           1            1 100.00000  93.10245  81.66667\n21           1            1  97.33400  98.57143  46.66667\n22           1            0  98.66600 100.00000  98.33333\n23           1            1  96.00000 100.00000  80.00000\n24           1            1  95.33400  97.77778  60.00000\n25           1            0  85.33400  94.53102  66.66667\n26           1            1  77.33267  93.10245  81.66667\n27           1            1  94.00000  97.14286  87.96667\n28           1            1  96.00000 100.00000  70.00000\n29           1            1  86.66600  93.14286  73.33333\n30           1            1  82.66400  88.92064  70.00000\n31           1            0  85.33400  91.55556  83.33333\n32           1            0  86.66533  94.34921  96.66667\n33           1            1  99.33400 100.00000  63.33333\n34           1            0  91.33400  95.95960  70.00000\n35           1            0  75.33400  93.73737  76.66667\n36           1            0  76.66600  87.06205  80.00000\n37           1            1  60.62733  92.69841  71.66667\n38           1            1  85.33400  92.34921  66.66667\n39           1            1  74.66800  79.79221  60.00000\n40           1            1  92.00000  98.00000  78.70000\n41           1            1  70.66400  88.34921  56.66667\n42           1            1  93.33400 100.00000  71.66667\n43           1            0  98.66800  85.44012  63.33333\n44           1            1  97.33400 100.00000  56.66667\n45           1            0  71.99800  93.77778  63.33333\n46           1            1  96.66600  95.77778  61.66667\n47           1            1  98.66600 100.00000  90.00000\n48           1            1   0.00000   0.00000  93.33333\n49           1            1  99.33400 100.00000  66.66667\n50           1            0  97.33400  98.57143  60.00000\n51           1            1  95.33400 100.00000  76.66667\n52           1            0  94.66733 100.00000  70.00000\n53           1            0  93.33200  98.57143  63.33333\n54           1            0  87.33200  95.77778  80.00000\n55           1            1  93.33400  96.00000  66.66667\n56           1            0  70.00000  81.77778  46.66667\nattr(,\"assign\")\n[1] 0 1 2 3 4\nattr(,\"contrasts\")\nattr(,\"contrasts\")$year\n[1] \"contr.treatment\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt1_ols.html#footnotes",
    "href": "chpt1_ols.html#footnotes",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "Messerli, F. H. (2012). Chocolate consumption, cognitive function, and Nobel laureates. New England Journal of Medicine, 367(16), 1562-1564.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "chpt2_ModAss.html",
    "href": "chpt2_ModAss.html",
    "title": "2  Model Assumptions and Choices",
    "section": "",
    "text": "2.1 Introduction\nThus far, we have considered linear regression given some key assumptions. Namely, for models of the form \\[\n  y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p + \\varepsilon\n\\] we assume that the noise or errors \\(\\varepsilon\\) have zero mean, constant finite variance, and are uncorrelated. Furthermore, in order to perform hypothesis tests–F test, t test, partial F-tests–and to construct confidence and prediction intervals as we did in the previous chapter, we further assumes that \\(\\varepsilon\\) has a normal distribution.\nIn this chapter, we will consider deviations from these assumptions, which will lead to questions such as",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Assumptions and Choices</span>"
    ]
  },
  {
    "objectID": "chpt2_ModAss.html#introduction",
    "href": "chpt2_ModAss.html#introduction",
    "title": "2  Model Assumptions and Choices",
    "section": "",
    "text": "What happens if the variance of \\(\\varepsilon\\) is not constant?\nWhat happens if \\(\\varepsilon\\) has heavier tails than those of a normal distribution?\nWhat effect can outliers have on our model?\nHow can we transform our data to correct for some of these deviations?\nWhat happens if the true model is not linear?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Assumptions and Choices</span>"
    ]
  },
  {
    "objectID": "chpt2_ModAss.html#plotting-residuals",
    "href": "chpt2_ModAss.html#plotting-residuals",
    "title": "2  Model Assumptions and Choices",
    "section": "2.2 Plotting Residuals",
    "text": "2.2 Plotting Residuals\nIn the last chapter, we constructed the residuals as follows. Assume we have a sample of \\(n\\) observations \\((Y_i,X_i)\\) where \\(Y_i\\in\\mathbb{R}\\) and \\(X_i\\in\\mathbb{R}^p\\) with \\(p\\) being the number of regressors and \\(p&lt;n\\). The model, as before, is \\[\n  Y = X\\beta + \\varepsilon\n\\] where \\(Y\\in\\mathbb{R}^n\\), \\(X\\in\\mathbb{R}^{n\\times (p+1)}\\), \\(\\beta\\in\\mathbb{R}^{p+1}\\), and \\(\\varepsilon\\in\\mathbb{R}^n\\). The least squares estimator is \\(\\hat{\\beta} = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\) and the vector of residuals is thus \\[\n  r = (I-P)Y = (I-X({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T})Y.\n\\]\nWe can use the residuals to look for problems in our data with respect to deviations from the assumptions. But first, they should be normalized in some way. As we know from the previous chapter, the covariance of the residuals is \\((I-P)\\sigma^2\\) and that \\(SS_\\text{res}/(n-p-1)\\) is an unbiased estimator for the unknown variance \\(\\sigma^2\\). This implies that while the errors \\(\\varepsilon_i\\) are assumed to be uncorrelated, the residuals are, in fact, correlated. Explicitly, \\[\n  \\mathrm{Var}\\left(r_i\\right) = (1 - P_{i,i})\\sigma^2,\n  \\text{ and }\n  \\mathrm{cov}\\left(r_i,r_j\\right) = -P_{i,j}\\sigma^2\n  \\text{ for } i\\ne j\n\\] where \\(P_{i,j}\\) is the \\((i,j)\\)th entry of the matrix \\(P\\).\nHence, a standard normalization technique is to write \\[\n  s_i = \\frac{r_i}{\\sqrt{(1-P_{i,i})SS_\\text{res}/(n-p-1)}},\n\\] which are denoted as the studentized residuals. For a linear model fit in R by the function lm(), we can extract the residuals with resid() and extract the studentized residuals with rstudent().\n\n2.2.1 Plotting Residuals\n\n2.2.1.1 Studentized Residuals\nA plot of studentized residuals from a simple linear regression is displayed below. Generally, abnormally large studentized residuals indicate that an observation may be an outlier.\n::: {#rem-residuals} There are other types of residuals that can be computed such as standardized residuals, PRESS residuals, and externally studentized residuals. These are also used to look for outliers. :::\n\nset.seed(128)\n# Simulate some linear regression data\nxx = runif(n=50,min=0,max=5)\nyy = 3*xx + rnorm(n=50,0,2)\n# Add in one outlier\nxx &lt;- c(xx, 3)\nyy &lt;- c(yy,-2)\n# Fit a linear regression\nmd = lm(yy~xx)\nsummary(md)\n\n\nCall:\nlm(formula = yy ~ xx)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0377  -1.2028   0.3087   1.4941   3.8238 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.3412     0.7148  -0.477    0.635    \nxx            3.1263     0.2400  13.028   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.549 on 49 degrees of freedom\nMultiple R-squared:  0.776, Adjusted R-squared:  0.7714 \nF-statistic: 169.7 on 1 and 49 DF,  p-value: &lt; 2.2e-16\n\n# Plot the data\nplot(xx[1:50],yy[1:50],las=1,ylim=c(-3,16),xlab=\"X\",ylab=\"Y\");\npoints(xx[51],yy[51],pch=17,col='blue')\nabline(md,col='red')\n\n\n\n\n\n\n\n# Plot Residuals\nres = rstudent(md)\nplot(\n  xx[1:50],res[1:50],las=1,ylim=c(-6,2.5),\n  xlab=\"X\",ylab=\"Studentized Residuals\"\n);\npoints(xx[51],res[51],pch=17,col='blue')\nabline(h=(-10):10,col='gray')\nabline(h=0,col='red')\n\n\n\n\n\n\n\n\n\n\n2.2.1.2 Residuals vs Fitted Values\nThe residuals and the fitted values as necessarily uncorrelated. That is, \\(\\mathrm{cov}\\left(r,\\hat{Y}\\right)=0\\). However, if \\(\\varepsilon\\) is not normally distributed, then they may not be independent. Plotting the fitted values against the residuals can give useful diagnostic information about your data.\nThe code below gives four examples of plots of the residuals against the fitted values. The first plot in the top left came from a simple linear regression model where all of the standard assumptions are met. The second plot in the top right came from a simple linear regression but with errors \\(\\varepsilon_i \\sim\\mathcal{N}\\left(0,\\sigma^2_i\\right)\\) where \\(\\sigma_i^2\\) was increasing in \\(i\\). Hence, the plot has an expanding look to it. The third plot in the bottom left came from a simple linear regression with the addition of a quadratic term. Fitting a model without the quadratic term still yielded significant test statistics, but failed to account for the nonlinear interaction between \\(x\\) and \\(y\\). The final plot in the bottom right came from a simple linear regression where the errors were correlated. Specifically, \\(\\mathrm{cov}\\left(\\varepsilon_i,\\varepsilon_j\\right) = \\min\\{x_i,x_j\\}\\).\n\nFun fact: This is actually the covariance of Brownian motion.\n\n\n# remove the outlier from the last code block\nxx &lt;- xx[1:50]\nset.seed(256)\n# Create a \"good\" linear regression\nyy.lin  = 3*xx + 2 + rnorm(50,0,1);\nmd.lin  = lm(yy.lin ~ xx)\nplot(md.lin,which=1,las=1)\n\n\n\n\n\n\n\n# Create a linear regression with increasing variance\nyy.het  = 3*xx + 2 + rnorm(50,0,xx);\nmd.het  = lm(yy.het ~ xx)\nplot(md.het,which=1,las=1)\n\n\n\n\n\n\n\n# Create a linear regression with quadratic trend\nyy.quad = xx^2 + 3*xx + 2 + rnorm(50,0,1);\nmd.quad = lm(yy.quad ~ xx)\nplot(md.quad,which=1,las=1)\n\n\n\n\n\n\n\n# Create a linear regression with correlated errors\nerr = rnorm(50,0,1);\nerr &lt;- cumsum(err);\nyy.cor = 3*xx + 2 + err;\nmd.cor  = lm(yy.cor ~ xx)\nplot(md.cor,which=1,las=1)\n\n\n\n\n\n\n\nplot(\n  err,type='l',las=1,\n  main=\"correlated errors, see Time Series Analysis\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.1.3 Normal Q-Q plots\nAnother type of plot that can offer insight into your data is the so-called Normal Q-Q plot. This tool plots the studentized residuals against the quantiles of a standard normal distribution.\n\nDefinition 2.1 The quantile function is the inverse of the cumulative distribution function. That is, in the case of the normal distribution, let \\(Z\\sim\\mathcal{N}\\left(0,1\\right)\\). Then the CDF is \\(\\Phi(z) = \\mathrm{P}\\left( Z&lt; z \\right) \\in(0,1)\\) for \\(z\\in\\mathbb{R}\\). The quantile function is \\(\\Phi^{-1}(t)\\in[-\\infty,\\infty]\\) for \\(t\\in[0,1]\\). For more details, see QQ Plot\n\nFor a normal Q-Q plot, let \\(s_1,\\ldots,s_n\\) be the ordered studentized residuals, so that \\(s_1\\le\\ldots\\le s_n\\). The theoretical quantiles are denoted \\(q_1,\\ldots,q_n\\) where \\(q_i = \\Phi^{-1}( i/(n+1) )\\). In R, a slightly different formula is used. The figures below compare various normal Q-Q plots. In the first, the errors are normally distributed and the ordered residuals roughly follow the red line. In the second, the heteroskedastic errors cause the black points to deviate from the red line. This is also seen in the third plot, which fits a linear model to quadratic data. The fourth plot considers correlated errors and the QQ-plot has black points very close to the red line.\n\nRemark 2.1. As noted in Montgomery, Peck, & Vining, these are not always easy to interpret and often fail to capture non-normality in the model. However, they seem to be very popular nonetheless.\n\n\n# plot the above residuals in QQ-plots \n# against the normal distribution\nqqnorm(rstudent(md.lin))\nabline(0,1,col='red')\n\n\n\n\n\n\n\nqqnorm(rstudent(md.het))\nabline(0,1,col='red')\n\n\n\n\n\n\n\nqqnorm(rstudent(md.quad))\nabline(0,1,col='red')\n\n\n\n\n\n\n\nqqnorm(rstudent(md.cor))\nabline(0,1,col='red')\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 Beer Data Example\nThe following section considers a dataset that tracks a person’s blood alcohol content after consuming some number of beers. Two covariates in this dataset are the subjects sex and weight. I got this dataset in 2018 from a statistician I met at ICOTS 2018, because, yes, statisticians trade datasets at conferences.\n\nbac = read.csv(\"data/BACfull.csv\")\nbac\n\n     BAC weight    sex beers\n1  0.100    132 female     5\n2  0.030    128 female     2\n3  0.190    110 female     9\n4  0.120    192   male     8\n5  0.040    172   male     3\n6  0.095    250 female     7\n7  0.070    125 female     3\n8  0.060    175   male     5\n9  0.020    175 female     3\n10 0.050    275   male     5\n11 0.070    130 female     4\n12 0.100    168   male     6\n13 0.085    128 female     5\n14 0.090    246   male     7\n15 0.010    164   male     1\n16 0.050    175   male     4\n\n\nWe can fit a simple regression model that only considers the number of beers consumed as the sole predictor for blood alcohol content. The resulting model estimates that each beer raises a subjects blood alcohol content by 0.018.\nIn the residual vs fitted plot, the subject with the largest residual (person 3) was also the subject with the lowest weight and most beers consumed. Of course, the following simple regression model does not consider weight as a predictor variable.\n\nmd.bac1 &lt;- lm( BAC~beers, data=bac  )\nsummary(md.bac1)\n\n\nCall:\nlm(formula = BAC ~ beers, data = bac)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.027118 -0.017350  0.001773  0.008623  0.041027 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.012701   0.012638  -1.005    0.332    \nbeers        0.017964   0.002402   7.480 2.97e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02044 on 14 degrees of freedom\nMultiple R-squared:  0.7998,    Adjusted R-squared:  0.7855 \nF-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06\n\nplot(md.bac1,which=1,las=1)\n\n\n\n\n\n\n\n\nWe can include sex as a predictor variable which results in the model below. In this case, we see a slight significant in the sex variable, which seems to suggest that males have on average a lower blood alcohol content than females by 0.02. The three subjects with the largest (absolute) residuals are subject 3, the female with the lowest overall weight, and subjects 6 and 9, the females with the highest overall weights. Once again, we note that without considering a subject’s weight in this regression model, the largest residuals (i.e. those points that most deviate from our fitted model) come from subjects with the largest and smallest weights.\n\nmd.bac2 &lt;- lm( BAC~beers+sex, data=bac  )\nsummary(md.bac2)\n\n\nCall:\nlm(formula = BAC ~ beers + sex, data = bac)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0308247 -0.0088126 -0.0003627  0.0133905  0.0305743 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.003476   0.012004  -0.290   0.7767    \nbeers        0.018100   0.002135   8.478 1.18e-06 ***\nsexmale     -0.019763   0.009086  -2.175   0.0487 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01816 on 13 degrees of freedom\nMultiple R-squared:  0.8532,    Adjusted R-squared:  0.8307 \nF-statistic: 37.79 on 2 and 13 DF,  p-value: 3.826e-06\n\nplot(md.bac2,which=1,las=1)\n\n\n\n\n\n\n\n\nFinally, we also include weight as a predictor variable. As a result, the sex variable is no longer seen to have any statistical significance in determining blood alcohol content. We note that the range of the residuals has decreased from the previous models meaning that this last model provides a tighter fit to the data.\n\nmd.bac3 &lt;- lm( BAC~beers+sex+weight, data=bac  )\nsummary(md.bac3)\n\n\nCall:\nlm(formula = BAC ~ beers + sex + weight, data = bac)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.018125 -0.005713  0.001501  0.007896  0.014655 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.871e-02  1.097e-02   3.528 0.004164 ** \nbeers        1.990e-02  1.309e-03  15.196 3.35e-09 ***\nsexmale     -3.240e-03  6.286e-03  -0.515 0.615584    \nweight      -3.444e-04  6.842e-05  -5.034 0.000292 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01072 on 12 degrees of freedom\nMultiple R-squared:  0.9528,    Adjusted R-squared:  0.941 \nF-statistic: 80.81 on 3 and 12 DF,  p-value: 3.162e-08\n\nplot(md.bac3,which=1,las=1)\n\n\n\n\n\n\n\n\nThe significance of sex is the second model was only due to the males in this dataset having, on average, higher weight than the females, which can be seen in the boxplot below. Thus, these two predictor variables are confounded or correlated. This is not seen in the case of beers consumed vs sex. For our two numeric variables, beers consumed and weight, there is a slight positive correlation of about 0.25.\n\nbac0 &lt;- bac\nbac0$sex &lt;- as.numeric(as.factor(bac0$sex))\ncor(bac0[,2:4])\n\n          weight        sex      beers\nweight 1.0000000 0.51282368 0.24887716\nsex    0.5128237 1.00000000 0.02937367\nbeers  0.2488772 0.02937367 1.00000000\n\nboxplot(weight~sex,data=bac,las=1)\n\n\n\n\n\n\n\nboxplot(beers~sex,data=bac,las=1)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Assumptions and Choices</span>"
    ]
  },
  {
    "objectID": "chpt2_ModAss.html#transformation",
    "href": "chpt2_ModAss.html#transformation",
    "title": "2  Model Assumptions and Choices",
    "section": "2.3 Transformation",
    "text": "2.3 Transformation\nOften, data does not follow all of the assumptions of the ordinary least squares regression model. However, it is often possible to transform data in order to correct for this deviations. We will consider such methods in the following subsections. One dissatisfying remark about such methods is that they often are applied empirically, which less euphemistically means in an add-hoc way. Sometimes there may be genuine information suggesting certain methods for transforming data. Other times, transforms are chosen because they seem to work.\n\n2.3.1 Variance Stabilizing\nOne of the major requirements of the least squares model is that the variance of the errors is constant, which is that \\(\\mathrm{Var}\\left(y_i\\right) = \\mathrm{Var}\\left(\\varepsilon_i\\right) = \\sigma^2\\) for \\(i=1,\\ldots,n\\). Mainly, when \\(\\sigma^2\\) is non-constant in \\(y\\), problems can occur. Our goal is thus to find some transformation \\(T(y)\\) so that \\(\\mathrm{Var}\\left(T(y_i)\\right)\\) is constant for all \\(i=1,\\ldots,n\\).\nSuch a transformation \\(T(\\cdot)\\) can be determined through a tool known as the delta method, which is beyond the scope of these notes.\nSee Chapter 3, Asymptotic Statistics, A W van der Vaart or Delta Method for more. However, we will consider a simplified version for our purposes. For simplicity of notation, we write \\(\\mathrm{E}Y = \\mu\\) instead of \\(\\mathrm{E}Y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p\\). Furthermore, assume that \\(T\\) is twice differentiable. Note that we only require the second derivative for a more pleasant exposition. Then, Taylor’s theorem says that \\[\n  T(Y) = T(\\mu) + T'(\\mu)(Y-\\mu) + \\frac{T''(\\xi)}{2}(Y-\\mu)^2\n\\] for some \\(\\xi\\) between \\(Y\\) and \\(\\mu\\). We can, with a little hand-waving, ignore the higher order remainder term and just write \\[\n  T(Y) \\approx T(\\mu) + T'(\\mu)(Y-\\mu),\n\\] which implies that \\[\n  \\mathrm{E}T(Y) \\approx T(\\mu)\n  ~~\\text{ and }~~\n  \\mathrm{Var}\\left(T(Y)\\right) \\approx T'(\\mu)^2\\mathrm{Var}\\left(Y\\right).\n\\] We want a transformation such that \\(\\mathrm{Var}\\left(T(Y)\\right) = 1\\) is constant. Meanwhile, we assume that the variance of \\(Y\\) is a function of the mean \\(\\mu\\), which is \\(\\mathrm{Var}\\left(Y\\right) = s(\\mu)^2\\). Hence, we need to solve \\[\n  1 = T'(\\mu)^2 s(\\mu)^2\n  ~~\\text{ or }~~\n  T(\\mu) = \\int \\frac{1}{s(\\mu)}d\\mu.\n\\]\n\nExample 2.1 For a trivial example, assume that \\(s(\\mu)=\\sigma\\) is already constant. Then, \\[\n    T(\\mu) = \\int \\frac{1}{\\sigma}d\\mu = \\mu/\\sigma.\n  \\] Thus, \\(T\\) is just scaling by \\(\\sigma\\) to achieve a unit variance.\n\n\nExample 2.2 Now, consider the nontrivial example with \\(s(\\mu) = \\sqrt{\\mu}\\), which is that the variance of \\(Y\\) is a linear function of \\(\\mu\\). Then, \\[\n    T(\\mu) = \\int \\frac{1}{\\sqrt{\\mu}} d\\mu\n    = 2 \\sqrt{\\mu}.\n  \\] This is the square root transformation, which is applied to, for example, Poisson data. The coefficient of 2 in the above derivation can be dropped as we are not concerned with the scaling.\n\nGiven that we have found a suitable transform, we can then apply it to the data \\(y_1,\\ldots,y_n\\) to get a model of the form \\[\n  T(y) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p + \\varepsilon\n\\] where the variance of \\(\\varepsilon\\) is constant–i.e not a function of the regressors.\n\n\n2.3.2 Linearization\nAnother key assumption is that the relationship between the regressors and response, \\(x\\) and \\(y\\), is linear. If there is reason to believe that \\[\n  y = f( \\beta_0 + \\beta_1x_1 +\\ldots+ \\beta_px_p + \\varepsilon)\n\\] and that \\(f(\\cdot)\\) is invertible, then we can rewrite this model as \\[\n  y' = f^{-1}(y) = \\beta_0 + \\beta_1x_1 +\\ldots+ \\beta_px_p + \\varepsilon\n\\] and apply our usual linear regression tools.\nAn example from the textbook, which is also quite common in practice, is to assume that \\[\n  y = c \\mathrm{e}^{ \\beta_1 x_1 + \\ldots + \\beta_px_p + \\varepsilon}\n\\] and apply a logarithm to transform to \\[\n  y' = \\log y =  \\beta_0 + \\beta_1x_1 +\\ldots+ \\beta_px_p + \\varepsilon\n\\] where \\(\\beta_0 = \\log c\\). Furthermore, if \\(\\varepsilon\\) has a normal distribution then \\(\\mathrm{e}^\\varepsilon\\) has a log-normal distribution. This model is particularly useful when one is dealing with exponential growth in some population.\n\nRemark 2.2. Linearization by applying a function to \\(y\\) looks very similar to the variance stabilizing transforms of the previous section. In fact, such transforms have an effect on both the linearity and the variance of the model and should be used with care. Often non-linear methods are preferred.\n\nSometimes it is beneficial to transform the regressors, \\(x\\)’s, as well. As we are not treating them as random variables, there are less problems to consider.\n\\[\n  ~~~~~~y = \\beta_0 + \\beta_1 f(x) + \\varepsilon\n  ~~\\Rightarrow~~\n  y = \\beta_0 + \\beta_1 x' + \\varepsilon,~~ x = f^{-1}(x')\n\\] Examples include \\[\\begin{align*}\n  y = \\beta_0 + \\beta_1 \\log x + \\varepsilon\n  ~~&\\Rightarrow~~\n  y = \\beta_0 + \\beta_1 x' + \\varepsilon,~~ x = \\mathrm{e}^{x'}\\\\\n  y = \\beta_0 + \\beta_1 x^2 + \\varepsilon\n  ~~&\\Rightarrow~~\n  y = \\beta_0 + \\beta_1 x' + \\varepsilon,~~ x = \\sqrt{x'}\n\\end{align*}\\] This second example can be alternatively dealt with by employing polynomial regression to be discussed in a subsequent section.\n\n\n2.3.3 Box-Cox and the power transform\nIn short, Box-Cox is a family of transforms parametrized by some \\(\\lambda\\in\\mathbb{R}\\), which can be optimized via maximum likelihood. Specifically, for \\(y_i&gt;0\\), we aim to choose the best transform of the form \\[\n  y_i \\rightarrow y_i^{(\\lambda)} =  \\left\\{\\begin{array}{ll}\n    \\frac{y_i^\\lambda-1}{\\lambda} & \\lambda\\ne0\\\\\n    \\log y_i & \\lambda=0\n  \\end{array}\\right.\n\\] by maximizing the likelihood as we did in Chapter 1, but with parameters \\(\\beta\\), \\(\\sigma^2\\), and \\(\\lambda\\).\nTo do this, we assume that the transformed variables follow all of the usual least squares regression assumptions and hence have a joint normal distribution with \\[\n  f( Y^{(\\lambda)} ) = (2\\pi\\sigma^2)^{-n/2}\\exp\\left(\n    -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - X_{i,\\cdot}\\beta)^2\n  \\right).\n\\] Transforming \\(Y \\rightarrow Y^{(\\lambda)}\\) is a change of variables with Jacobian \\[\n  \\prod_{i=1}^n\\frac{dy_i}{dy_i}^{(\\lambda)} = \\prod_{i=1}^ny_i^{\\lambda-1}.\n\\] Hence, the likelihood function in terms of \\(X\\) and \\(y\\) is \\[\n  L(\\beta,\\sigma^2,\\lambda|y,X) =\n  (2\\pi\\sigma^2)^{-n/2}\\exp\\left(\n    -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i^{\\lambda} - X_{i,\\cdot}\\beta)^2\n  \\right) \\prod_{i=1}^n y_i^{\\lambda-1}\n\\] with log likelihood \\[\n  \\log L =\n  -\\frac{n}{2}\\log( 2\\pi\\sigma^2 )\n  -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - X_{i,\\cdot}\\beta)^2\n  + (\\lambda-1)\\sum_{i=1}^n \\log y_i.\n\\] From here, the MLEs for \\(\\beta\\) and \\(\\sigma^2\\) are solved for as before but are now in terms of the transformed \\(Y^{(\\lambda)}\\). \\[\\begin{align*}\n  \\hat{\\beta}    &= ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y^{(\\lambda)}\\\\\n  \\hat{\\sigma}^2  \n  &= \\frac{1}{n}\\sum_{i=1}^n(y_i^{(\\lambda)}-X_{i,\\cdot}\\hat{\\beta})^2\n   = \\frac{SS_\\text{res}^{(\\lambda)}}{n}.\n\\end{align*}\\] Plugging these into the log likelihood gives and replacing all of the constants with some \\(C\\) gives \\[\\begin{align*}\n  \\log L &=\n  -\\frac{n}{2}\\log( 2\\pi\\hat{\\sigma}^2 )\n  -\\frac{n}{2} + (\\lambda-1)\\sum_{i=1}^n \\log y_i\\\\\n  &= C - \\frac{n}{2}\\log\\hat{\\sigma}^2 +\n     \\log\\left( (\\prod y_i)^{\\lambda-1}\\right).\n\\end{align*}\\] Defining the geometric mean of the \\(y_i\\) to be \\(\\gamma = (\\prod_{i=1}^n y_i)^{1/n}\\), we have \\[\\begin{align*}\n  \\log L  \n  &= C - \\frac{n}{2}\\log\\hat{\\sigma}^2 +\n     \\frac{n}{2}\\log\\left( \\gamma^{2(\\lambda-1)}\\right)\\\\\n  &= C - \\frac{n}{2}\\log\\left(\n       \\frac{\\hat{\\sigma}^2}{\\gamma^{2(\\lambda-1)}}\n     \\right).\n\\end{align*}\\] Considering the term inside the log, we have that it is just the residual sum of squares from the least squares regression \\[\n  \\frac{Y^{(\\lambda)}}{\\gamma^{\\lambda-1}} = X\\theta + \\varepsilon\n\\] where \\(\\theta\\in\\mathbb{R}^{p+1}\\) is a transformed version of the original \\(\\beta\\). Hence, we can choose \\(\\hat{\\lambda}\\) by maximizing the log likelihood above, which is equivalent to minimizing the residual sum of squares for this new model. This can be calculated numerically in statistical programs like R.\n\n\n2.3.4 Cars Data\nTo test some of these linearization techniques, we consider the cars dataset, which is included in the standard distribution of R. It consists of 50 observations of a car’s speed and a car’s stopping distance. The goal is to model and predict the stopping distance given the speed of the car. Such a study could be used, for example, for influencing speed limits and other road rules for the sake of public safety. The observed speeds range from 4 to 25 mph.\n\nplot(\n  cars,las=1,main=\"Car Stopping Distance\",\n  xlim=c(0,max(cars$speed)),\n  ylim=c(0,max(cars$dist))\n)\n\n\n\n\n\n\n\n\nWe can first fit a simple regression to the data with lm( dist~speed, data=cars). This results in a significant p-value, an \\(R^2 = 0.651\\), and an estimated model of \\[\n  (\\text{dist}) = -17.6 + 3.9(\\text{speed}) + \\varepsilon.\n\\] If we wanted to extrapolate a bit, we can use this model to predict the stopping distance for a speed of 50 mph, which is 179 feet.\n\nmd.car.lin = lm( dist~speed, data=cars)\nsummary(md.car.lin)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\npredict(md.car.lin,data.frame(speed=50))\n\n       1 \n179.0413 \n\n\nWe could stop here and be happy with a significant fit. However, looking at the data, there seems to be a nonlinear relationship between speed and stopping distance. Hence, we could try to fit a model with the response being the square root of the stopping distance: lm( sqrt(dist)~speed, data=cars). Doing so results in \\[\n  \\sqrt{(\\text{dist})} = 1.28 + 0.32(\\text{speed}) + \\varepsilon.\n\\] In this case, we similarly get a significant p-value and a slightly higher \\(R^2 = 0.709\\). The prediction for the stopping distance for a speed of 50 mph is now the much higher 302 feet.\n\nmd.car.sqrt = lm( sqrt(dist)~speed, data=cars)\nsummary(md.car.sqrt)\n\n\nCall:\nlm(formula = sqrt(dist) ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0684 -0.6983 -0.1799  0.5909  3.1534 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.27705    0.48444   2.636   0.0113 *  \nspeed        0.32241    0.02978  10.825 1.77e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.102 on 48 degrees of freedom\nMultiple R-squared:  0.7094,    Adjusted R-squared:  0.7034 \nF-statistic: 117.2 on 1 and 48 DF,  p-value: 1.773e-14\n\n# Note that we have to use ( )^2 to get our \n# prediction on the right scale\npredict(md.car.sqrt,data.frame(speed=50))^2\n\n       1 \n302.6791 \n\n\nWe can further apply a log-log transform with lm( log(dist)~log(speed), data=cars). This results in an even higher \\(R^2=0.733\\). The fitted model is \\[\n  \\log(y) = -0.73 + 1.6 \\log(x) + \\varepsilon,\n\\] and the predicted stopping distance for a car travelling at 50 mph is 254 feet. Note that the square root transform is modelling \\(y \\propto x^2\\) whereas the log-log transform is modelling \\(y \\propto x^{1.6}\\), which is a slower rate of increase.\n\nmd.car.log = lm( log(dist)~log(speed), data=cars)\nsummary(md.car.log)\n\n\nCall:\nlm(formula = log(dist) ~ log(speed), data = cars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00215 -0.24578 -0.02898  0.20717  0.88289 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.7297     0.3758  -1.941   0.0581 .  \nlog(speed)    1.6024     0.1395  11.484 2.26e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4053 on 48 degrees of freedom\nMultiple R-squared:  0.7331,    Adjusted R-squared:  0.7276 \nF-statistic: 131.9 on 1 and 48 DF,  p-value: 2.259e-15\n\n# Note that we have to use exp( ) to get our \n# prediction on the right scale\nexp( predict(md.car.log,data.frame(speed=50)) )\n\n       1 \n254.4037 \n\n\nWe can also let the data decide on the best transformation by using the Box-Cox transformation. In this case, we may want to test the hypothesis that the optimal power to transform with is 1/2; i.e. the square root transformation. The Box-Cox transform choose a power of \\(\\lambda=0.424\\). However, the value 1/2 lies within the confidence interval for \\(\\lambda\\). We may prefer to use the square root transform instead of the power 0.424, since the square root is more interpretable.\n\n# Need the package MASS for boxcox\nlibrary(MASS)\n# Do the Box-Cox transform\nbc  = boxcox(md.car.lin);\nabline(v=c(0.5),col=c('red'))\n\n\n\n\n\n\n\nlmb = bc$x[ which.max(bc$y) ];\nprint(paste(\"Optimal lambda is \",lmb))\n\n[1] \"Optimal lambda is  0.424242424242424\"\n\ncarsBC = cbind(cars, distBC = (cars$dist^lmb-1)/lmb)\nmd.car.bc = lm( distBC~speed, data=carsBC );\nsummary(md.car.bc);\n\n\nCall:\nlm(formula = distBC ~ speed, data = carsBC)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0926 -1.0444 -0.3055  0.7999  4.7520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08227    0.73856   1.465    0.149    \nspeed        0.49541    0.04541  10.910 1.35e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 48 degrees of freedom\nMultiple R-squared:  0.7126,    Adjusted R-squared:  0.7066 \nF-statistic:   119 on 1 and 48 DF,  p-value: 1.354e-14\n\n\nThe four models considered are plotted below. As we move away from the range of the regressors–i.e 4 to 25 mph–the models begin to diverge making extrapolating a bit dangerous.\n\nplot(\n  cars,las=1,main=\"Car Stopping Distance\",\n  xlim=c(0,40),\n  ylim=c(0,160)\n)\nxx = 0:40\nlines( \n  xx, predict(md.car.lin,data.frame(speed=xx)),  \n  col=1,lty=1,lwd=2\n)\nlines( \n  xx, predict(md.car.sqrt,data.frame(speed=xx))^2,  \n  col=2,lty=2,lwd=2\n)\nlines( \n  xx, exp(predict(md.car.log,data.frame(speed=xx))),  \n  col=3,lty=3,lwd=2\n)\nlines( \n  xx, (lmb*predict(md.car.bc,data.frame(speed=xx))+1)^(1/lmb),  \n  col=4,lty=4,lwd=2\n)\nlegend(\n  \"topleft\",legend = c(\"Linear\",\"Square Root\",\"Log-Log\",\"Box-Cox\"),\n  col=1:4,lty=1:4,lwd=2\n)\n\n\n\n\n\n\n\n\nThe choice in transformation can also have a big effect on both predictions and the confidence surronding such predictions.\n\nnew.val = 40;\npredict(md.car.lin,data.frame(speed=new.val),interval=\"prediction\")\n\n       fit      lwr      upr\n1 139.7173 102.3311 177.1034\n\npredict(md.car.sqrt,data.frame(speed=new.val),interval=\"prediction\")^2\n\n       fit      lwr      upr\n1 200.8895 132.1058 284.0361\n\nexp(predict(md.car.log,data.frame(speed=new.val),interval=\"prediction\"))\n\n       fit      lwr      upr\n1 177.9245 74.39848 425.5077\n\n(lmb*predict(\n  md.car.bc,data.frame(speed=new.val),interval=\"prediction\"\n)+1)^(1/lmb)\n\n      fit      lwr      upr\n1 220.465 139.8189 322.8649",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Assumptions and Choices</span>"
    ]
  },
  {
    "objectID": "chpt2_ModAss.html#polynomial-regression",
    "href": "chpt2_ModAss.html#polynomial-regression",
    "title": "2  Model Assumptions and Choices",
    "section": "2.4 Polynomial Regression",
    "text": "2.4 Polynomial Regression\nIn the following subsections, we will only consider models with a single regressor \\(x\\in\\mathbb{R}\\) until the final part below where we will consider polynomials in multiple variables \\(x_1,\\ldots,x_p\\).\nOne of the examples of atypical residual behavior from above occurs when there is an unaccounted for quadratic term in the model such as trying to fit a model of the form \\[\n  y = \\beta_0 + \\beta_1x_1 +\\varepsilon\n\\] to data generated by \\[\n  y = \\beta_0 + \\beta_1x_1^2 + \\varepsilon.\n\\] If this is suspected, we can look at the residuals and try to transform \\(x\\), such as \\(x \\rightarrow \\sqrt{x}\\), in order to put this back into the linear model framework. However, we can also just fit a polynomial model to our data.\nConsider the setting where we observe \\(n\\) data pairs \\((y_i,x_i)\\in\\mathbb{R}^2\\). We can then attempt to fit a model of the form \\[\n  y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_px^p + \\varepsilon\n\\] to the observations. The \\(n\\times p\\) design matrix will look like \\[\n  X = \\begin{pmatrix}\n    1 & x_1 & x_1^2 & \\ldots & x_1^p \\\\\n    1 & x_2 & x_2^2 & \\ldots & x_2^p \\\\\n    \\vdots & \\vdots & \\vdots &\\ddots & \\vdots\\\\\n    1 & x_n & x_n^2 & \\ldots & x_n^p\n  \\end{pmatrix}\n\\] and the parameters can be estimated as usual: \\(\\hat{\\beta} = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\). This matrix arises in Linear Algebra as the Vandermonde matrix.\n\nRemark 2.3. While the columns of \\(X\\) are, in general, linearly independent, as \\(p\\) gets larger, many problems can occur. In particular, the columns become near linearly dependent resulting in instability when computing \\(({X}^\\mathrm{T}X)^{-1}\\).\n\n\n2.4.1 Model Problems\nPolynomial regression is very powerful for modelling, but can also lead to very erroneous results if used incorrectly. What follows are some potential issues to take into consideration.\n\n2.4.1.1 Overfitting\nAs a general rule, the degree of the polynomial model should be kept as low as possible. High order polynomials can be misleading as they can often fit the data quite well. In fact, given \\(n\\) data points, it is possible to fit an \\(n-1\\) degree polynomial that passes through each data point. In this extreme case, all of the residuals would be zero, but we would never expect this to be the correct model for the data.\nProblems can occur even when \\(p\\) is much smaller than \\(n\\). As an example, two models were fit to \\(n=50\\) data points generated from the model \\[\n  y = 3x^2 + \\varepsilon\n\\] with \\(\\varepsilon\\sim\\mathcal{N}\\left(0,1\\right)\\). The first was a cubic model. The second was a degree 20 model. The first regression resulted in three significant regressors. Note that in these two cases, orthogonal polynomials were used to maintain numerical stability.\n\nset.seed(128)\n# Simulate some regression data with\n# a quadratic trend\nxx  = seq(0,2,0.05)\nlen = length(xx)\nyy  = 3*xx^2 + rnorm(n=len,0,1)\n# Fit a cubic model using orthogonal polynomials\n# and extract the most significant terms\nmd3  &lt;- lm( yy~poly(xx,3) )\nsm3  &lt;- summary(md3)\nsm3$coefficients[which(sm3$coefficients[,4]&lt;0.01),]\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   4.115491  0.1730479 23.782384 5.118095e-24\npoly(xx, 3)1 21.730420  1.1080471 19.611459 3.843069e-21\npoly(xx, 3)2  8.025826  1.1080471  7.243217 1.347334e-08\n\n# Fit a degree-20 model using orthogonal polynomials\n# and extract the most significant terms\nmd20 &lt;- lm( yy~poly(xx,20) )\nsm20&lt;- summary(md20)\nsm20$coefficients[which(sm20$coefficients[,4]&lt;0.01),]\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    4.115491  0.1944119 21.168925 3.641662e-15\npoly(xx, 20)1 21.730420  1.2448436 17.456345 1.424531e-13\npoly(xx, 20)2  8.025826  1.2448436  6.447256 2.748160e-06\n\n# Plot the data and models\nplot( xx,yy, xlab='X',ylab=\"Y\",las=1 )\nlines( xx, 3*xx^2,col='black',lty=1 )\nlines( xx, predict(md3),col='red',lty=2,lwd=2 )\nlines( xx, predict(md20),col='blue',lty=2,lwd=2 )\nlegend(\n  \"topleft\",legend = c(\"Truth\",\"Cubic\",\"Deg20\"),\n  col = c(\"black\",\"red\",\"blue\"), lty=1:3,lwd=2\n)\n\n\n\n\n\n\n\n\n\n\n2.4.1.2 Extrapolating\nThe overfitting from the previous section also indicates problems that can occur when extrapolating with polynomial models. When high degrees are present, the best fit curve can change directions quickly and even make impossible or illogical predictions.\nBeyond that, even when the fitted polynomial models all trend in the same general direction, polynomials of different degree will diverge quickly from one another. Consider a model where the data are generated from \\[\n  y = 2x + 3x^3 + \\varepsilon.\n\\] All four models displayed below generated very significant F tests. However, each one will give very different answers to predicting the value of \\(y\\) when \\(x=5\\).\n\nset.seed(128)\n# Simulate some regression data with\n# a quadratic trend\nxx  = seq(0,2,0.05)\nlen = length(xx)\nyy  = 2*xx + 3*xx^3 + rnorm(n=len,0,1)\n# Fit some low degree polynomial models\nmd1 = lm( yy~poly(xx,1) )\nmd2 = lm( yy~poly(xx,2) )\nmd3 = lm( yy~poly(xx,3) )\nmd4 = lm( yy~poly(xx,4) )\n# plot the models (interpolate)\nplot(xx,yy,las=1,xlab=\"X\",ylab=\"Y\",main=\"Interpolation\")\nlines(xx,predict(md1),lty=1,col=1,lwd=2)\nlines(xx,predict(md2),lty=2,col=2,lwd=2)\nlines(xx,predict(md3),lty=3,col=3,lwd=2)\nlines(xx,predict(md4),lty=4,col=4,lwd=2)\nlegend(\n  \"topleft\",legend = c(\"linear\",\"quad\",\"cube\",\"quart\"),\n  lty=1:4,col=1:4,lwd=2\n)\n\n\n\n\n\n\n\n# plot the models (extrapolate)\nxx.ext = seq(0,5,0.05)\nplot(\n  xx,yy,las=1,xlab=\"X\",ylab=\"Y\",main=\"Extrapolation\",\n  xlim=c(0,5),ylim=c(min(yy),200)\n)\nlines(xx.ext,predict(md1,data.frame(xx=xx.ext)),lty=1,col=1,lwd=2)\nlines(xx.ext,predict(md2,data.frame(xx=xx.ext)),lty=2,col=2,lwd=2)\nlines(xx.ext,predict(md3,data.frame(xx=xx.ext)),lty=3,col=3,lwd=2)\nlines(xx.ext,predict(md4,data.frame(xx=xx.ext)),lty=4,col=4,lwd=2)\nlegend(\n  \"topleft\",legend = c(\"linear\",\"quad\",\"cube\",\"quart\"),\n  lty=1:4,col=1:4,lwd=2\n)\n\n\n\n\n\n\n\n\n\n\n2.4.1.3 Hierarchy\nAn hierarchical polynomial model is one such that if it contains a term of degree \\(k\\), then it will contain all terms of order \\(i=0,1\\ldots,k-1\\) as well. In practice, it is not strictly necessary to do this. However, doing so will maintain invariance to linear shifts in the data.\nConsider the simple linear regression model \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\). If we were to shift the values of \\(x\\) by some constant \\(a\\), then \\[\\begin{align*}\n  y\n  &= \\beta_0 + \\beta_1 (x+a)+ \\varepsilon\\\\\n  &= (\\beta_0+a\\beta_1) + \\beta_1 x + \\varepsilon\\\\\n  &= \\beta_0' + \\beta_1 x + \\varepsilon\n\\end{align*}\\] and we still have the same model but with a modified intercept term.\nNow consider the polynomial regression model \\(y = \\beta_0 + \\beta_2 x^2 + \\varepsilon\\). If we were to similarly shift the values of \\(x\\) by some constant \\(a\\), then \\[\\begin{align*}\n  y\n  &= \\beta_0 + \\beta_2 (x+a)^2+ \\varepsilon\\\\\n  &= (\\beta_0+a^2\\beta_2) + 2\\beta_2ax + \\beta_2 x^2 + \\varepsilon\\\\\n  &= \\beta_0' + \\beta_1' x + \\beta_2 x^2 + \\varepsilon.\n\\end{align*}\\] Now, our model has a linear term, that is \\(\\beta_1'x\\), in it, which was not there before.\nIn general, for a degree \\(p\\) model, if \\(x\\) is shifted by some constant \\(a\\), then \\[\n  y = \\beta_0 + \\sum_{i=1}^p \\beta_ix^i\n  ~~\\Rightarrow~~\n  y = \\beta_0' + \\sum_{i=1}^p \\beta_i'x^i.\n\\] Thus, the model is invariant under linear translation.\n\n\n\n2.4.2 Piecewise Polynomials\nWhile a single high degree polynomial can be fit very closely to the observed data, there exist the already discussed problems of overfitting and subsequent extrapolation. Hence, an alternative to capture the behaviour of highly nonlinear data is to apply a piecewise polynomial model, which is often referred to as a spline model.\nTo begin, assume that the observed regressors take values in the interval \\([a,b]\\). Then, we partition the interval with \\(k+1\\) by \\(a=t_1 &lt; t_2 &lt; \\ldots &lt; t_{k+1}=b\\). The general spline model of order \\(p\\) takes on the form \\[\n  y = \\sum_{j=1}^k \\beta_{0,j}\\boldsymbol{1}\\!\\left[x\\ge t_j\\right] +\n  \\sum_{i=1}^p \\sum_{j=1}^k \\beta_{i,j}( x - t_j )^i_+\n\\tag{2.1}\\] where \\(\\boldsymbol{1}\\!\\left[x&gt;t_j\\right]\\) is the indicator function that takes on a value of \\(0\\) when \\(x&lt;t_j\\) and a value of \\(1\\) otherwise, and where \\(( x - t_j )^i_+ = ( x - t_j )^i\\boldsymbol{1}\\!\\left[x&gt;t_j\\right]\\).\n\nEquation 2.1 is equivalent to fitting a separate \\(p\\)th order polynomial to the data in each interval \\([t_j,t_{j+1}]\\). While doing so does result in many parameters to estimate, it will allow us to perform hypothesis tests for continuity and differentiability of the data, which we will discuss in the next section.\n\nSubmodels of Equation 2.1 have a collection of interesting properties. If the coefficients \\(\\beta_{0,j}\\) are set to \\(0\\), then we will fit a model that ensures continuity at the knots. For example, the model \\[\n  y = \\beta_{0,1} + \\sum_{j=1}^k \\beta_{1,j}( x-t_j )_+\n\\] is a piecewise linear model with continuity at the knots. Conversely, a model of the form \\[\n  y = \\sum_{j=1}^k \\beta_{0,j} \\boldsymbol{1}\\!\\left[ x\\ge t_j \\right]\n\\] fits a piecewise constant model and can be used to look for change points in an otherwise constant process.\nIn practice, spline models with only cubic terms are generally preferred as they have a high enough order to ensure a good amount of smoothness–i.e. the curves are twice continuously differentiable–but generally do not lead to overfitting of the data.\n\n2.4.2.1 A Spline Example\nNote that there are R packages to fit spline models to data. However, in this example we will do it manually for pedagogical purposes.\nConsider a model of the form \\[\n  y = 2+3x-4x^5+x^7 + \\varepsilon\n\\] with \\(\\varepsilon\\sim\\mathcal{N}\\left(0,4\\right)\\) with \\(n=41\\) observations with regressor \\(x\\in[0,2]\\). A degree 4 and degree 7 polynomial regression were fit to the data, and the results are displayed below. These deviate quickly from the true curve outside of the range of the data.\n\nset.seed(256)\n# Generate Data from a degree-7 polynomial\nxx  = seq(0,2,0.05)\nlen = length(xx)\nyy  = 2 + 3*xx - 4*xx^5 + xx^7 + rnorm(len,0,4)\n# Fit degree 4 and degree 7 models\nmd4 = lm( yy~poly(xx,4) )\nmd7 = lm( yy~poly(xx,7) )\n# Plot polynomial models \nplot( xx,yy,las=1,xlab=\"X\",ylab=\"Y\" )\nlines( xx, 2 + 3*xx - 4*xx^5 + xx^7, col='gray',lwd=2,lty=1 )\nlines( xx, predict(md4), col='red',lwd=2,lty=2 )\nlines( xx, predict(md7), col='blue',lwd=2,lty=3 )\nlegend(\n  \"bottomleft\",legend = c(\"Truth\",\"Deg 4\",\"Deg 7\"),\n  col=c(\"gray\",\"red\",\"blue\"),lty=1:3,lwd=2\n)\n\n\n\n\n\n\n\n\nThe next plot fits a piecewise linear spline with 6 knots–i.e. \\(k=5\\). The model is of the form \\[\n  y = \\beta_{0,1} + \\beta_{1,1}( x-0.0 )_+ + \\beta_{1,2}( x-0.4 )_+ +\n  \\ldots + \\beta_{1,5}( x-1.6 )_+.\n\\]\n\n# Manually creating the inputs to the regression model\nknt  = c(1,9,17,25,33)\nreg1 = c(rep(0,knt[1]-1),xx[knt[1]:len] - xx[knt[1]]);\nreg2 = c(rep(0,knt[2]-1),xx[knt[2]:len] - xx[knt[2]]);\nreg3 = c(rep(0,knt[3]-1),xx[knt[3]:len] - xx[knt[3]]);\nreg4 = c(rep(0,knt[4]-1),xx[knt[4]:len] - xx[knt[4]]);\nreg5 = c(rep(0,knt[5]-1),xx[knt[5]:len] - xx[knt[5]]);\n# Fit linear spline model\nmd.1spline &lt;- lm( yy~reg1+reg2+reg3+reg4+reg5 )\n# Plot the linear spline model\nplot(\n  xx,yy,las=1,xlab=\"X\",ylab=\"Y\"\n)\nlines( xx, 2 + 3*xx - 4*xx^5 + xx^7, col='gray',lwd=2,lty=1 )\nlines(\n  xx,predict(md.1spline),lwd=2,col='red'\n)\nlegend(\n  \"bottomleft\",legend = c(\"Truth\",\"Linear Spline\"),\n  col=c(\"gray\",\"red\"),lty=1,lwd=2\n)\n\n\n\n\n\n\n\n\nThe final plot fits a spline with only piecewise cubic terms and a spline with cubic and linear terms. The only cubic model provides a very reasonable approximation to the data. The cubic-linear spline model becomes a bit crazy.\n\n# Manually creating the inputs to the regression model\nknt  = c(1,9,17,25,33)\ncub1 = c(rep(0,knt[1]-1),xx[knt[1]:len] - xx[knt[1]])^3;\ncub2 = c(rep(0,knt[2]-1),xx[knt[2]:len] - xx[knt[2]])^3;\ncub3 = c(rep(0,knt[3]-1),xx[knt[3]:len] - xx[knt[3]])^3;\ncub4 = c(rep(0,knt[4]-1),xx[knt[4]:len] - xx[knt[4]])^3;\ncub5 = c(rep(0,knt[5]-1),xx[knt[5]:len] - xx[knt[5]])^3;\n# Fit cubic spline model\nmd.3spline &lt;- lm( yy~cub1+cub2+cub3+cub4+cub5 )\n# Fit linear and cubic spline model\nmd.13spline &lt;- lm( \n  yy~reg1+reg2+reg3+reg4+reg5+cub1+cub2+cub3+cub4+cub5 \n)\n# Plot the linear spline model\nplot(\n  xx,yy,las=1,xlab=\"X\",ylab=\"Y\"\n)\nlines( xx, 2 + 3*xx - 4*xx^5 + xx^7, col='gray',lwd=2,lty=1 )\nlines(\n  xx,predict(md.3spline),lwd=2,col='red'\n)\nlines(\n  xx,predict(md.13spline),lwd=2,col='green',lty=2\n)\nlegend(\n  \"bottomleft\",legend = c(\"Truth\",\"Cubic Spline\",\"Cubic & Linear\"),\n  col=c(\"gray\",\"red\",\"green\"),lty=c(1,1,2),lwd=2\n)\n\n\n\n\n\n\n\n\n\n\n2.4.2.2 Hypothesis testing for spline models\nIt is useful to consider what the hypothesis tests mean in the context of spline models. For example, consider fitting the piecewise constant model to some data: \\[\n  y = \\sum_{j=1}^k \\beta_{0,j} \\boldsymbol{1}\\!\\left[ x\\ge t_j \\right].\n\\] The usual F-test will consider the hypotheses \\[\n  H_0: \\beta_{0,2}=\\ldots=\\beta_{0,k}=0~~~~~~\n  H_1: \\exists i\\in\\{2,3,\\ldots,k\\}~\\text{s.t.}~\\beta_{0,i}\\ne0.\n\\] This hypothesis is asking whether or not we believe the mean of the observations changes as \\(x\\) increases. Note that \\(\\beta_{0,1}\\) is the overall intercept term in this model.\nWe can also compare two different spline models with a partial F-test. For example, \\[\\begin{align*}\n  &\\text{Model 1:}& y&= \\beta_{0,1} +\n   \\textstyle\n   \\sum_{j=1}^k \\beta_{3,j}( x-t_j )^3_+\\\\\n  &\\text{Model 2:}& y&= \\beta_{0,1} +\n   \\textstyle\n   \\sum_{j=2}^k \\beta_{0,j} \\boldsymbol{1}\\!\\left[ x\\ge t_j \\right]+\n   \\sum_{j=1}^k \\beta_{3,j}( x-t_j )^3_+\n\\end{align*}\\] The partial F-test between models 1 and 2 asks whether or not the addition of the piecewise constant terms adds any explanatory power to our model. Equivalently, it is testing for whether or not the model has any discontinuities in it. Similarly, first order terms can be used to test for differentiability.\nInstead of constructing a bigger model by adding polynomial terms of different orders, it is also possible to increase the number of knots in the model. Similar to all of the past examples, we will require that the new set of knots contains the old set–that is, \\(\\{t_1,\\ldots,t_k\\}\\subset\\{s_1,\\ldots,s_m\\}\\).\nThis requirement ensures that our models are nested and thus can be compared in an ANOVA table.\nIn what we considered above, the spline models were comprised of polynomials with supports of the form \\([t_j,\\infty)\\) for knots \\(t_1&lt;\\ldots&lt;t_k\\). However, there are many different families of splines with different desirable properties. Some such families are the B-splines, Non-uniform rational B-splines (NURBS), box splines, Bézier splines, and many others. Here we will briefly consider the family of B-splines due to their simplicity and popularity. Note that in the spline literature, sometimes the knots are referred to as control points.\nThe ultimate goal of the of the B-splines is to construct a polynomial basis where the constituent polynomials have finite support. Specifically, for an interval \\([a,b]\\) and knots \\(a=t_1&lt;t_2&lt;\\ldots&lt;t_k&lt;t_{k+1}=b\\), the constant polynomials will have support on two knots such as \\([t_1,t_2]\\), the linear terms on three knots such as \\([t_1,t_3]\\), and so on up to degree \\(p\\) terms, which require \\(p+2\\) knots.\nB-splines can be defined recursively starting with the constant, or degree 0, terms: \\[\n  B_{j,0}(x) = \\boldsymbol{1}_{ x\\in[t_j,t_{j+1}] },\n\\] which takes a value of 1 on the interval \\([t_j,t_{j+1}]\\) and is 0 elsewhere. From here, the higher order terms can be written as \\[\n  B_{j,i}(x) =\n  \\left(\\frac{x-t_j}{t_{j+i}-t_{j}}\\right)B_{j,i-1}(x) +\n  \\left(\\frac{t_{j+i+1}-x}{t_{j+i+1}-t_{j+1}}\\right)B_{j+1,i-1}(x).\n\\] For example, with knots \\(\\{0,1,2,3\\}\\), we have \\[\\begin{align*}\n  &\\text{Constant:}&\n  B_{j,0} &= \\boldsymbol{1}_{ x\\in[j,j+1] }&j&=0,1,2\\\\\n  &\\text{Linear:}&\n  B_{j,1} &= \\left\\{\n    \\begin{array}{ll}\n      (x-j),&   x\\in[j,j+1]  \\\\\n      (j+2-x),& x\\in[j+1,j+2]  \n    \\end{array}\n  \\right. &j&=0,1\\\\\n  &\\text{Quadratic:}&\n  B_{j,2} &= \\left\\{\n    \\begin{array}{ll}\n      x^2/2,&   x\\in[0,1]  \\\\\n      (-2x^2+6x-3)/2,& x\\in[1,2]    \\\\\n      (3-x)^2/2,&   x\\in[2,3]  \n    \\end{array}\n  \\right. &j&=0.\\\\\n\\end{align*}\\] The linear and quadratic splines are displayed below.\n\nxx   = seq(0,3,0.1)\nlin1 = c(xx[1:11],2-xx[12:21],rep(0,10))\nlin2 = rev(lin1)\nquad = c(\n  xx[1:11]^2/2,(-2*xx[12:21]^2+6*xx[12:21]-3)/2,\n  (3-xx[22:31])^2/2\n)\nplot(\n  0,0,type='n',las=1,xlim=c(0,3),ylim=c(0,1),\n  xlab=\"X\",ylab=\"Y\"\n)\nlines(xx,lin1,lty=2,col=2,lwd=2)\nlines(xx,lin2,lty=3,col=3,lwd=2)\nlines(xx,quad,lty=4,col=4,lwd=2)\nabline(v=0:3,col='gray')\n\n\n\n\n\n\n\n\nJust as we fit a spline model above, we could use the linear regression tools to fit a B-spline model of the form \\[\n  y = \\sum_{i=0}^p \\sum_{j=1}^{k-i} \\beta_{i,j} B_{j,i}(x).\n\\] Here, we require that \\(k&gt;p\\) as we will at least need knots \\(t_1,\\ldots,t_{p+1}\\) for a \\(p\\)th degree polynomial. The total number of terms in the regression, which is number of parameters \\(\\beta_{i,j}\\) to estimate, is \\[\n  k + (k-1) + \\ldots + (k-p).\n\\]\n\n\n\n2.4.3 Interacting Regressors\nThus far in this section, we have considered only polynomial models with a single regressor. However, it is certainly possible to fit a polynomial model for more than one regressor such as \\[\n  y = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2\n  + \\beta_{11} x_1^2 + \\beta_{12} x_1x_2 + \\beta_{22}x_2^2 +\n  \\varepsilon.\n\\] Here we have linear and quadratic terms for \\(x_1\\) and \\(x_2\\) as well as an interaction term \\(\\beta_{12}x_1x_2\\).\nFitting such models to the data follows from what we did for single variable polynomials. In this case, the number of interaction terms can grow quite large in practice. With \\(k\\) regressors, \\(x_1,\\ldots,x_k\\), there will be \\(k\\choose p\\) interaction terms of degree \\(p\\) assuming \\(p&lt;k\\). This leads to the topic of Response Surface Methodology, which is a subtopic of the field of experimental design.\nWe will not consider this topic further in these notes. However, it is worth noting that in R, it is possible to fit a linear regression with interaction terms such as \\[\n  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 +\n  \\beta_{12}x_1x_2 + \\beta_{13}x_1x_3 + \\beta_{23}x_2x_3 +\n  \\beta_{123}x_1x_2x_3 +\\varepsilon\n\\] with the simple syntax lm( y~x1*x2*x3 ) where the symbol * replaces the usual + from before.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Assumptions and Choices</span>"
    ]
  },
  {
    "objectID": "chpt2_ModAss.html#influence-and-leverage",
    "href": "chpt2_ModAss.html#influence-and-leverage",
    "title": "2  Model Assumptions and Choices",
    "section": "2.5 Influence and Leverage",
    "text": "2.5 Influence and Leverage\nThe overall intuition for this section is that each observation does not have an equal influence on the estimation of \\(\\hat{\\beta}\\). If a given observed regressor \\(x\\) lies far from the other observed values, it can have a strong effect on the least squares regression line. The goal of this section is to identify such points or subset of points that have a large influence on the regression.\nIf we use the R command lm() to fit a linear model to some data, then we can use the command influence.measures() to compute an array of diagnostic metrics for each observation to test its influence on the regression. The function influence.measures() computes DFBETAS, DFFITS, covariance ratios, Cook’s distances and the diagonal elements of the so-called hat matrix. We will look at each of these in the following subsections.\n\nWarning: You may notice that the word heuristic appears often in the following subsections when it comes to identifying observations with significant influence. Ultimately, these are rough guidelines based on the intuition of past statisticians and should not be taken as strict rules.\n\n\n2.5.1 The Hat Matrix\nThe projection or ``hat’’ matrix, \\(P = X({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}\\), directly measures the influence of one point on another.\nThis is because under the usual model assumptions, the fitted values have a covariance matrix \\(\\text{var}(\\hat{Y}) = \\sigma^2P\\). Hence, the \\(i,j\\)th entry in \\(P\\) is a measure of the covariance between the fitted values \\(\\hat{Y}_i\\) and \\(\\hat{Y}_j\\).\nThe function influence.measures() reports the diagonal entries of the matrix \\(P\\). Heuristically, any entries that are much larger than the rest will have a strong influence on the regression. More precisely, we know that for a linear model \\[\n  y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p,\n\\] we have that \\(\\text{rank}(P)=p+1\\).\nHence, \\(\\text{trace}(P)=p+1\\) where the trace of a matrix is the sum of the diagonal entries. Thus, as \\(P\\) is an \\(n\\times n\\) matrix, we roughly expect the diagonal entries to be approximately \\((p+1)/n\\). Large deviations from this value should be investigated. For example, Montgomery, Peck, & Vining recommend looking at observations with \\(P_{i,i}&gt;2(p+1)/n\\).\n\nThe \\(i\\)th diagonal entry \\(P_{i,i}\\) is referred to as the leverage of the \\(i\\)th observation in Montgomery, Peck, & Vining. However, in , leverage is \\(P_{i,i}/(1-P_{i,i})\\). This sometimes referred to as the leverage factor.\n\n\n\n2.5.2 Cook’s D\nCook’s D or distance computes the distance between the vector of estimated parameters on all \\(n\\) data points, \\(\\hat{\\beta}\\), and the vector of estimated parameters on \\(n-1\\) data points, \\(\\hat{\\beta}_{(i)}\\) where the \\(i\\)th observation has been removed. Intuitively, if the \\(i\\)th observation has a lot of influence on the estimation of \\(\\beta\\), then the distance between \\(\\hat{\\beta}\\) and \\(\\hat{\\beta}_{(i)}\\) should be large.\nFor a linear model with \\(p+1\\) parameters and a sample size of \\(n\\) observations, the usual form for Cook’s D is \\[\n  D_i = \\frac{\n    {(\\hat{\\beta}_{(i)}-\\hat{\\beta})}^\\mathrm{T} {X}^\\mathrm{T}X\n    {(\\hat{\\beta}_{(i)}-\\hat{\\beta})}/(p+1)\n  }{SS_\\text{res}/(n-p-1)}.\n\\] This is very similar to the confidence ellipsoid from the last chapter. However, this is not an usual F statistic, so we do not compute a p-value as we have done before. Instead, some heuristics are used to determine what a large value is. Some authors suggest looking for \\(D_i\\) greater than \\(1\\) or greater than \\(4/n\\). See Cook’s Distance.\nCook’s D can be written in different forms. One, in terms of the diagonal entries of \\(P\\), is \\[\n  D_i =\n  \\frac{s_i^2}{p+1}\n  \\left(\\frac{P_{i,i}}{1-P_{i,i}}\\right)\n\\] where \\(s_i\\) is the \\(i\\)th studentized residual. Another form of this measure compares the distance between the usual fitted values on all of the data \\(\\hat{Y} = X\\hat{\\beta}\\) and the fitted values based on all but the \\(i\\)th observation, \\(\\hat{Y}_{(i)} = X\\hat{\\beta}_{(i)}\\).\nThat is, \\[\n  D_i = \\frac{\n    {(\\hat{Y}_{(i)}-\\hat{Y})}^\\mathrm{T}(\\hat{Y}_{(i)}-\\hat{Y})/(p+1)\n  }{\n    SS_\\text{res}/(n-p-1)\n  }\n\\] Note that like \\(\\hat{Y}\\), the vector \\(\\hat{Y}_{(i)}\\in\\mathbb{R}^n\\).\nThe \\(i\\)th entry in the vector \\(\\hat{Y}_{(i)}\\) is the predicted value of \\(y\\) given \\(x_i\\).\n\n\n2.5.3 DFBETAS\nThe intuition behind DFBETAS is similar to that for Cook’s D. In this case, we consider the normalized difference between \\(\\hat{\\beta}\\) and \\(\\hat{\\beta}_{(i)}\\). What results is an \\(n\\times(p+1)\\) matrix whose \\(i\\)th row is \\[\n  \\text{DFBETAS}_{i} =\n  \\frac{\\hat{\\beta} - \\hat{\\beta}_{(i)}\n  }{\n    \\sqrt{({X}^\\mathrm{T}X)_{i,i}^{-1}  SS_{\\text{res}(i)}/(n-p-2) }\n  } \\in \\mathbb{R}^{p+1}\n\\] where \\(SS_{\\text{res}(i)}\\) is the sum of the squared residuals for the model fit after removing the \\(i\\)th data point and where \\(({X}^\\mathrm{T}X)^{-1}_{i,i}\\) is the \\(i\\)th diagonal entry of the matrix \\(({X}^\\mathrm{T}X)^{-1}\\). The recommended heuristic is to consider the \\(i\\)th observation as an influential point if the \\(i,j\\)th entry of DFBETAS has a magnitude greater than \\(2/\\sqrt{n}\\).\n\n\n2.5.4 DFFITS\nThe DFFITS value is very similar to the previously discussed DFBETAS. In this case, we are concerned with by how much the fitted values change when the \\(i\\)th observation is removed. Explicitly, \\[\n  \\text{DFFIT} =\n  \\frac{\\hat{Y} - \\hat{Y}_{(i)}\n  }{\n    \\sqrt{ ({X}^\\mathrm{T}X)_{i,i}^{-1} SS_{\\text{res}(i)}/(n-p-2) }\n  } \\in \\mathbb{R}^n.\n\\] The claim is that DFFIT is effected by both leverage and prediction error. The heuristic is to investigate any observation with DFFIT greater in magnitude than \\(2\\sqrt{(p+1)/n}\\).\n\n\n2.5.5 Covariance Ratios\nThe covariance ratio whether the precision of the model increases or decreases when the \\(i\\)th observation is included. This measure is based on the idea that a small value for \\(\\det\\left[ ({X}^\\mathrm{T}X)^{-1}SS_{\\text{res}} \\right]\\) indicates high precision in our model. Hence, the covariance ratio considers a ratio of determinants \\[\\begin{multline*}\n  \\text{covratio}_i =\n  \\frac{\n    \\det\\left[\n      ({X}^\\mathrm{T}_{(i)}X_{(i)})^{-1}SS_{\\text{res}(i)}/(n-p-2)\n    \\right]\n  }{\n    \\det\\left[ ({X}^\\mathrm{T}X)^{-1}SS_{\\text{res}}/(n-p-1) \\right]\n  } = \\\\ =\n  \\left(\n    \\frac{\n      SS_{\\text{res}(i)}/(n-p-2)\n    }{\n      SS_\\text{res}/(n-p-1)\n    }\n  \\right)^{p+1}\n  \\left(\\frac{1}{1-P_{i,i}}\\right).\n\\end{multline*}\\] If the value is greater than 1, then inclusion of the \\(i\\)th point has increased the model’s precision. If it is less than 1, then the precision has decreased. The suggested heuristic threshold for this measure of influence is when the value is greater than \\(1+3(p+1)/n\\) or less than \\(1-3(p+1)/n\\). Though, it is noted that this only is valid for large enough sample sizes.\n\n\n2.5.6 Influence Measures: An Example\nTo test these different measures, we create a dataset of \\(n=50\\) observations from the model \\[\n  y = 3 + 2x + \\varepsilon\n\\] where \\(\\varepsilon\\sim\\mathcal{N}\\left(0,1\\right)\\) and \\(x\\in[0,2]\\). To this dataset, we add 2 anomalous points at \\((4,8)\\) and at \\((1,0)\\). Thus, we fit a simple linear regression to the original 50 data points and also to the new set of 52 data points resulting in the red and blue lines, respectively.\n\nset.seed(216)\n# Generate some data and fit a linear regression\nxx  &lt;- runif(50,0,2);\nyy  &lt;- 3 + 2*xx + rnorm(50,0,1);\nmd0 &lt;- lm( yy~xx )\n# Add two influential points and fit a new regression\nxx  &lt;- c(xx,4,1);\nyy  &lt;- c(yy,8,0);\nmd1 &lt;- lm( yy~xx )\n# Plot the data and two linear models\nplot(xx,yy,las=1,xlab=\"X\",ylab=\"Y\")\nabline(md0,col='red')\nabline(md1,col='blue')\ntext(\n  x=c(4,1,xx[1]),y=c(8,0,yy[1]),labels = c(\"51\",\"52\",\"1\"),pos = 2\n)\n\n\n\n\n\n\n\n\nWe can use the R function influence.measures() to compute a matrix containing the DFBETAS, DFFITS, covariance ratios, Cook’s D, and leverage for each data point. Applying the recommended thresholds in the previous sections results in the following extreme points, which are labelled in the above figure:\n\ninfluence.measures(md1)\n\nInfluence measures of\n     lm(formula = yy ~ xx) :\n\n      dfb.1_    dfb.xx     dffit cov.r   cook.d    hat inf\n1  -1.02e-01  0.317757  0.476552 0.846 1.03e-01 0.0346   *\n2   1.58e-01 -0.133249  0.158687 1.098 1.28e-02 0.0652    \n3  -1.53e-02  0.155431  0.289707 0.949 4.03e-02 0.0270    \n4   1.27e-02 -0.053722 -0.087782 1.064 3.91e-03 0.0307    \n5   6.08e-02 -0.036088  0.069801 1.061 2.48e-03 0.0262    \n6   6.90e-02 -0.037061  0.083446 1.055 3.53e-03 0.0240    \n7  -5.31e-02  0.038346 -0.055547 1.077 1.57e-03 0.0367    \n8  -5.16e-02  0.005189 -0.095960 1.042 4.65e-03 0.0193    \n9   3.75e-02 -0.013404  0.054260 1.057 1.50e-03 0.0205    \n10  3.50e-02 -0.153273 -0.252471 0.990 3.12e-02 0.0305    \n11 -1.52e-01  0.120320 -0.153509 1.076 1.19e-02 0.0499    \n12  8.34e-03  0.025927  0.068098 1.056 2.36e-03 0.0225    \n13 -1.56e-02  0.054631  0.085064 1.067 3.68e-03 0.0327    \n14  9.02e-03 -0.027047 -0.040094 1.077 8.19e-04 0.0353    \n15 -6.29e-02  0.157343  0.218130 1.036 2.37e-02 0.0401    \n16  2.06e-01 -0.178913  0.205853 1.108 2.14e-02 0.0786    \n17  3.07e-01 -0.241294  0.310959 1.014 4.75e-02 0.0483    \n18  7.82e-03 -0.031485 -0.050843 1.071 1.32e-03 0.0312    \n19  6.17e-02 -0.043728  0.065015 1.074 2.15e-03 0.0351    \n20 -1.57e-02  0.047578  0.070775 1.073 2.55e-03 0.0351    \n21 -3.96e-05  0.001572  0.003130 1.069 5.00e-06 0.0257    \n22 -3.36e-02 -0.041662 -0.148244 1.020 1.10e-02 0.0209    \n23 -2.25e-02  0.166146  0.299265 0.946 4.29e-02 0.0278    \n24  9.87e-03  0.064128  0.148393 1.028 1.10e-02 0.0236    \n25 -2.78e-01  0.198487 -0.292542 0.984 4.17e-02 0.0356    \n26 -4.68e-02  0.037911 -0.047125 1.100 1.13e-03 0.0545    \n27  1.29e-02  0.016904  0.058694 1.057 1.75e-03 0.0210    \n28  1.73e-03  0.055587  0.116249 1.045 6.82e-03 0.0249    \n29  5.59e-02 -0.005676  0.103967 1.038 5.45e-03 0.0193    \n30  3.01e-03 -0.049070 -0.094617 1.055 4.54e-03 0.0263    \n31  1.09e-02 -0.028400 -0.039997 1.081 8.16e-04 0.0388    \n32  3.00e-01 -0.186937  0.336339 0.917 5.34e-02 0.0278    \n33 -5.67e-02  0.006466 -0.104152 1.038 5.47e-03 0.0193    \n34 -7.21e-04 -0.001449 -0.004288 1.064 9.38e-06 0.0217    \n35  1.74e-02 -0.049266 -0.071575 1.075 2.61e-03 0.0365    \n36  3.98e-03  0.001045  0.010009 1.062 5.11e-05 0.0194    \n37 -2.31e-02  0.008147 -0.033508 1.061 5.72e-04 0.0204    \n38 -6.27e-02  0.042836 -0.067087 1.070 2.29e-03 0.0325    \n39 -2.67e-03  0.031091  0.058627 1.064 1.75e-03 0.0268    \n40 -3.47e-01  0.301114 -0.346781 1.067 5.96e-02 0.0782    \n41 -2.01e-01  0.170219 -0.201503 1.091 2.05e-02 0.0671    \n42  5.05e-03 -0.002790  0.006023 1.067 1.85e-05 0.0245    \n43 -7.14e-02 -0.017213 -0.176762 0.997 1.54e-02 0.0194    \n44 -5.01e-02 -0.036746 -0.171232 1.003 1.45e-02 0.0202    \n45  1.50e-04  0.000339  0.000969 1.065 4.79e-07 0.0219    \n46 -2.25e-02  0.060242  0.085784 1.074 3.74e-03 0.0379    \n47  1.88e-04 -0.003403 -0.006597 1.069 2.22e-05 0.0262    \n48  1.01e-01 -0.058014  0.118145 1.045 7.04e-03 0.0253    \n49  2.45e-02 -0.020183  0.024645 1.105 3.10e-04 0.0584    \n50 -5.75e-02  0.044956 -0.058414 1.090 1.74e-03 0.0472    \n51  8.89e-01 -1.195032 -1.234540 1.307 7.26e-01 0.3053   *\n52 -4.63e-01  0.209038 -0.608400 0.594 1.41e-01 0.0218   *\n\n\nNote that point 1 is just part of the randomly generated data while points 51 and 52 were purposefully added to be anomalous. Hence, just because an observation is beyond one of these thresholds does not necessarily imply that it lies outside of the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Assumptions and Choices</span>"
    ]
  },
  {
    "objectID": "chpt2_ModAss.html#weighted-least-squares",
    "href": "chpt2_ModAss.html#weighted-least-squares",
    "title": "2  Model Assumptions and Choices",
    "section": "2.6 Weighted Least Squares",
    "text": "2.6 Weighted Least Squares\nA key assumption of the Gauss-Markov theorem is that \\(\\varepsilon_i = \\sigma^2\\) for all \\(i=1,\\ldots,n\\). What happens when \\(\\varepsilon_i = \\sigma_i^2\\)–i.e. when the variance can differ for each observation? Normalizing the errors \\(\\varepsilon_i\\) can be done by \\[\n  \\frac{\\varepsilon_i}{\\sigma_i}  \n  = \\frac{1}{\\sigma_{i,i}}(Y_i - X_{i,\\cdot}\\beta)\n  \\sim\\mathcal{N}\\left(0,1\\right).\n\\] In Chapter 1, we computed the least squares estimator as the vector \\(\\hat{\\beta}\\) such that \\[\n  \\hat{\\beta} = \\underset{\\tilde{\\beta}\\in\\mathbb{R}^{p+1}}{\\arg\\min}\n  \\sum_{i=1}^n (Y_i - X_{i,\\cdot}\\tilde{\\beta})^2\n\\] Now, we will solve the slighly modified equation \\[\n  \\hat{\\beta} = \\underset{\\tilde{\\beta}\\in\\mathbb{R}^{p+1}}{\\arg\\min}\n  \\sum_{i=1}^n \\frac{(Y_i - X_{i,\\cdot}\\tilde{\\beta})^2}{\\sigma^2_i}.\n\\] In this setting, dividing by \\(\\sigma_i^2\\) is the ``weight’’ that gives this method the name weighted least squares.\nProceeding as in chapter 1, we take a derivative with respect to the \\(j\\)th \\(\\tilde{\\beta}_j\\) to get \\[\\begin{align*}\n  \\frac{\\partial}{\\partial \\tilde{\\beta}_j}\n  \\sum_{i=1}^n \\frac{(Y_i - X_{i,\\cdot}\\tilde{\\beta})^2}{\\sigma^2_i}\n  &=\n  2\\sum_{i=1}^n \\frac{(Y_i - X_{i,\\cdot}\\tilde{\\beta})}{\\sigma^2_i}X_{i,j}\\\\\n  &=\n  2\\sum_{i=1}^n \\frac{Y_iX_{i,j}}{\\sigma^2_i} -\n  2\\sum_{i=1}^n \\frac{X_{i,\\cdot}\\tilde{\\beta}X_{i,j}}{\\sigma^2_i}\\\\\n  &=\n  2\\sum_{i=1}^n {Y_i'X_{i,j}'} -\n  2\\sum_{i=1}^n {X_{i,\\cdot}'\\tilde{\\beta}X_{i,j}'}\n\\end{align*}\\] where \\(Y_i' = Y_i/\\sigma_i\\) and \\(X_{i,j}' = X_{i,j}/\\sigma_i\\). Hence, the least squares estimator is as before \\[\\begin{align*}\n  \\hat{\\beta}\n  &= ({X'}^\\mathrm{T}X')^{-1}{X'}^\\mathrm{T}Y'\\\\\n  &= ({X}^\\mathrm{T}WX)^{-1}{X}^\\mathrm{T}WY\n\\end{align*}\\] where \\(W \\in\\mathbb{R}^{n\\times n}\\) is the diagonal matrix with entries \\(W_{i,i} = \\sigma_{i}^2\\).\nIn practise, we do not know the values for \\(\\sigma_i^2\\). Methods to find a good matrix of weights \\(W\\) exist such as Iteratively Reweighted Least Squares which is equivalent to finding the estimator \\[\n  \\hat{\\beta} = \\underset{\\tilde{\\beta}\\in\\mathbb{R}^{p+1}}{\\arg\\min}\n  \\sum_{i=1}^n \\lvert Y_i - X_{i,\\cdot}\\tilde{\\beta}\\rvert^q\n\\] for some \\(q\\in[1,\\infty)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Assumptions and Choices</span>"
    ]
  },
  {
    "objectID": "chpt3_ModBuild.html",
    "href": "chpt3_ModBuild.html",
    "title": "3  Model Building",
    "section": "",
    "text": "3.1 Introduction\nThe following sections will discuss various topics regarding constructing a good representative regression model for your data. Three main topics will be considered/\nFirst, multicollinearity deals with the problem of linear relationships between your regressions.\nWe already know that we require the columns of the design matrix to be linearly independent in order to solve for the least squares estimate. However, it is possible to have near dependencies among the columns. This can lead to numerical stability issues and unnecessary redundancy among the regressors.\nSecond, there are many different variable selection techniques in existence. Given a large number of regressors to can be included in a model, the question is, which should and which should not be included? We will discuss various techniques such as forward and backward selection as well as different tools for comparing models.\nThird, penalized regression will be discussed. This section introduces two modern and quite powerful approaches to linear regression:\nridge regression from the 1970’s and LASSO from the 1990’s. Both arise from modifying how we estimate the parameter vector \\(\\hat{\\beta}\\). Up until now, we have chosen \\(\\hat{\\beta}\\) to minimize the sum of the squared error. Now, we will add a penalty term to this optimization problem, which will encourage choices of \\(\\hat{\\beta}\\) with small-in-magnitude or just zero entries.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building</span>"
    ]
  },
  {
    "objectID": "chpt3_ModBuild.html#multicollinearity",
    "href": "chpt3_ModBuild.html#multicollinearity",
    "title": "3  Model Building",
    "section": "3.2 Multicollinearity",
    "text": "3.2 Multicollinearity\nThe concept of multicollinearity is intuitively simple. Say we have a model of the form \\[\n  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +\\varepsilon.\n\\] This results in a design matrix of the form \\[\n  X = \\begin{pmatrix}\n    1 & x_{1,1} & x_{1,2} \\\\\n    \\vdots & \\vdots & \\vdots \\\\\n    1 & x_{n,1} & x_{n,2}\n  \\end{pmatrix}\n\\] Then, we can consider a new model of the form \\[\n  x_2 = \\alpha_0 + \\alpha_1x_1 + \\varepsilon.\n\\] If this simple regression has a strong fit—e.g. A significant F-test or \\(R^2\\) value—then the addition of the regressor \\(x_2\\) to the original model is unnecessary as almost all of the explanatory information provided by \\(x_2\\) with regards to predicting \\(y\\) is already provided by \\(x_1\\). Hence, the inclusion of \\(x_2\\) in our model is superfluous.\nTaking a more mathematical approach, it can be shown that such near linear dependencies lead to a very high variance for the least squares estimator \\(\\hat{\\beta}\\). Furthermore, the magnitude of the vector is much larger than it should be.\nAssuming that the errors have a covariance matrix \\(\\mathrm{Var}\\left(\\varepsilon\\right) = \\sigma^2I_n\\), then we have from before that \\[\\begin{multline*}\n  \\mathrm{Var}\\left(\\hat{\\beta}\\right)\n  = \\mathrm{Var}\\left( ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y \\right) = \\\\\n  = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T} \\mathrm{Var}\\left(Y\\right) X({X}^\\mathrm{T}X)^{-1}\n  = \\sigma^2({X}^\\mathrm{T}X)^{-1}.\n\\end{multline*}\\] With some effort, it can be shown that the diagonal entries of the matrix \\(({X}^\\mathrm{T}X)^{-1}\\) are equal to \\((1-R^2_0)^{-1},\\ldots,(1-R^2_p)^{-1}\\) where $ R_j^2 $ is the coefficient of determination for the model \\[\n  x_j = \\alpha_0 + \\alpha_1x_1 +\\ldots\n  + \\alpha_{j-1}x_{j-1} + \\alpha_{j+1}x_{j+1} + \\ldots\n  + \\alpha_{p}x_p + \\varepsilon,\n\\] which is trying to predict the \\(j\\)th regressor by the other \\(p-1\\) regressors. If the remaining regressors are good predictors for \\(x_j\\), then the value \\(R_j^2\\) will be close to 1. Hence, \\[\\mathrm{Var}\\left(\\hat{\\beta}_j\\right) = \\frac{\\sigma^2}{1-R_j^2}\\] will be very large.\nFurthermore, this implies that the expected Euclidean distance between \\(\\hat{\\beta}\\) and \\(\\beta\\) will be quite large as well. Indeed, we have \\[\n  \\mathrm{E}\\left(\n    {(\\hat{\\beta}-\\beta)}^\\mathrm{T}\n    {(\\hat{\\beta}-\\beta)}\n  \\right)\n  = \\sum_{i=0}^p \\mathrm{E}\\left( \\hat{\\beta}_i-\\beta_i \\right)^2\n  = \\sum_{i=0}^p \\mathrm{Var}\\left( \\hat{\\beta}_i \\right)\n  = \\sigma^2 \\mathrm{tr}\\left(({X}^\\mathrm{T}X)^{-1}\\right)\n\\] where \\(\\mathrm{tr}\\left(\\cdot\\right)\\) denotes the trace of a matrix–i.e. the sum of the diagonal entries. Hence, if at least one of the \\(R_j^2\\) is close to 1, then the expected distance from our estimator to the true \\(\\beta\\) will be quite large.\nThe trace of a matrix is also equal to the sum of its eigenvalues. Hence, if we denote the eigenvalues of \\({X}^\\mathrm{T}X\\) by \\(\\lambda_1,\\ldots,\\lambda_{p+1}\\), then \\[\n  \\mathrm{tr}\\left(({X}^\\mathrm{T}X)^{-1}\\right) = \\sum_{i=1}^{p+1} \\lambda_{i}^{-1}.\n\\] Hence, an equivalent condition to check for multicollinearity is the presence of eigenvalues of \\({X}^\\mathrm{T}X\\) very close to zero, which would make the above sum very large.\n\n3.2.1 Identifying Multicollinearity\nTo identify the presence of multicollinearity in our linear regression, there are many measures to consider.\nWe already established that near linear dependencies will result in large values for the diagonal entries of \\(({X}^\\mathrm{T}X)^{-1}\\). These values are known as the Variance Inflation Factors and sometimes written as $ _i = (1-R_i2){-1}. $\nAn interesting interpretation of the VIF is in terms of confidence intervals. Recall that for \\(\\beta_j\\), we can construct a \\(1-\\alpha\\) confidence interval as \\[\n  -t_{\\alpha/2,n-p-1}\\sqrt{ ({X}^\\mathrm{T}X)^{-1}_{j,j}\\frac{SS_\\text{res}}{n-p-1} }\n  \\le \\beta_j - \\hat{\\beta}_j \\le\n  t_{\\alpha/2,n-p-1}\\sqrt{ ({X}^\\mathrm{T}X)^{-1}_{j,j}\\frac{SS_\\text{res}}{n-p-1} }.\n\\] If all \\(i\\ne j\\) regressors are orthogonal to the \\(j\\)th regressor, then \\(R_j^2=0\\) and the term \\(({X}^\\mathrm{T}X)^{-1}_{j,j}=1\\). Under multicollinearity, \\(({X}^\\mathrm{T}X)^{-1}_{j,j}\\gg1\\). Hence, the confidence interval is expanded by a factor of \\(\\sqrt{({X}^\\mathrm{T}X)^{-1}_{j,j}}\\) when the regressors are not orthogonal.\nWe can alternatively examine the eigenvalues of the matrix \\({X}^\\mathrm{T}X\\). Recall that finding the least squares estimator is equivalent to solving a system of linear equations of the form \\[\n  {X}^\\mathrm{T}X\\hat{\\beta} = {X}^\\mathrm{T}Y.\n\\] To measure to stability of a solution to a system of equations to small perturbations, a term referred to as the condition number is used. This term arises in more generality in numerical analysis; See Condition Number. It is \\[\n  \\kappa = \\lambda_{\\max}/\\lambda_{\\min}\n\\] where \\(\\lambda_{\\max}\\) and \\(\\lambda_{\\min}\\) are the maximal and minimal eigenvalues, respectively. According to Montgomery, Peck, & Vining, values of \\(\\kappa\\) less than 100 are not significant whereas values greater than 1000 indicate severe multicollinearity.\nIf the minimal eigenvalue is very small, we can use the corresponding eigenvector to understand the nature of the linear dependency. That is, consider the eigenvector \\(u = (u_0,u_1,\\ldots,u_p)\\) for the matrix \\({X}^\\mathrm{T}X\\) corresponding to the eigenvalue \\(\\lambda_{\\min}\\). Recall that this implies that \\[\n  ({X}^\\mathrm{T}X) u = \\lambda_{\\min} u \\approx 0,\n\\] which is approximately zero because \\(\\lambda_{\\min}\\) is close to zero. Hence, for regressors \\(1,x_1,\\ldots,x_p\\), \\[\n  u_0 + u_1x_1 + \\ldots + u_p x_p \\approx 0.\n\\] Thus, we can use the eigenvectors with small eigenvalues to get a linear relationship between the regressors.\n\nRemark 3.1. If you are familiar with the concept of the Singular Value Decomposition, then you could alternatively consider the ratio between the maximal and minimal singular values of the design matrix \\(X\\). Furthermore, you can also analyze the singular vectors instead of the eigen vectors.\n\n\n\n3.2.2 Correcting Multicollinearity\nIdeally, we would design a model such that the columns of the design matrix \\(X\\) are linearly independent.\nOf course, in practise, this is often not achievable. When confronted with real world data, there are still some options available.\nFirst, the regressors can be respecified. That is, if \\(x_1\\) and \\(x_2\\) are near linearly related, then instead of including both terms in the model, we can include a single combination term like \\(x_1x_2\\) or \\((x_1+x_2)/2\\). Second, one of the two variables can be dropped from the model, which will be discussed below when we consider variable selection.\nMore sophisticated solutions to this problem include penalized regression techniques, which we will discuss below. Also, principal components regression—See Montgomery, Peck, Vining Sections 9.5.4 for more on PC regression—and partial least squares are two other methods that can be applied to deal with multicollinear data.\n\nRemark 3.2. A common thread among all of these alternatives is that they result in a biased estimate for \\(\\beta\\) unlike the usual least squares estimator. Often in statistics, we begin with unbiased estimators, but can often achieve a better estimator by adding a small amount of bias. This is the so-called bias-variance tradeoff.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building</span>"
    ]
  },
  {
    "objectID": "chpt3_ModBuild.html#variable-selection",
    "href": "chpt3_ModBuild.html#variable-selection",
    "title": "3  Model Building",
    "section": "3.3 Variable Selection",
    "text": "3.3 Variable Selection\nIn general, if we have \\(p\\) regressors, we may want to build a model consisting only of the best regressors for modelling the response variable. In some sense, we could compare all possible subset models. However, there are many issues with this, which we will address in the following subsections. First, what are the effects of removing regressors from your model? Second, how do we compare models if they are not nested? Third, there are \\(2^p\\) possible models to consider. Exhaustively fitting and comparing all of these models may be computational impractical or impossible. Hence, how do we find a good subset of the regressors?\n\n3.3.1 Subset Models\nWhat happens to the model when we remove some regressors? Assume we have a sample of \\(n\\) observations and \\(p+q\\) regressors and want to remove \\(q\\) of them.\nThe full model would be \\[\n  y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_{p+q}x_{p+q} +\\varepsilon.\n\\] This can be written in terms of the design matrix and partitioned over the two sets of regressors as \\[\\begin{align*}\n  Y &= X\\beta + \\varepsilon\\\\\n    &= X_p\\beta_p + X_q\\beta_q + \\varepsilon\n\\end{align*}\\] where \\(X_p \\in\\mathbb{R}^{n\\times p}\\), \\(X_q\\in\\mathbb{R}^{n\\times q}\\), \\(\\beta_p\\in\\mathbb{R}^p\\), \\(\\beta_q\\in\\mathbb{R}^q\\), and \\[\n  X = \\begin{pmatrix}\n    X_p & X_q\n  \\end{pmatrix},~~~~\n  \\beta = \\begin{pmatrix}\n    \\beta_p \\\\ \\beta_q\n  \\end{pmatrix}\n\\]\nWe have two models to compare. The first is the full model, \\(Y = X\\beta + \\varepsilon\\), where we denote the least squares estimator as \\(\\hat{\\beta} = ({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}Y\\) as usual with components \\[\n  \\hat{\\beta} =\n  \\begin{pmatrix} \\hat{\\beta}_p \\\\ \\hat{\\beta}_q \\end{pmatrix}.\n\\] The second is the reduced model obtained by deleting \\(q\\) regressors: \\(Y = X_p\\beta_p + \\varepsilon\\). The least squares estimator for this model will be denoted as \\(\\tilde{\\beta}_p = ({X}^\\mathrm{T}_pX_p)^{-1}{X}^\\mathrm{T}_pY\\)\n\n3.3.1.1 Bias may increas\nThe first concern with the reduced model is that the estimator \\(\\tilde{\\beta}_p\\) can be biased as \\[\\begin{multline*}\n  \\mathrm{E}{ \\tilde{\\beta}_p } =\n  ({X}^\\mathrm{T}_pX_p)^{-1}{X}^\\mathrm{T}_p \\mathrm{E}Y =\n  ({X}^\\mathrm{T}_pX_p)^{-1}{X}^\\mathrm{T}_p ( X_p\\beta_p + X_q\\beta_q ) =\\\\=\n  \\beta_p + ({X}^\\mathrm{T}_pX_p)^{-1}{X}^\\mathrm{T}_p X_q\\beta_q =\n  \\beta_p + A\\beta_q.\n\\end{multline*}\\] Hence, our reduced estimator is only unbiased in two cases. Case one is when \\(A=0\\), which occurs if the \\(p\\) regressors and \\(q\\) regressors are orthogonal resulting in \\({X}^\\mathrm{T}_p X_q=0\\). Case two is when \\(\\beta_q=0\\), which occurs if those regressors have no effect on the given response. If neither of these cases occurs, then \\(A\\beta_q\\ne0\\) and represents the bias in our estimator \\(\\tilde{\\beta}_p\\). Note that the matrix \\(A\\) is referred to as the alias matrix.\n\n\n3.3.1.2 Variance may decrease\nWhile deleting regressors can result in the addition of bias to our estimate, it can also result in a reduction in the variance of our estimator. Namely, \\[\\begin{align*}\n  \\mathrm{Var}\\left(\\tilde{\\beta}_p\\right) &=\n  \\sigma^2({X}^\\mathrm{T}_pX_p)^{-1}, \\text{ while }\\\\\n  \\mathrm{Var}\\left(\\hat{\\beta}_p\\right) &=\n  \\sigma^2({X}^\\mathrm{T}_pX_p)^{-1} +\n  \\sigma^2 A[ {X_q}^\\mathrm{T}( I-P_p )X_q ]^{-1}{A}^\\mathrm{T},\n\\end{align*}\\] where \\(P_p = X_p({X}^\\mathrm{T}_pX_p)^{-1}{X}^\\mathrm{T}_p\\). This expression can be derived via the formula for inverting a block matrix. The matrix \\(A[ {X_q}^\\mathrm{T}( I-P_p )X_q ]^{-1}{A}^\\mathrm{T}\\) is symmetric positive semi-definite, so the variance for \\(\\hat{\\beta}_p\\) can only be larger than \\(\\tilde{\\beta}_p\\).\n\n\n3.3.1.3 MSE may or may not improve\nGenerally in statistics, when deciding whether or not the increase in the bias is worth the decrease in the variance, we consider the change in the mean squared error (MSE) of our estimate.\nThis is, \\[\\begin{align*}\n  \\text{MSE}(\\tilde{\\beta}_p)\n  &= \\mathrm{E}\\left(\n        (\\tilde{\\beta}_p-\\beta_p)\n    {(\\tilde{\\beta}_p-\\beta_p)}^\\mathrm{T} \\right) \\\\\n  &= \\mathrm{E}\\left(\n        (\\tilde{\\beta}_p-\\mathrm{E}\\tilde{\\beta}_p+\\mathrm{E}\\tilde{\\beta}_p-\\beta_p)\n    {(\\tilde{\\beta}_p-\\mathrm{E}\\tilde{\\beta}_p+\\mathrm{E}\\tilde{\\beta}_p-\\beta_p)}^\\mathrm{T} \\right) \\\\\n  &= \\text{var}(\\tilde{\\beta}_p) + \\text{bias}(\\tilde{\\beta}_p)^2 \\\\\n  &= \\sigma^2({X}^\\mathrm{T}_pX_p)^{-1} + A\\beta_q{\\beta}^\\mathrm{T}_q{A}^\\mathrm{T}.\n\\end{align*}\\] For the full model, \\[\n  \\text{MSE}(\\hat{\\beta}_p) =\n  \\text{var}(\\hat{\\beta}_p) =\n  \\sigma^2({X}^\\mathrm{T}_pX_p)^{-1} +\n  \\sigma^2 A[ {X}^\\mathrm{T}_q( I-P_p )X_q ]^{-1}{A}^\\mathrm{T},\n\\] If \\(\\text{MSE}(\\hat{\\beta}_p) - \\text{MSE}(\\tilde{\\beta}_p)\\) is positive semi-definite, then the mean squared error has decreased upon the removal of the regressors in \\(X_q\\).\n\n\n\n3.3.2 Model Comparison\nWe have already compared models in Chapter 1 with the partial F-test. However, for that test to make sense, we require the models to be nested–i.e. the larger model must contain all of the parameters of the smaller model. But, given a model \\[\n  y = \\beta_0 + \\beta_1x_1 +\\ldots+ \\beta_px_p + \\varepsilon,\n\\] we may want to compare two different subset models that are not nested. Hence, we have some different measures to consider.\nNote that ideally, we would compare all possible subset models. However, given \\(p\\) regressors, there are \\(2^p\\) different models to consider, which will often be computationally infeasible. Hence, we will consider two approaches to model selection that avoid this combinatorial problem.\n\nRemark 3.3. To avoid confusion and awkward notation, assume that all subset models will always contain the intercept term \\(\\beta_0\\)\n\n\n3.3.2.1 Residual Sum of Squares\nFor two subset models with \\(p_1\\) and \\(p_2\\) regressors, respectively, with \\(p_1&lt;p\\) and \\(p_2&lt;p\\), we can compare the mean residual sum of squares for each \\[\n  \\frac{SS_\\text{res}(p_1)}{n-p_1-1}\n  ~~~\\text{ vs }~~~\n  \\frac{SS_\\text{res}(p_2)}{n-p_2-1}\n\\] and choose the model with the smaller value.\nWe know from before that the mean of the residual sum of squares for the full model, \\(SS_\\text{res}/(n-p-1)\\), is an unbiased estimator for \\(\\sigma^2\\). Similar to the calculations in the previous section, we can show that \\[\n  \\mathrm{E}\\left(\\frac{SS_\\text{res}(p_1)}{n-p_1-1}\\right) \\ge \\sigma^2\n  ~~~\\text{ and }~~~\n  \\mathrm{E}\\left(\\frac{SS_\\text{res}(p_2)}{n-p_2-1}\\right) \\ge \\sigma^2,\n\\] which is that these estimators for subset models are upwardly biased.\n\n\n3.3.2.2 Mallows’ \\(C_p\\)\nWe can also compare different models by computing Mallows’ \\(C_p\\). The goal of this value is to choose the model the minimizes the mean squared prediction error, which is \\[\n  MSPE = \\sum_{i=1}^n \\frac{\\mathrm{E}\\left(\\tilde{y}_i-\\mathrm{E}y_i\\right)^2}{\\sigma^2}\n\\] where \\(\\tilde{y_i}\\) is the \\(i\\)th fitted value of the submodel and \\(\\mathrm{E}y_i\\) is the \\(i\\)th fitted value of the true model. Furthermore, let \\(\\hat{y}_i\\) be the \\(i\\)th fitted value for the full model. This is the expected squared difference between what the submodel predicts and what the real value is. As usual with mean squared errors in statistics, we rewrite this in terms of the variance plus the squared bias, which is \\[\\begin{align*}\n  MSPE\n  &= \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\left[\n    \\mathrm{E}\\left(\\tilde{y}_i-\\mathrm{E}\\tilde{y}_i+\\mathrm{E}\\tilde{y}_i-\\mathrm{E}y_i\\right)^2\n  \\right] \\\\\n  &= \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\left[\n    \\mathrm{E}\\left(\\tilde{y}_i-\\mathrm{E}\\tilde{y}_i\\right)^2+({\\mathrm{E}\\tilde{y}_i-\\mathrm{E}y_i})^2\n  \\right] \\\\\n  &= \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\left[\n    \\mathrm{Var}\\left(\\tilde{y}_i\\right)+\\text{bias}(\\tilde{y}_i)^2\n  \\right]\n\\end{align*}\\]\nRecall that the variance of the fitted values for the full model is \\(\\mathrm{Var}\\left(\\hat{y}\\right) = \\sigma^2P_x\\) where \\(P_x = X({X}^\\mathrm{T}X)^{-1}{X}^\\mathrm{T}\\). For a submodel with \\(p_1&lt;p\\) regressors and design matrix \\(X_{p_1}\\), we get the similar \\(\\mathrm{Var}\\left(\\tilde{y}\\right) = \\sigma^2X_{p_1}({X}^\\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\\mathrm{T}_{p_1}\\). As \\(X_{p_1}({X}^\\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\\mathrm{T}_{p_1}\\) is a rank \\(p_1+1\\) projection matrix, we have that \\[\n  \\sum_{i=1}^n \\mathrm{Var}\\left(\\tilde{y}_i\\right) =\n  \\sigma^2\\mathrm{tr}\\left(X_{p_1}({X}^\\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\\mathrm{T}_{p_1}\\right) =\n  \\sigma^2(p_1+1).\n\\]\nFor the bias term, consider the expected residual sum of squares for the submodel: \\[\\begin{align*}\n  \\mathrm{E}\\left(SS_\\text{res}(p_1)\\right)\n  &= \\mathrm{E}\\sum_{i=1}^n ( y_i - \\tilde{y}_i )^2 \\\\\n  &= \\mathrm{E}\\sum_{i=1}^n (\n     y_i -\n     \\mathrm{E}{\\tilde{y}_i} + \\mathrm{E}{\\tilde{y}_i} -\n     \\mathrm{E}{{y}_i} + \\mathrm{E}{{y}_i}\n     - \\tilde{y}_i )^2 \\\\\n  &= \\sum_{i=1}^n\\left[\n    \\mathrm{Var}\\left(\\tilde{r}_i\\right) + (\\mathrm{E}\\tilde{y}_i-\\mathrm{E}y_i)^2  \n  \\right]\\\\\n  &=  (n-p_1-1)\\sigma^2 + \\sum_{i=1}^n \\text{bias}(\\tilde{y}_i)^2.\n\\end{align*}\\] Hence, rearranging the terms above gives \\[\n  \\sum_{i=1}^n \\text{bias}(\\tilde{y}_i)^2\n  = \\mathrm{E}\\left(SS_\\text{res}(p_1)\\right) - (n-p_1-1).\n\\] Combining the bias and the variance terms derived above results in Mallows’ \\(C_p\\) statistic for a submodel with \\(p_1&lt;p\\) regressors: %\\begin{multline} \\[\n  C_{p_1} =\n  \\frac{\\mathrm{E}\\left(SS_\\text{res}(p_1)\\right)}{\\sigma^2} - n+2p_1+2 \\approx%\\\\\\approx\n  \\frac{SS_\\text{res}(p_1)}{SS_\\text{res}/(n-p-1)} - n+2p_1+2.\n\\] %\\end{multline} Here, we estimate \\(\\mathrm{E}\\left(SS_\\text{res}(p_1)\\right)\\) by \\(SS_\\text{res}(p_1)\\) and estimate \\(\\sigma^2\\) by \\(SS_\\text{res}/(n-p-1)\\).\n\nRemark 3.4. Note that if we compute Mallows’ \\(C_p\\) for the full model, we get \\[\n    C_p  \n    = \\frac{SS_\\text{res}}{SS_\\text{res}/(n-p-1)} - n+2p+2\n    = p+1.\n  \\] Hence, Mallows’ \\(C_p\\) in this case is just the number of parameters in the model. In general, we want to find submodels with \\(C_p\\) value smaller than \\(p+1\\).\n\n\n\n3.3.2.3 Information Criteria\nInformation criteria are concerned with quantifying the amount of information in a model. With such a measure, we can choose a model that optimizes this measurement. A main requirement for these methods is that the response \\(y\\) is the same. Hence, we should not use the measures below when comparing transformed models–e.g. different linearized models–without the necessary modifications.\nThe first such measure is the Akaike Information Criterion or AIC, which is a measure of the entropy of a model. Its general definition is \\[\n  \\text{AIC} = -2\\log(\\text{Likelihood}) +\n  2(\\text{\\# parameters})\n\\] where \\(p\\) is the number of parameters in the model. This can be thought of a measurement of how much information is lost when modelling complex data with a \\(p\\) parameter model. Hence, the model with the minimal AIC will be optimal in some sense.\nIn our least squares regression case with normally distributed errors, \\[\n  \\text{AIC} = n\\log(SS_\\text{res}/n) + 2(p+1)\n\\] where \\(p+1\\) is for the \\(p\\) regressors and 1 intercept term. Thus, adding more regressors will decrease \\(SS_\\text{res}\\) but will increase \\(p\\). The goal is to find a model with the minimal AIC. This can be shown to give the same ordering as Mallows’ \\(C_p\\) when the errors are normally distributed.\nThe second such measure is the closely related Bayesian Information Criterion or BIC, which, in general, is \\[\n  \\text{BIC} = -2\\log(\\text{Likelihood}) + (\\text{\\# parameters})\\log n.\n\\] In the linear regression setting with normally distributed errors, \\[\n  \\text{BIC} = n\\log(SS_\\text{res}/n) + (p+1)\\log n.\n\\]\n\nRemark 3.5. Using AIC versus using BIC for model selection can sometimes result in different final choices. In some cases, one may be preferred, but often both can be tried and discrepancies, if they exist, can be reported.\nThere are also other information criterion that are not as common in practise such as the Deviation Information Criterion (DIC) and the Focused Information Criterion (FIC).\n\n\n\n\n3.3.3 Forward and Backward Selection\nIdeally, we choose a measure for model selection from the previous section and then compare all possible models. However, for \\(p\\) possible regressors, this will result in \\(2^p\\) models to check, which may be computationally infeasible. Hence, there are iterative approaches that can be effective.\nForward selection is the process of starting with the constant model \\[\n  y = \\beta_0 + \\varepsilon\n\\] and choosing the best of the \\(p\\) regressors with respect to the model selection criterion. This gives \\[\n  y = \\beta_0 + \\beta_1x_1 + \\varepsilon.\n\\] This process will continue to add terms to the model as long as it results in an improvement in the criterion. For example, computing the AIC at each step.\nBackwards selection is the reverse of forward selection. In this case, the algorithm begins with the full model, \\[\n  y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p +\\varepsilon,\n\\] and iteratively removes the regressor that gives the biggest improvement in the model selection criterion. If the best choice is to remove no regressors, then the process terminates.\nA third option is stepwise selection, which incorporates both forward and backward steps. In this case, we begin with the constant model as in forward selection. However, at every step, we choose either to add a new regressor to our model or remove one that is already in the model depending on which choice improves the criterion the most.\n\n3.3.3.1 Variable Selection Example\nConsider the same example as in the spline section where \\(x\\in[0,2]\\) and \\[\n  y = 2 + 3x - 4x^5 + x^7 + \\varepsilon\n\\] with a sample of \\(n=41\\) observations. We can fit two regression models, an empty and a saturated model, respectively, \\[\n  y = \\beta_0 +\\varepsilon~\\text{ and }~\n  y = \\beta_0 + \\sum_{i=1}^7 \\beta_i x^i + \\varepsilon.\n\\] and use the R function step( ) to choose a best model with respect to AIC.\n\nset.seed(256)\n# Generate Data from a degree-7 polynomial\nxx  = seq(0,2,0.05)\nlen = length(xx)\nyy  = 2 + 3*xx - 4*xx^5 + xx^7 + rnorm(len,0,4)\n# Fit the null and saturated models\n# Note: never ever fit a polynomial model\n#       like this.  We are trying to create\n#       a model with high multicollinearity\n#       for educational purposes. \nmd0 = lm(yy~1);\nmd7 = lm(\n  yy~xx+I(xx^2)+I(xx^3)+I(xx^4)+I(xx^5)+I(xx^6)+I(xx^7)\n)\nsummary(md0)\n\n\nCall:\nlm(formula = yy ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4234  -3.9053   0.8976   4.0282   9.4048 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.6483     0.7630    0.85    0.401\n\nResidual standard error: 4.886 on 40 degrees of freedom\n\nsummary(md7)\n\n\nCall:\nlm(formula = yy ~ xx + I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + \n    I(xx^6) + I(xx^7))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7630 -2.3611 -0.7164  2.2562  7.3220 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    0.529      3.554   0.149    0.883\nxx           -26.747     70.159  -0.381    0.705\nI(xx^2)      271.061    434.342   0.624    0.537\nI(xx^3)     -799.042   1158.236  -0.690    0.495\nI(xx^4)     1124.811   1557.892   0.722    0.475\nI(xx^5)     -824.786   1108.239  -0.744    0.462\nI(xx^6)      300.118    397.719   0.755    0.456\nI(xx^7)      -42.561     56.669  -0.751    0.458\n\nResidual standard error: 3.982 on 33 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.3356 \nF-statistic: 3.886 on 7 and 33 DF,  p-value: 0.003437\n\n\nFirst, we note that this (non-orthogonal) polynomial model is very poorly specified. That is, the estimated coefficients vary wildly and are effectively trying to counter balance each other. None of our t-statistics are significant meaning that we can remove any of these terms individually without harming the fit of the model. The F-statistic is significant indicating that, globally, this model is significantly reducing the residual sum of squares.\nFirst, we apply backwards variable selection. The result is an AIC that drops from 120.42 to 113.29 and the following fitted model: \\[\n  y = 0.56 + 20.47x^2 - 18.59 x^3 + 1.09 x^6.\n\\] In this case, we did not recover the model that was used to generate the data. However, this one still fits the noisy data well.\n\n# Perform a backward variable selection\nmd.bck = step(md7)\n\nStart:  AIC=120.42\nyy ~ xx + I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6) + I(xx^7)\n\n          Df Sum of Sq    RSS    AIC\n- xx       1    2.3052 525.69 118.60\n- I(xx^2)  1    6.1770 529.56 118.90\n- I(xx^3)  1    7.5484 530.93 119.00\n- I(xx^4)  1    8.2678 531.65 119.06\n- I(xx^5)  1    8.7846 532.17 119.10\n- I(xx^7)  1    8.9462 532.33 119.11\n- I(xx^6)  1    9.0311 532.42 119.12\n&lt;none&gt;                 523.39 120.42\n\nStep:  AIC=118.6\nyy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6) + I(xx^7)\n\n          Df Sum of Sq    RSS    AIC\n- I(xx^7)  1    7.4088 533.10 117.17\n- I(xx^6)  1    7.9536 533.64 117.21\n- I(xx^5)  1    8.3113 534.00 117.24\n- I(xx^4)  1    8.7075 534.40 117.27\n- I(xx^3)  1    9.8197 535.51 117.36\n- I(xx^2)  1   13.0549 538.75 117.60\n&lt;none&gt;                 525.69 118.60\n\nStep:  AIC=117.17\nyy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6)\n\n          Df Sum of Sq    RSS    AIC\n- I(xx^5)  1    1.5784 534.68 115.29\n- I(xx^4)  1    1.5872 534.69 115.29\n- I(xx^6)  1    2.1726 535.27 115.34\n- I(xx^3)  1    2.6951 535.79 115.38\n- I(xx^2)  1    6.2706 539.37 115.65\n&lt;none&gt;                 533.10 117.17\n\nStep:  AIC=115.29\nyy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^6)\n\n          Df Sum of Sq    RSS    AIC\n- I(xx^4)  1    0.0096 534.69 113.29\n- I(xx^3)  1    3.4790 538.16 113.56\n- I(xx^6)  1    7.0789 541.76 113.83\n- I(xx^2)  1   12.5541 547.23 114.24\n&lt;none&gt;                 534.68 115.29\n\nStep:  AIC=113.29\nyy ~ I(xx^2) + I(xx^3) + I(xx^6)\n\n          Df Sum of Sq    RSS    AIC\n&lt;none&gt;                 534.69 113.29\n- I(xx^2)  1    143.39 678.08 121.03\n- I(xx^3)  1    197.82 732.50 124.20\n- I(xx^6)  1    229.28 763.96 125.92\n\nsummary(md.bck)\n\n\nCall:\nlm(formula = yy ~ I(xx^2) + I(xx^3) + I(xx^6))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3054 -2.4655  0.0047  2.3310  6.7750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.5640     1.3500   0.418 0.678528    \nI(xx^2)      20.4701     6.4984   3.150 0.003226 ** \nI(xx^3)     -18.5896     5.0245  -3.700 0.000698 ***\nI(xx^6)       1.0863     0.2727   3.983 0.000306 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.801 on 37 degrees of freedom\nMultiple R-squared:   0.44, Adjusted R-squared:  0.3946 \nF-statistic: 9.691 on 3 and 37 DF,  p-value: 7.438e-05\n\n\nNext, doing forward selection, we drop the AIC from 131.07 to 113.36, which is almost the same ending AIC as in the backwards selection performed above. In this case, the fitted model is \\[\n  y = 0.78 + 17.19x^2 - 14.86 x^3 + 0.41 x^7.\n\\]\n\n# Perform a forward variable selection\nmd.fwd = step(md0,direction = \"forward\",scope = list(upper=md7))\n\nStart:  AIC=131.07\nyy ~ 1\n\n          Df Sum of Sq    RSS    AIC\n+ I(xx^2)  1   189.254 765.55 124.01\n+ I(xx^3)  1   178.275 776.53 124.59\n+ xx       1   151.475 803.33 125.98\n+ I(xx^4)  1   150.951 803.85 126.01\n+ I(xx^5)  1   121.568 833.23 127.48\n+ I(xx^6)  1    95.317 859.48 128.75\n+ I(xx^7)  1    73.561 881.24 129.78\n&lt;none&gt;                 954.80 131.06\n\nStep:  AIC=124.01\nyy ~ I(xx^2)\n\n          Df Sum of Sq    RSS    AIC\n+ I(xx^7)  1    44.476 721.07 123.55\n&lt;none&gt;                 765.55 124.01\n+ I(xx^6)  1    33.044 732.50 124.20\n+ I(xx^5)  1    21.043 744.50 124.86\n+ xx       1    15.086 750.46 125.19\n+ I(xx^4)  1     9.728 755.82 125.48\n+ I(xx^3)  1     1.583 763.96 125.92\n\nStep:  AIC=123.55\nyy ~ I(xx^2) + I(xx^7)\n\n          Df Sum of Sq    RSS    AIC\n+ I(xx^3)  1    185.46 535.61 113.36\n+ I(xx^4)  1    178.81 542.26 113.87\n+ xx       1    170.88 550.19 114.46\n+ I(xx^5)  1    170.31 550.76 114.51\n+ I(xx^6)  1    161.22 559.85 115.18\n&lt;none&gt;                 721.07 123.55\n\nStep:  AIC=113.36\nyy ~ I(xx^2) + I(xx^7) + I(xx^3)\n\n          Df Sum of Sq    RSS    AIC\n&lt;none&gt;                 535.61 113.36\n+ xx       1   2.72717 532.89 115.15\n+ I(xx^4)  1   1.21296 534.40 115.27\n+ I(xx^5)  1   1.01714 534.60 115.28\n+ I(xx^6)  1   0.93707 534.68 115.29\n\nsummary(md.fwd)\n\n\nCall:\nlm(formula = yy ~ I(xx^2) + I(xx^7) + I(xx^3))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0984 -2.3891  0.0842  2.4374  6.7973 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.7795     1.3253   0.588 0.560027    \nI(xx^2)      17.1905     5.7868   2.971 0.005195 ** \nI(xx^7)       0.4142     0.1043   3.972 0.000317 ***\nI(xx^3)     -14.8630     4.1525  -3.579 0.000984 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.805 on 37 degrees of freedom\nMultiple R-squared:  0.439, Adjusted R-squared:  0.3935 \nF-statistic: 9.652 on 3 and 37 DF,  p-value: 7.672e-05",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building</span>"
    ]
  },
  {
    "objectID": "chpt3_ModBuild.html#penalized-regressions",
    "href": "chpt3_ModBuild.html#penalized-regressions",
    "title": "3  Model Building",
    "section": "3.4 Penalized Regressions",
    "text": "3.4 Penalized Regressions\nNo matter how we design our model, thus far we have always computed the least squares estimator, \\(\\hat{\\beta}\\), by minimizing the sum of squared errors \\[\n  \\hat{\\beta} = \\underset{\\beta\\in\\mathbb{R}^{p+1}}{\\arg\\min}\n  \\left\\{\n  \\sum_{i=1}^n( y_i - {X}^\\mathrm{T}_i\\beta )^2\n  \\right\\}.\n\\] This is an unbiased estimator for \\(\\beta\\). However, as we have seen previously, the variance of this estimator can be quite large. Hence, we shrink the estimator towards zero adding bias but decreasing the variance. General idea of shrinkage is attributed to Stein (1956) and the so-called Stein Estimator. In the context of regression, we add a penalty term to the above minimization to get a new estimator \\[\n  \\hat{\\beta}^\\text{pen} = \\underset{\\beta\\in\\mathbb{R}^{p+1}}{\\arg\\min}\n  \\left\\{\n    \\sum_{i=1}^n( y_i - {X}^\\mathrm{T}_i\\beta )^2\n    + \\text{penalty}(\\beta)\n  \\right\\},\n\\] which increases as \\(\\beta\\) increases thus attempting to enforce smaller choices for the estimated parameters. We will consider some different types of penalized regression. In R, the glmnet package has a lot of functionality to fit different types of penalized general linear models ::: {#rem-penalIntercept} We generally do not want to penalize the intercept term \\(\\beta_0\\). Often to account for this, the regressors and response are centred–i.e. \\(Y\\) is replaced with \\(Y - \\bar{Y}\\) and each \\(X_j\\) is replaced with \\(X_j-\\bar{X_j}\\) for \\(j=1,\\ldots,p\\)–in order to set the intercept term to zero. :::\n\n3.4.1 Ridge Regression\nThe first method we consider is ridge regression, which arose in statistics in the 1970’s—see Hoerl, A.E.; R.W. Kennard (1970)—but similar techniques arise in other areas of computational mathematics. In short, a quadratic penalty is applied to the least squares estimator resulting in \\[\n  \\hat{\\beta}^\\text{R}_\\lambda= \\underset{\\beta\\in\\mathbb{R}^{p}}{\\arg\\min}\n  \\left\\{\n    \\sum_{i=1}^n( y_i - {X}^\\mathrm{T}_i\\beta )^2 +\n    \\lambda\\sum_{j=1}^p \\beta_j^2\n  \\right\\}\n\\] for any \\(\\lambda\\ge0\\). When \\(\\lambda=0\\), we have the usual least squares estimator. As \\(\\lambda\\) grows, the \\(\\beta\\)’s are more strongly penalized.\nTo solve for \\(\\hat{\\beta}^\\text{R}_\\lambda\\), we proceed as before with the least squares estimator \\(\\hat{\\beta}\\) by setting the partial derivatives equal to zero \\[\\begin{align*}\n  0 &= \\frac{\\partial}{\\partial\\beta_k}\n  \\left\\{\n    \\sum_{i=1}^n( y_i - \\beta_1x_{i,1} -\\ldots-\\beta_px_{i,p} )^2 +\n    \\lambda\\sum_{j=1}^p \\beta_j^2\n  \\right\\}.\n\\end{align*}\\] This results in the system of equations \\[\n  {X}^\\mathrm{T}Y - ({X}^\\mathrm{T}X)\\hat{\\beta}^\\text{R}_\\lambda-\n  \\lambda\\hat{\\beta}^\\text{R}_\\lambda= 0\n\\] with the ridge estimator being $ ^_= (X + I_n)^{-1}Y. $\nThe matrix \\({X}^\\mathrm{T}X\\) is positive semi-definite even when \\(p&gt;n\\)–i.e. the number of parameters exceeds the sample size. Hence, any positive value \\(\\lambda\\) will make \\({X}^\\mathrm{T}X + \\lambda I_n\\) invertible as it adds the positive constant \\(\\lambda\\) to all of the eigenvalues. Increasing the value of \\(\\lambda\\) will increase the numerical stability of the estimator–i.e. decrease the condition number of the matrix. Furthermore, it will decrease the variance of the estimator while increasing the bias. It can also be shown that the bias of \\(\\hat{\\beta}^\\text{R}_\\lambda\\) is \\[\n  \\mathrm{E}{\\hat{\\beta}^\\text{R}_\\lambda} - \\beta =\n  %(\\TT{X}X + \\lmb I_n)^{-1}\\TT{X}X\\beta - \\beta =\n  %(\\TT{X}X + \\lmb I_n)^{-1}( \\TT{X}X - \\TT{X}X - \\lmb I_n )\\beta\n  -\\lambda({X}^\\mathrm{T}X + \\lambda I_n)^{-1}\\beta,\n\\] which implies that the estimator does, in fact, shrink towards zero as \\(\\lambda\\) increases.\n\n\n3.4.2 Best Subset Regression\nAnother type of penalty related to the variable selection techniques from the previous section is the Best Subset Regression approach, which counts the number of non-zero \\(\\beta\\)’s and adds a larger penalty as more terms are included in the model. The optimization looks like \\[\n  \\hat{\\beta}^\\text{B}_\\lambda= \\underset{\\beta\\in\\mathbb{R}^{p}}{\\arg\\min}\n  \\left\\{\n    \\sum_{i=1}^n( y_i - {X}^\\mathrm{T}_i\\beta )^2 +\n    \\lambda\\sum_{j=1}^p \\boldsymbol{1}\\!\\left[\\beta_j\\ne 0\\right]\n  \\right\\}.\n\\] The main problem with this method is that the optimization is non-convex and becomes severely difficult to compute in practice. This is why the forwards and backwards selection methods are used for variable selection.\n\n\n3.4.3 LASSO\nThe last method we consider is the Least Absolute Shrinkage and Selection Operator, which is commonly referred to as just LASSO. This was introduced by Tibshirani (1996) and has since been applied to countless areas of statistics. The form is quite similar to ridge regression with one small but profound modification, \\[\n  \\hat{\\beta}^\\text{L}_\\lambda= \\underset{\\beta\\in\\mathbb{R}^{p}}{\\arg\\min}\n  \\left\\{\n    \\sum_{i=1}^n( y_i - {X}^\\mathrm{T}_i\\beta )^2 +\n    \\lambda\\sum_{j=1}^p \\lvert\\beta_j\\rvert\n  \\right\\},\n\\] which is that the penalty term is now the sum of the absolute values instead of a sum of squares.\nThe main reason for why this technique is popular is that it combines shrinkage methods like ridge regression with variable selection and still results in a convex optimization problem. Delving into the properties of this estimator requires convex analysis and will be left for future investigations.\n\n\n3.4.4 Elastic Net\nThe elastic net regularization method combines both ridge and lasso regression into one methodology. Here, we include a penalty term for each of the two methods: \\[\n  \\hat{\\beta}^\\text{EN}_\\lambda= \\underset{\\beta\\in\\mathbb{R}^{p}}{\\arg\\min}\n  \\left\\{\n    \\sum_{i=1}^n( y_i - {X}^\\mathrm{T}_i\\beta )^2 +\n    \\lambda_1 \\sum_{j=1}^p \\lvert\\beta_j\\rvert +\n    \\lambda_2 \\sum_{j=1}^p \\beta_j^2\n  \\right\\}.\n\\] This method has two tuning parameters \\(\\lambda_1\\ge0\\) and \\(\\lambda_2\\ge0\\). In the R library glmnet, a mixing parameter \\(\\alpha\\) and a scale parameter \\(\\lambda\\) is specified to get \\[\n  \\hat{\\beta}^\\text{EN}_\\lambda= \\underset{\\beta\\in\\mathbb{R}^{p}}{\\arg\\min}\n  \\left\\{\n    \\sum_{i=1}^n( y_i - {X}^\\mathrm{T}_i\\beta )^2 +\n    \\lambda\\sum_{j=1}^p\\left[\n      \\alpha \\lvert\\beta_j\\rvert +\n      \\frac{1-\\alpha}{2} \\beta_j^2\n    \\right]\n  \\right\\}.\n\\] The intuition behind this approach is to combine the strengths of both ridge and lasso regression. Namely, ridge regression shrinks the coefficients towards zero reducing the variance while lasso selects a subset of the parameters to remain in the model.\n\n\n3.4.5 Penalized Regression: An Example\nConsider a sample of size \\(n=100\\) generated by the model \\[\n  y = \\beta_0 + \\beta_1x_1 +\\ldots+\\beta_{50}x_{50} + \\varepsilon\n\\] where \\(\\beta_{27}=2\\), \\(\\beta_{34}=-2\\), all other \\(\\beta_i=0\\), and \\(\\varepsilon\\sim\\mathcal{N}\\left(0,16\\right)\\). Even though only two of the regressors have any effect on the response \\(y\\), feeding all 50 regressors into R’s lm() function can result in many false positives as in the code below. In this example, we have three terms in the model that have weakly small p-values around 6% - 7%, two false positive p-values significant at the 5% level, and the two true positive results—entries 27 and 34—which both have very significant p-values.\n\nset.seed(256)\n# simulate some data\nxx = matrix( \n  rnorm(100*50,0,1), 100, 50\n)\nyy = 2*xx[,27] - 2*xx[,34] + rnorm(100,0,4)\ndat.sim = data.frame(  yy,xx )\n# fit a least squares model with all 50 inputs\nmd.lm = lm( yy ~ ., data=dat.sim )\nsummary(md.lm)\n\n\nCall:\nlm(formula = yy ~ ., data = dat.sim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4593 -1.7764  0.0529  1.3162  8.6453 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.114956   0.604874  -0.190  0.85006    \nX1          -0.186314   0.501304  -0.372  0.71175    \nX2           0.293722   0.698205   0.421  0.67583    \nX3           0.846036   0.635611   1.331  0.18933    \nX4          -0.617202   0.719074  -0.858  0.39489    \nX5           0.947231   0.573410   1.652  0.10494    \nX6           1.226520   0.530814   2.311  0.02510 *  \nX7          -0.316981   0.585094  -0.542  0.59044    \nX8           0.133680   0.563881   0.237  0.81359    \nX9           0.583568   0.515985   1.131  0.26357    \nX10          0.130990   0.472349   0.277  0.78270    \nX11         -0.017902   0.469984  -0.038  0.96977    \nX12         -1.269013   0.668925  -1.897  0.06372 .  \nX13          0.427024   0.564001   0.757  0.45260    \nX14          0.295055   0.679860   0.434  0.66620    \nX15         -0.008477   0.565341  -0.015  0.98810    \nX16          0.684455   0.507938   1.348  0.18401    \nX17         -0.852380   0.655739  -1.300  0.19973    \nX18         -1.482044   0.788183  -1.880  0.06601 .  \nX19         -0.582075   0.567622  -1.025  0.31018    \nX20          1.184024   0.720145   1.644  0.10655    \nX21         -0.825218   0.549479  -1.502  0.13956    \nX22          0.368528   0.600818   0.613  0.54246    \nX23         -0.846930   0.623998  -1.357  0.18092    \nX24         -0.066278   0.526772  -0.126  0.90039    \nX25          0.982897   0.644828   1.524  0.13387    \nX26          0.444571   0.513192   0.866  0.39056    \nX27          1.733198   0.552519   3.137  0.00289 ** \nX28         -0.263400   0.510541  -0.516  0.60823    \nX29          0.287366   0.623317   0.461  0.64682    \nX30         -0.460872   0.505887  -0.911  0.36675    \nX31         -0.057786   0.493507  -0.117  0.90727    \nX32          0.522936   0.503179   1.039  0.30378    \nX33          0.338087   0.581553   0.581  0.56367    \nX34         -3.384722   0.537832  -6.293 8.25e-08 ***\nX35          0.635258   0.530874   1.197  0.23721    \nX36          0.942947   0.500341   1.885  0.06542 .  \nX37          0.385363   0.565600   0.681  0.49887    \nX38          0.517330   0.628746   0.823  0.41461    \nX39         -0.305261   0.535436  -0.570  0.57120    \nX40         -0.539736   0.532697  -1.013  0.31594    \nX41         -1.357445   0.608486  -2.231  0.03030 *  \nX42         -0.102478   0.657813  -0.156  0.87684    \nX43          0.573256   0.681116   0.842  0.40408    \nX44         -0.291538   0.591831  -0.493  0.62449    \nX45          0.512879   0.571004   0.898  0.37347    \nX46          0.235569   0.561770   0.419  0.67681    \nX47          0.475641   0.565423   0.841  0.40432    \nX48          0.308767   0.596236   0.518  0.60689    \nX49          0.196569   0.535140   0.367  0.71496    \nX50         -0.878138   0.578599  -1.518  0.13552    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4 on 49 degrees of freedom\nMultiple R-squared:  0.7283,    Adjusted R-squared:  0.4511 \nF-statistic: 2.627 on 50 and 49 DF,  p-value: 0.0004624\n\n\nWe could attempt one of the stepwise variable selection procedures from the previous section. Running backwards and forwards selection results in the many terms being retained in the model, which furthermore are deemed to be statistically significant from the t-test.\nIn particular, backwards selection results in 18 of 50 terms kept in the model with 10 significant at the 5% level. Meanwhile, forward selection results in 17 of 50 terms kept in the model with 4 significant at the 5% level and another 7 terms just above the classic 5% threshold.\n\nmd0 = lm(yy~1,data=dat.sim)\nmd.bck = step( \n  md.lm, direction = \"backward\", trace=0 \n)\nsummary(md.bck)\n\n\nCall:\nlm(formula = yy ~ X5 + X6 + X9 + X12 + X14 + X17 + X20 + X21 + \n    X23 + X25 + X27 + X34 + X35 + X36 + X38 + X41 + X45 + X50, \n    data = dat.sim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1215 -1.9121 -0.0406  1.7660 10.7895 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.02405    0.40411   0.060   0.9527    \nX5           0.55510    0.36882   1.505   0.1362    \nX6           0.83115    0.38887   2.137   0.0356 *  \nX9           0.54329    0.35032   1.551   0.1248    \nX12         -0.93968    0.45563  -2.062   0.0424 *  \nX14          0.86042    0.42725   2.014   0.0473 *  \nX17         -0.84374    0.39494  -2.136   0.0357 *  \nX20          0.67458    0.44559   1.514   0.1339    \nX21         -0.99140    0.41366  -2.397   0.0188 *  \nX23         -0.81608    0.36562  -2.232   0.0284 *  \nX25          0.64772    0.44846   1.444   0.1525    \nX27          1.85091    0.37052   4.996 3.32e-06 ***\nX34         -3.16199    0.38734  -8.163 3.58e-12 ***\nX35          0.50745    0.38561   1.316   0.1919    \nX36          0.91381    0.34915   2.617   0.0106 *  \nX38          0.82018    0.42639   1.924   0.0579 .  \nX41         -0.99945    0.41261  -2.422   0.0177 *  \nX45          0.66715    0.38839   1.718   0.0897 .  \nX50         -0.52955    0.39024  -1.357   0.1786    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.49 on 81 degrees of freedom\nMultiple R-squared:  0.658, Adjusted R-squared:  0.582 \nF-statistic: 8.659 on 18 and 81 DF,  p-value: 2.152e-12\n\nmd.fwd = step( \n  md0, direction = \"forward\", trace=0, scope = list(upper=md.lm) \n)\nsummary(md.fwd)\n\n\nCall:\nlm(formula = yy ~ X34 + X27 + X29 + X50 + X23 + X41 + X25 + X36 + \n    X45 + X21 + X5 + X17 + X12 + X6 + X48 + X8 + X9, data = dat.sim)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.818 -2.077  0.162  1.371 10.639 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.1072     0.3931   0.273  0.78584    \nX34          -3.1401     0.3868  -8.117 4.10e-12 ***\nX27           1.9382     0.3809   5.088 2.26e-06 ***\nX29           0.5857     0.4051   1.446  0.15198    \nX50          -0.6915     0.3859  -1.792  0.07679 .  \nX23          -0.5109     0.3562  -1.434  0.15537    \nX41          -1.1933     0.4098  -2.912  0.00463 ** \nX25           0.7984     0.4472   1.785  0.07790 .  \nX36           0.6600     0.3486   1.893  0.06186 .  \nX45           0.7560     0.3979   1.900  0.06091 .  \nX21          -1.0932     0.4203  -2.601  0.01102 *  \nX5            0.6229     0.3679   1.693  0.09425 .  \nX17          -0.7021     0.4025  -1.744  0.08484 .  \nX12          -0.8150     0.4439  -1.836  0.06997 .  \nX6            0.5831     0.3835   1.521  0.13217    \nX48           0.6162     0.3868   1.593  0.11503    \nX8           -0.5647     0.3906  -1.446  0.15208    \nX9            0.4483     0.3461   1.295  0.19888    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.516 on 82 degrees of freedom\nMultiple R-squared:  0.6487,    Adjusted R-squared:  0.5759 \nF-statistic: 8.909 on 17 and 82 DF,  p-value: 1.866e-12\n\n\nHence, both of these procedures retain too many regressors in the final model. The stepwise selection method was also run, but returned results equivalent to forward selection.\nApplying ridge regression to this dataset will result in all 50 of the estimated parameters being shrunk towards zero. The code and plot below demonstrate this behaviour. The vertical axis corresponds to the values of \\(\\beta_1,\\ldots,\\beta_{50}\\). The horizontal axis corresponds to increasing values of the penalization parameter \\(\\lambda\\). As \\(\\lambda\\) increases, the estimates for the \\(\\beta\\)’s tend towards zero. Hence we see all 50 of the curves bending towards the zero.\n\n# Load in the glmnet package\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-10\n\n# Run a ridge regression\nmd.ridge = glmnet( xx,yy,alpha=0 )\nplot( md.ridge,las=1 )\n\n\n\n\n\n\n\n\nApplying LASSO to the dataset results in a different set of paths from the ridge regression. The plot below displays the LASSO paths. In this case, the horizontal axis corresponds to some \\(K\\) such that \\(\\lVert\\hat{\\beta}^\\text{L}_\\lambda\\rVert_1&lt;K\\), which is equivalent to adding the penalty term \\(\\lambda\\sum_{j=1}^p \\lvert\\beta_j\\rvert\\). As this bound \\(K\\) grows, more variables will enter the model. The blue and green lines represent the regressors \\(x_{34}\\) and \\(x_{27}\\), which are the first two terms to enter the model.\nEventually, as the penalty is relaxed, many more terms begin to enter the model. Hence, choosing a suitable \\(K\\), or equivalently \\(\\lambda\\), is a critical problem for this method.\n\n# Run a lasso regression\nmd.lasso = glmnet( xx,yy,alpha=1 )\nplot( md.lasso,las=1 )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building</span>"
    ]
  },
  {
    "objectID": "chpt4_AdvRegress.html",
    "href": "chpt4_AdvRegress.html",
    "title": "4  Advanced Regression Methods",
    "section": "",
    "text": "4.1 Generalized Linear Models\nThis final chapter is a brief survey of various methods that go beyond standard linear regression. In particular, we we look at generalized linear models and Bayesian regression.\nA generalized linear model (GLM) extends the usual linear model to cases where the errors \\(\\varepsilon\\) have proposed distributions that are very different than the normal distribution. In this case, we assume that \\(\\varepsilon\\) comes from an exponential family, and the form of the model is \\[\n  g\\left( \\mathrm{E}y \\right) =\n  \\beta_0 + \\beta_1x_1 +\\ldots+ \\beta_px_p.\n\\] Here, \\(g(\\cdot)\\) is referred to as the link function. As these are still parametric models, the parameters \\(\\beta_0,\\ldots,\\beta_p\\) are often solved for via maximum likelihood. These models can be fit in R via the glm() function. While such models can be studied in full generality, for this course, we will only consider two specific GLMs: the logistic regression and the Poisson regression.\nIn the case of the logistic regression, we can use this result to construct an analogue to the F-test when the errors have a normal distribution. That is, we can compute the likelihood of the constant model \\[\n  \\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0\n\\tag{4.1}\\] and the likelihood of the full model \\[\n  \\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) =\n  \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p.\n\\tag{4.2}\\] and claim from Wilks’ theorem that \\(-2\\log(LR)\\) is approximately distributed \\(\\chi^2\\left(p\\right)\\) assuming the sample size is large enough.\nIn R, the glm() function does not return a p-value as the lm() function does for the F test. Instead, it provides two values, the null and residual deviances, which are respectively \\[\\begin{align*}\n  \\text{null deviance}\n  &= -2 \\log\\left(\n    \\frac{L(\\text{null model})}{L(\\text{saturated model})}\n  \\right), \\text{ and}\\\\\n  \\text{residual deviance}\n  &= -2 \\log\\left(\n    \\frac{L(\\text{full model})}{L(\\text{saturated model})}\n  \\right).\n\\end{align*}\\] Here, the null model is from Equation 4.1 and the full model is from Equation 4.2. The saturated model is the extreme model with \\(p=n\\) parameters, which perfectly models the data. It is, in some sense, the largest possible model used as a baseline.\nIn practice, the deviances can be thought of like the residual sum of squares from the ordinary linear regression setting. If the decrease is significant, then the regressors have predictive power over the response. Furthermore, \\[\n  (\\text{null deviance})-(\\text{residual deviance}) =\n  -2 \\log\\left(\n    \\frac{L(\\text{null model})}{L(\\text{full model})}\n  \\right),\n\\] which has an asymptotic \\(\\chi^2\\left(p\\right)\\) distribution. Such a goodness of fit test can be run in R using anova( model, test=\"LRT\" ). Note that there are other goodness-of-fit tests possible for such models.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced Regression Methods</span>"
    ]
  },
  {
    "objectID": "chpt4_AdvRegress.html#generalized-linear-models",
    "href": "chpt4_AdvRegress.html#generalized-linear-models",
    "title": "4  Advanced Regression Methods",
    "section": "",
    "text": "4.1.1 Logistic Regression\nOne of the most useful GLMs is the logistic regression. This is applied in the case of a binary response–i.e when \\(y\\in\\{0,1\\}\\). This could, for example, be used for diagnosis of a disease where \\(x_1,\\ldots,x_p\\) are predictors and \\(y\\) corresponds to the presence or absence of the disease.\nThe usual setup is to treat the observed responses \\(y_i\\in\\{0,1\\}\\) as \\(\\mathrm{Bernoulli}\\left(\\pi_i\\right)\\) random variables, which is \\[\n  \\mathrm{P}\\left(y_i=1\\right)=\\pi_i~\\text{ and }~\\mathrm{P}\\left(y_i=0\\right)=1-\\pi_i.\n\\] This implies that the mean is \\(\\mathrm{E}{y_i}=\\pi_i\\) and the variance is \\(\\mathrm{Var}\\left(y_i\\right) = \\pi_i(1-\\pi_i)\\). Hence, the variance is a function of the mean, which violates the assumption of constant variance in the Gauss-Markov theorem.\nThe goal is then to model \\(\\mathrm{E}{y_i}=\\pi_i\\) as a function of \\({x}^\\mathrm{T}_i\\beta\\). While different link functions are possible—see, for example, probit regression—the standard link function chosen is the logistic response function, sometimes referred to as the logit function, which is \\[\n  \\mathrm{E}y_i = \\pi_i =\n  \\frac{\\exp({x}^\\mathrm{T}_i\\beta)}{1+\\exp({x}^\\mathrm{T}_i\\beta)}\n\\] and results in an S-shaped curve. Rearranging the equation results in \\[\n  \\log\\left( \\frac{\\pi_i}{1-\\pi_i} \\right)\n  = {x}^\\mathrm{T}_i\\beta = \\beta_0 + \\beta_1x_{i,1}+\\ldots+\\beta_px_{i,p}\n\\] where the ratio \\(\\pi_i/(1-\\pi_i)\\) is the odds ratio or simply the odds. Furthermore, the entire response \\(\\log\\left( {\\pi_i}/({1-\\pi_i}) \\right)\\) is often referred to as the log odds.\nThe estimator \\(\\hat{\\beta}\\) can be computed numerically via maximum likelihood as we assume the underlying distribution of the data. Hence, we also get fitted values of the form \\[\n  \\hat{y}_i = \\hat{\\pi}_i =\n  \\frac{\\exp{{x}^\\mathrm{T}_i\\hat{\\beta}}}{1+\\exp{{x}^\\mathrm{T}_i\\hat{\\beta}}}.\n\\] However, as noted already, the variance is not constant. Hence, for residuals of the form \\(y_i-\\hat{\\pi}_i\\) to be useful, we will need to normalize them. The Pearson residual for the logistic regression is \\[\n  r_i = \\frac{y_i - \\hat{\\pi}_i}{\\sqrt{\\hat{\\pi}_i(1-\\hat{\\pi}_i)}}.\n\\]\n\n4.1.1.1 Binomial responses\nIn some cases, multiple observations can be made for each value of predictors \\(x\\). For example, \\(x_i\\in\\mathbb{R}^+\\) could correspond to a dosage of medication and \\(y_i\\in\\{0,1,\\ldots,m_i\\}\\) could correspond to the number of the \\(m_i\\) subjects treated with dosage \\(x_i\\) that are cured of whatever ailment the medication was supposed to treat.\nIn this setting, we can treat the observed values \\(\\pi_i = y_i/m_i\\). Often when fitting a logistic regression model to binomial data, the data points are weighted with respect to the number of observations \\(m_i\\) at each regressor \\(x_i\\). That is, if \\(m_1\\) is very large and \\(m_2\\) is small, then the estimation of \\(\\pi_1 = y_1/m_1\\) is more accurate–i.e. lower variance–than \\(\\pi_2 =y_2/m_2\\).\n\n\n4.1.1.2 Testing model fit\nEstimation of the parameters \\(\\beta\\) is achieved by finding the maximum likelihood estimator. The log likelihood in the logistic regression with Bernoulli data is \\[\\begin{align*}\n  \\log L(\\beta)\n  &= \\log \\prod_{i=1}^n \\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\\\\\n  &= \\sum_{i=1}^n\\left[\n    y_i\\log\\pi_i + (1-y_i)\\log(1-\\pi_i)\n  \\right]\\\\\n  &= \\sum_{i=1}^n\\left[\n    y_i\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) + \\log(1-\\pi_i)\n  \\right]\\\\\n  &= \\sum_{i=1}^n\\left[\n    y_i{x}^\\mathrm{T}_i\\beta - \\log(1+\\mathrm{e}^{{x}^\\mathrm{T}_i\\beta})\n  \\right].\n\\end{align*}\\] The MLE \\(\\hat{\\beta}\\) is solved for numerically.\nBeyond finding the MLEs, the log likelihood is also used to test for the goodness-of-fit of the regression model. This comes from a result known as Wilks’ theorem.\n\n\n\n4.1.2 Wilks’ Theorem\nFor two statistical models with parameter spaces \\(\\Theta_0\\) and \\(\\Theta_1\\) such that \\(\\Theta_0\\subset\\Theta_1\\)—i.e. the models are nested—and likelihoods \\(L_0(\\theta)\\) and \\(L_1(\\theta)\\), then -2 times the log likelihood ratio has an asymptotic chi-squared distribution with degrees of freedom equal to the difference in the dimensionality of the parameter spaces. That is, \\[\n    -2\\log(LR) =\n    -2\\log\\left(\n      \\frac{\n        \\sup_{\\theta\\in\\Theta_0}L_0(\\theta)\n      }{\n        \\sup_{\\theta\\in\\Theta_1}L_1(\\theta)\n      }\n    \\right)  \\xrightarrow{\\text{d}} \\chi^2\\left(\\lvert\\Theta_1\\rvert-\\lvert\\Theta_0\\rvert\\right).\n  \\]\n\n\n\n\n\n4.1.2.1 Logistic Regression: An Example\nA sample of size \\(n=21\\) was randomly generated with \\(x_i\\in[-1,1]\\) and \\(y_i\\) such that \\[\n  y_i\\sim\\mathrm{Bernoulli}\\left(\n    \\frac{\\mathrm{e}^{2x_i}}{1+\\mathrm{e}^{2x_i}}\n  \\right).\n\\] A logistic regression was fit to the data in R using the glm() function with the family=binomial(logit) argument to specify that the distribution is Bernoulli–i.e. binomial with \\(n=1\\)–and that we want a logistic link function.\nThe result from summary() is the model \\[\n  \\log\\left( \\frac{\\pi}{1-\\pi} \\right) =\n  -0.4 + 2.16 x\n\\] with a p-value of \\(0.03\\) for \\(\\hat{\\beta_1}=2.16\\). A plot of the true and predicted curves is displayed below.\nWe could consider the alternative case were \\[\n  y_i\\sim\\mathrm{Binomial}\\left(m_i,\n    \\frac{\\mathrm{e}^{2x_i}}{1+\\mathrm{e}^{2x_i}}\n  \\right)\n\\] for some \\(m_i\\in\\mathbb{Z}^+\\), which is \\(m_i\\) Bernoulli observations at each \\(x_i\\). In this case, we have a much larger dataset, which results in the better fitting curve displayed below.\n\n# Need to add this ...\n\n\n\n4.1.3 Poisson Regression\nThe Poisson regression is another very useful GLM to know, which models counts or occurrences of rare events. Examples include modelling the number of defects in a manufacturing line or predicting lightning strikes across a large swath of land.\nThe Poisson distribution with parameter \\(\\lambda&gt;0\\) has PDF \\(f(x) = \\mathrm{e}^{-\\lambda}\\lambda^x/x!\\). This can often be thought of as a limiting case of the the binomial distribution as \\(n\\rightarrow\\infty\\) and \\(p\\rightarrow0\\) such that \\(np\\rightarrow\\lambda\\). If we want a linear model for a response variable \\(y\\) such that \\(y_i\\) has a \\(\\mathrm{Poisson}\\left(\\lambda_i\\right)\\) distribution, then, similarly to the logistic regression, we apply a link function. In this case, one of the most common link functions used is the log. The resulting model is \\[\n  \\log\\mathrm{E}{y_i} = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\n\\] where \\(\\mathrm{E}{y_i} = \\lambda_i\\).\nSimilarly to the logistic regression case–and to GLMS in general–the parameters \\(\\beta\\) are estimated via maximum likelihood based on the assumption that \\(y\\) has a Poisson distribution. Furthermore, the deviances can be computed and a likelihood ratio test can be run to assess the fit of the model. The log likelihood for a Poisson regression is \\[\\begin{align*}\n  \\log L(\\beta)\n  &= \\log \\prod_{i=1}^n \\frac{\\mathrm{e}^{-\\lambda_i}\\lambda_i^{y_i}}{y_i!} \\\\\n  &= \\sum_{i=1}^n\\left[\n    -\\lambda_i + y_i\\log\\lambda_i - \\log(y_i!)\n  \\right]\\\\\n  &= \\sum_{i=1}^n\\left[\n    -\\mathrm{e}^{{x}^\\mathrm{T}_i\\beta} + y_i{x}^\\mathrm{T}_i\\beta - \\log(y_i!)\n  \\right]\n\\end{align*}\\]\n\n4.1.3.1 Poisson Regression: An Example\nAs an example, consider a sample of size \\(n=21\\) generated randomly by \\[\n  y_i \\sim\\mathrm{Poisson}\\left( \\mathrm{e}^{2x_i} \\right).\n\\] A Poisson regression with log link can be fit to the data using R’s glm() function with the argument family=poisson(link=log).\nThe result from summary() is \\[\n  \\log(\\lambda) = 0.16 + 1.95x\n\\] with a p-value of \\(10^{-8}\\) for \\(\\hat{\\beta_1}=1.95\\). A plot of the true and predicted curves is displayed below.\n\n# Add more R code here!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced Regression Methods</span>"
    ]
  },
  {
    "objectID": "apen1_LinearAlgebra.html",
    "href": "apen1_LinearAlgebra.html",
    "title": "Appendix A — Linear Algebra",
    "section": "",
    "text": "A.1 Vectors and Matrices\nTo successfully understand linear regression, we will require some basic notations regrading the manipulation of vectors and matrices. First, we will denote a \\(n\\) dimensional vector as \\(Y\\in\\mathbb{R}^n\\) and an \\(n\\times p\\) matrix as \\(X\\in\\mathbb{R}^{n\\times p}\\).\nIn linear regression, we are ultimately faced with the problem of solving a linear system of equations. That is, let \\(A\\in\\mathbb{R}^{p\\times p}\\) be an invertible matrix, \\(z\\in\\mathbb{R}^p\\) a vector, and \\(b\\in\\mathbb{R}^p\\) another vector. If we know \\(A\\) and \\(b\\), then we want to solve the following system for \\(Z\\). \\[\n  \\left.\n  \\begin{array}{c}\n    a_{11}z_1 + \\ldots a_{1p}z_p = b_1\\\\\n    \\vdots\\\\\n    a_{p1}z_1 + \\ldots a_{pp}z_p = b_p\n  \\end{array}\n  \\right\\} \\Rightarrow Az = b.\n\\] This solution can be written as \\(z = A^{-1}b\\) assuming, again, that \\(A\\) is invertible. An invertible matrix must necessarilly be square; i.e. number of rows = number of columns.\nFor a matrix \\(X\\in\\mathbb{R}^{n\\times p}\\), its transpose is \\({X}^\\mathrm{T}\\in\\mathbb{R}^{p\\times n}\\). This matrix has \\(ij\\)th entry \\(({X}^\\mathrm{T})_{i,j} = X_{j,i}\\). If \\(X = {X}^\\mathrm{T}\\), then we say that \\(X\\) is symmetric. Symmetric matrices always have real-valued eigenvalues.\nA square matrix \\(A\\in\\mathbb{R}^{p\\times p}\\) is said to be positive definite if \\({x}^\\mathrm{T}Ax&gt;0\\) for all choices of vector \\(x\\in\\mathbb{R}^p\\) such that \\(x\\ne0\\). If we replace the \\(&gt;\\) with a \\(\\ge\\), then we say that \\(A\\) is positive semi-definite; i.e. \\({x}^\\mathrm{T}Ax\\ge0\\) for all choices of vector \\(x\\in\\mathbb{R}^p\\).\nA matrix is said to be idempotent if \\(A^2=A\\). Idempotent matrices define orthogonal projections in \\(\\mathbb{R}^p\\). These are of critical importance in linear regression as least squares regression is simply a projection in \\(\\mathbb{R}^n\\) of the observed output \\(Y\\) onto the column space of \\(X\\in\\mathbb{R}^{n\\times p}\\), which is the \\(p\\)-dimensional subspace spanned by the columns of \\(X\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "apen1_LinearAlgebra.html#eigenvalues",
    "href": "apen1_LinearAlgebra.html#eigenvalues",
    "title": "Appendix A — Linear Algebra",
    "section": "A.2 Eigenvalues",
    "text": "A.2 Eigenvalues\nA very important aspect of a square matrix \\(A\\) is its eigenvalues. For a much more general discussion, see the Spectral Theorem. For our purposes, assume \\(A\\) is a symmetric matrix. Then, we can write \\(A = UD{U}^\\mathrm{T}\\) where \\(D\\) is the diagonal matrix of eigenvalues, and \\(U\\) is an orthogonal matrix; i.e. \\(U{U}^\\mathrm{T} = {U}^\\mathrm{T}U = I\\), the identity matrix. The eigenvalues will be denoted by \\(\\lambda_1,\\ldots,\\lambda_p \\in \\mathbb{R}\\).\nThere are some certain special cases we will need to consider:\n\nif all \\(\\lambda_i\\ne0\\), then \\(A\\) is invertible.\nif at least one \\(\\lambda_i=0\\), then \\(A\\) is singular.\nif all \\(\\lambda_i&gt;0\\), then \\(A\\) is positive definite.\nif all \\(\\lambda_i\\ge0\\), then \\(A\\) is positive semi-definite.\nif all \\(\\lambda_i&lt;0\\), then \\(A\\) is negative definite.\nif all \\(\\lambda_i\\le0\\), then \\(A\\) is negative semi-definite.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "apen1_LinearAlgebra.html#covariance-matrices",
    "href": "apen1_LinearAlgebra.html#covariance-matrices",
    "title": "Appendix A — Linear Algebra",
    "section": "A.3 Covariance Matrices",
    "text": "A.3 Covariance Matrices\nFor a random vector \\(X\\in\\mathbb{R}^n\\), its covariance matrix is the \\(n\\times n\\) matrix with \\(ij\\)th entry \\(\\mathrm{cov}\\left(X_i,X_j\\right)\\). Covariance matrices are necessarily symmetric and positive definite. Furthermore, any symmetric positive definite matrix can be thought of as a covariance matrix. Strictly speaking, we may have a covariance matrix that is positive semi-definite. In this case, we say that the covariance matrix is degenerate.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "apen1_LinearAlgebra.html#quick-list-of-facts",
    "href": "apen1_LinearAlgebra.html#quick-list-of-facts",
    "title": "Appendix A — Linear Algebra",
    "section": "A.4 Quick List of Facts",
    "text": "A.4 Quick List of Facts\n\nFor a matrix \\(A\\in\\mathbb{R}^{n\\times p}\\) with row \\(i\\) and column \\(j\\) entry denoted \\(a_{i,j}\\), then the transpose of \\(A\\) is \\({A}^\\mathrm{T}\\in\\mathbb{R}^{p\\times n}\\) with row \\(i\\) and column \\(j\\) entry \\(a_{j,i}\\). That is, the indices have swapped.\nFor matrices \\(A\\in\\mathbb{R}^{m\\times n}\\) and \\(B\\in\\mathbb{R}^{n\\times p}\\), we have that \\({(AB)}^\\mathrm{T} = {B}^\\mathrm{T}{A}^\\mathrm{T}\\)\nFor an invertible matrix \\(A\\in\\mathbb{R}^{n\\times n}\\), we have that \\({(A^{-1})}^\\mathrm{T} = ({A}^\\mathrm{T})^{-1}\\).\nA matrix \\(A\\) is square if the number of rows equals the number of columns. That is, \\(A\\in\\mathbb{R}^{n\\times n}\\).\nA square matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is symmetric if \\(A={A}^\\mathrm{T}\\).\nA symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) necessarily has real eigenvalues.\nA symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is positive definite if for all \\(x\\in\\mathbb{R}^n\\) with \\(x\\ne0\\), we have that \\({x}^\\mathrm{T}Ax&gt;0\\).\nA symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is also positive definite if all of its eigenvalues are positive real valued.\nA symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is positive semi-definite (also non-negative definite) if for all \\(x\\in\\mathbb{R}^n\\) with \\(x\\ne0\\), we have that \\({x}^\\mathrm{T}Ax\\ge0\\). Or alternatively, all of the eigenvalues are non-negative real valued.\nCovariance matrices are always positive semi-definite. If a covariance matrix has some zero valued eigenvalues, then it is called degenerate.\nIf \\(X,Y\\in\\mathbb{R}^{n}\\) are random vectors, then \\[\n    \\mathrm{cov}\\left(X,Y\\right) = \\mathrm{E}\\left( (X-\\mathrm{E}X){(Y-\\mathrm{E}Y)}^\\mathrm{T} \\right)\\in\\mathbb{R}^{n\\times n}.\n  \\]\nIf \\(X,Y\\in\\mathbb{R}^{n}\\) are random vectors and \\(A,B\\in\\mathbb{R}^{m\\times n}\\) are non-random real valued matrices, then \\[\n    \\mathrm{cov}\\left(AX,BY\\right) = A\\mathrm{cov}\\left(X,Y\\right){B}^\\mathrm{T}\\in\\mathbb{R}^{m\\times m}.\n  \\]\nIf \\(Y\\in\\mathbb{R}^n\\) is multivariate normal–i.e. \\(Y\\sim\\mathcal{N}\\left(\\mu,\\Sigma\\right)\\)–and \\(A\\in\\mathbb{R}^{m\\times n}\\) then \\(AY\\) is also multivariate normal with \\(AY\\sim\\mathcal{N}\\left(A\\mu,A\\Sigma{A}^\\mathrm{T}\\right)\\).\nA square matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is idempotent if \\(A^2 = A\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "apen2_ProbDist.html",
    "href": "apen2_ProbDist.html",
    "title": "Appendix B — Some Useful Probability Distributions",
    "section": "",
    "text": "B.1 Introduction\nThe following is a short overview of some of the most common probability distributions encountered in the context of linear regression.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Some Useful Probability Distributions</span>"
    ]
  },
  {
    "objectID": "apen2_ProbDist.html#normal-distribution",
    "href": "apen2_ProbDist.html#normal-distribution",
    "title": "Appendix B — Some Useful Probability Distributions",
    "section": "B.2 Normal distribution",
    "text": "B.2 Normal distribution\nThe normal or Gaussian distribution is of principal importance in probability and statistics. In its univariate form, we say that \\(X\\sim\\mathcal{N}\\left(\\mu,\\sigma^2\\right)\\) with mean \\(\\mu\\in\\mathbb{R}\\) and variance \\(\\sigma^2\\in\\mathbb{R}^+\\) if it has the following probability density function (pdf): \\[\n  f_{\\mathcal{N}\\left(\\mu,\\sigma^2\\right)}(x) =\n  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\n    -\\frac{1}{2\\sigma^2}(x-\\mu)^2\n  \\right).\n\\] Such a random variable \\(X\\) can be centred and scaled into a standardized form as \\((X-\\mu)/\\sigma\\sim\\mathcal{N}\\left(0,1\\right)\\). Furthermore, the normal distribution is very stable in the sense that the sum of a collection of independent normal random variables has a normal distribution as well as scaling a normal random variable by some real valued scalar.\nPart of its importance stems from the central limit theorem, which in its simplest form states that the standardized sum of a collection on independent random variables converges in distribution to a standard normal random variable. However, its worth noting that there are many other central limit theorems in existence.\n\nTheorem B.1 (Central Limit Theorem) Let \\(Y_1,\\ldots,Y_n\\) be a sample of \\(n\\) iid random variables with mean \\(\\mu\\in\\mathbb{R}\\) and finite variance \\(\\sigma^2&lt;\\infty\\). Letting \\(\\bar{Y} = n^{-1}\\sum_{i=1}^n Y_i\\) be the sample mean, then \\[\n    \\sqrt{n}( \\bar{Y}-\\mu )  \\xrightarrow{\\text{d}} \\mathcal{N}\\left(0,1\\right)\n  \\] where \\( \\xrightarrow{\\text{d}} \\) denotes convergence in distribution.\n\nThe normal distribution can be extended to the multivariate normal distribution for a vector \\(X\\in\\mathbb{R}^p\\) where we write \\(X\\sim\\mathcal{N}\\left({\\bf \\mu},\\Sigma\\right)\\). Here, \\({\\bf\\mu}\\in\\mathbb{R}^p\\) is the mean vector while \\(\\Sigma\\in\\mathbb{R}^{p\\times p}\\) is a \\(p\\times p\\) matrix that is symmetric and positive semi-definite. This distribution also has elliptical symmetry. The form of the pdf, assuming \\(\\Sigma\\) is positive definite, is \\[\n  f_{\\mathcal{N}\\left(\\mu,\\Sigma\\right)}(x) =\n  \\frac{1}{\\sqrt{2\\pi\\det(\\Sigma)}}\\exp\\left(\n    -\\frac{1}{2}{(x-\\mu)}^\\mathrm{T}\\Sigma^{-1}(x-\\mu)\n  \\right)\n\\] Otherwise, \\(X\\) will have a degenerate normal distribution, which is still normal but supported on a subspace of dimension less than \\(p\\).\nThe multivariate normal (MVN) distribution has some very nice characterizations. A vector \\(X=(X_1,\\ldots,X_p)\\) is MVN if and only if every linear combination of the \\(X_i\\) is univariate normal. That is, for all \\(a\\in\\mathbb{R}^p\\), \\(a\\cdot X \\sim\\mathcal{N}\\left(\\tilde{\\mu},\\tilde{\\sigma}^2\\right)\\). Hence, it is worth emphasizing that if \\(X\\) is a vector that is marginally normal (i.e. every component \\(X_i\\) is univariate normal), then the joint distribution is not necessarily multivariate normal.\nIn general if two random variables \\(X,Y\\in\\mathbb{R}\\) are independent, then we have that \\(\\mathrm{cov}\\left(X,Y\\right)=0\\). However, the reverse implication is not necessarily true. One exception to this is that if \\((X,Y)\\) is MVN then if \\(\\mathrm{cov}\\left(X,Y\\right)=0\\) then \\(X\\) and \\(Y\\) are independent.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Some Useful Probability Distributions</span>"
    ]
  },
  {
    "objectID": "apen2_ProbDist.html#chi-squared-distribution",
    "href": "apen2_ProbDist.html#chi-squared-distribution",
    "title": "Appendix B — Some Useful Probability Distributions",
    "section": "B.3 Chi-Squared distribution",
    "text": "B.3 Chi-Squared distribution\nThe chi-squared distribution arises throughout the field of statistics usually in the context of goodness-of-fit testing. Let \\(Z\\sim\\mathcal{N}\\left(0,1\\right)\\), then \\(Z\\sim\\chi^2\\left(1\\right)\\), which is said to be chi-squared with one degree of freedom. Furthermore, chi-squared random variables are additive in the sense that if \\(X\\sim\\chi^2\\left(\\nu\\right)\\) and \\(Y\\sim\\chi^2\\left(\\eta\\right)\\) are independent, then \\(X+Y\\sim\\chi^2\\left(\\nu+\\eta\\right)\\).\nA chi-squared random variable is supported on the positive real line and the pdf for \\(X\\sim\\chi^2\\left(\\nu\\right)\\) with \\(\\nu&gt;0\\) is \\[\n  f_{\\chi^2\\left(\\nu\\right)}(x) =\n  \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)}x^{\\nu/2-1}\\mathrm{e}^{-x/2}\n\\] Its mean and variance are \\(\\nu\\) and \\(2\\nu\\), respectively. Note that while very often the degrees of freedom parameter is a positive integer, it can take on any positive real value and still be valid. In fact, the chi-squared distribution is a specific type of the more general gamma distribution, which will not be discussed in these notes.\nThis random variable is quite useful in the context of linear regression with respect to how it leads to the t and F distributions discussed in the following subsections. However, it also arises in many other areas of statistics including Wilks’ Theorem regarding the asymptotic distribution of the log likelihood ratio, goodness-of-fit in multinomial hypothesis testing, and testing for independence in a contingency table.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Some Useful Probability Distributions</span>"
    ]
  },
  {
    "objectID": "apen2_ProbDist.html#t-distribution",
    "href": "apen2_ProbDist.html#t-distribution",
    "title": "Appendix B — Some Useful Probability Distributions",
    "section": "B.4 t distribution",
    "text": "B.4 t distribution\nThe t distribution is sometimes referred to as Student’s t distribution in recognition of its, at the time, anonymous progenitor William Sealy Gosset working at the Guinness brewery. It most notably arises in the context of estimation of a normal random variable when the variance is unknown as occurs frequently in these lecture notes.\nLet \\(Z\\sim\\mathcal{N}\\left(0,1\\right)\\) and let \\(V \\sim\\chi^2\\left(\\nu\\right)\\) be independent random variables. Then, we say that \\[\n  T = \\frac{Z}{\\sqrt{V/\\nu}} \\sim t\\left(\\nu\\right)\n\\] has a t distribution with \\(\\nu\\) degrees of freedom. Such a distribution can be thought of as a heavier tailed version of the standard normal distribution. In fact, \\(t\\left(1\\right)\\) is the Cauchy distribution with pdf \\[\n  f_{t\\left(1\\right)}(x) = (\\pi(1+x^2))^{-1}\n\\] while \\(T\\sim t\\left(\\nu\\right)\\) converges to a standard normal distribution as \\(\\nu\\rightarrow\\infty\\).\nA noteworthy property of a t distributed random variable is that it will only have moments up to but not including order \\(\\nu\\).\nThat is, for \\(T\\sim t\\left(\\nu\\right)\\), \\(\\mathrm{E}T^k &lt; \\infty\\) for \\(k&lt;\\nu\\).\nFor \\(k\\ge\\nu\\), the moments do not exist.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Some Useful Probability Distributions</span>"
    ]
  },
  {
    "objectID": "apen2_ProbDist.html#f-distribution",
    "href": "apen2_ProbDist.html#f-distribution",
    "title": "Appendix B — Some Useful Probability Distributions",
    "section": "B.5 F distribution",
    "text": "B.5 F distribution\nThe F distribution arises often in the context of linear regression when a comparison is made between two sources of variation. Let \\(X\\sim\\chi^2\\left(\\nu\\right)\\) and \\(Y\\sim\\chi^2\\left(\\eta\\right)\\) be independent random variables, then we write that \\[\n  F = \\frac{X/\\nu}{Y/\\eta} \\sim F\\left(\\nu,\\eta\\right)\n\\] has an F distribution with degrees of freedom \\(\\nu\\) and \\(\\eta\\). Due to this form of the distribution, if \\(F\\sim F\\left(\\nu,\\eta\\right)\\), then \\(F^{-1}\\sim F\\left(\\eta,\\nu\\right)\\). The F distribution is supported on the positive real line. The F and t distributions are related by the fact that if \\(T\\sim t\\left(\\nu\\right)\\), then \\(T^2\\sim F\\left(1,\\nu\\right)\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Some Useful Probability Distributions</span>"
    ]
  }
]