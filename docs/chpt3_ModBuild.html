<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Model Building – STAT 378: Linear Regression Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./chpt2_ModAss.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-364982630eef5352dd1537128a8ed5cb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chpt3_ModBuild.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model Building</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 378: Linear Regression Analysis</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt1_ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt2_ModAss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model Assumptions and Choices</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt3_ModBuild.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model Building</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apen1_LinearAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apen2_ProbDist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Some Useful Probability Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">3.1</span> Introduction</a></li>
  <li><a href="#multicollinearity" id="toc-multicollinearity" class="nav-link" data-scroll-target="#multicollinearity"><span class="header-section-number">3.2</span> Multicollinearity</a>
  <ul class="collapse">
  <li><a href="#identifying-multicollinearity" id="toc-identifying-multicollinearity" class="nav-link" data-scroll-target="#identifying-multicollinearity"><span class="header-section-number">3.2.1</span> Identifying Multicollinearity</a></li>
  <li><a href="#correcting-multicollinearity" id="toc-correcting-multicollinearity" class="nav-link" data-scroll-target="#correcting-multicollinearity"><span class="header-section-number">3.2.2</span> Correcting Multicollinearity</a></li>
  </ul></li>
  <li><a href="#variable-selection" id="toc-variable-selection" class="nav-link" data-scroll-target="#variable-selection"><span class="header-section-number">3.3</span> Variable Selection</a>
  <ul class="collapse">
  <li><a href="#subset-models" id="toc-subset-models" class="nav-link" data-scroll-target="#subset-models"><span class="header-section-number">3.3.1</span> Subset Models</a></li>
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison"><span class="header-section-number">3.3.2</span> Model Comparison</a></li>
  <li><a href="#forward-and-backward-selection" id="toc-forward-and-backward-selection" class="nav-link" data-scroll-target="#forward-and-backward-selection"><span class="header-section-number">3.3.3</span> Forward and Backward Selection</a></li>
  </ul></li>
  <li><a href="#penalized-regressions" id="toc-penalized-regressions" class="nav-link" data-scroll-target="#penalized-regressions"><span class="header-section-number">3.4</span> Penalized Regressions</a>
  <ul class="collapse">
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression"><span class="header-section-number">3.4.1</span> Ridge Regression</a></li>
  <li><a href="#best-subset-regression" id="toc-best-subset-regression" class="nav-link" data-scroll-target="#best-subset-regression"><span class="header-section-number">3.4.2</span> Best Subset Regression</a></li>
  <li><a href="#lasso" id="toc-lasso" class="nav-link" data-scroll-target="#lasso"><span class="header-section-number">3.4.3</span> LASSO</a></li>
  <li><a href="#elastic-net" id="toc-elastic-net" class="nav-link" data-scroll-target="#elastic-net"><span class="header-section-number">3.4.4</span> Elastic Net</a></li>
  <li><a href="#penalized-regression-an-example" id="toc-penalized-regression-an-example" class="nav-link" data-scroll-target="#penalized-regression-an-example"><span class="header-section-number">3.4.5</span> Penalized Regression: An Example</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model Building</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">3.1</span> Introduction</h2>
<p>The following sections will discuss various topics regarding constructing a good representative regression model for your data. Three main topics will be considered/</p>
<p>First, multicollinearity deals with the problem of linear relationships between your regressions.<br>
We already know that we require the columns of the design matrix to be linearly independent in order to solve for the least squares estimate. However, it is possible to have near dependencies among the columns. This can lead to numerical stability issues and unnecessary redundancy among the regressors.</p>
<p>Second, there are many different variable selection techniques in existence. Given a large number of regressors to can be included in a model, the question is, which should and which should not be included? We will discuss various techniques such as forward and backward selection as well as different tools for comparing models.</p>
<p>Third, penalized regression will be discussed. This section introduces two modern and quite powerful approaches to linear regression:<br>
ridge regression from the 1970’s and LASSO from the 1990’s. Both arise from modifying how we estimate the parameter vector <span class="math inline">\(\hat{\beta}\)</span>. Up until now, we have chosen <span class="math inline">\(\hat{\beta}\)</span> to minimize the sum of the squared error. Now, we will add a penalty term to this optimization problem, which will encourage choices of <span class="math inline">\(\hat{\beta}\)</span> with small-in-magnitude or just zero entries.</p>
</section>
<section id="multicollinearity" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="multicollinearity"><span class="header-section-number">3.2</span> Multicollinearity</h2>
<p>The concept of multicollinearity is intuitively simple. Say we have a model of the form <span class="math display">\[
  y = \beta_0 + \beta_1x_1 + \beta_2x_2 +\varepsilon.
\]</span> This results in a design matrix of the form <span class="math display">\[
  X = \begin{pmatrix}
    1 &amp; x_{1,1} &amp; x_{1,2} \\
    \vdots &amp; \vdots &amp; \vdots \\
    1 &amp; x_{n,1} &amp; x_{n,2}
  \end{pmatrix}
\]</span> Then, we can consider a new model of the form <span class="math display">\[
  x_2 = \alpha_0 + \alpha_1x_1 + \varepsilon.
\]</span> If this simple regression has a strong fit—e.g.&nbsp;A significant F-test or <span class="math inline">\(R^2\)</span> value—then the addition of the regressor <span class="math inline">\(x_2\)</span> to the original model is unnecessary as almost all of the explanatory information provided by <span class="math inline">\(x_2\)</span> with regards to predicting <span class="math inline">\(y\)</span> is already provided by <span class="math inline">\(x_1\)</span>. Hence, the inclusion of <span class="math inline">\(x_2\)</span> in our model is superfluous.</p>
<p>Taking a more mathematical approach, it can be shown that such near linear dependencies lead to a very high variance for the least squares estimator <span class="math inline">\(\hat{\beta}\)</span>. Furthermore, the magnitude of the vector is much larger than it should be.</p>
<p>Assuming that the errors have a covariance matrix <span class="math inline">\(\mathrm{Var}\left(\varepsilon\right) = \sigma^2I_n\)</span>, then we have from before that <span class="math display">\[\begin{multline*}
  \mathrm{Var}\left(\hat{\beta}\right)
  = \mathrm{Var}\left( ({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T}Y \right) = \\
  = ({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T} \mathrm{Var}\left(Y\right) X({X}^\mathrm{T}X)^{-1}
  = \sigma^2({X}^\mathrm{T}X)^{-1}.
\end{multline*}\]</span> With some effort, it can be shown that the diagonal entries of the matrix <span class="math inline">\(({X}^\mathrm{T}X)^{-1}\)</span> are equal to <span class="math inline">\((1-R^2_0)^{-1},\ldots,(1-R^2_p)^{-1}\)</span> where $ R_j^2 $ is the coefficient of determination for the model <span class="math display">\[
  x_j = \alpha_0 + \alpha_1x_1 +\ldots
  + \alpha_{j-1}x_{j-1} + \alpha_{j+1}x_{j+1} + \ldots
  + \alpha_{p}x_p + \varepsilon,
\]</span> which is trying to predict the <span class="math inline">\(j\)</span>th regressor by the other <span class="math inline">\(p-1\)</span> regressors. If the remaining regressors are good predictors for <span class="math inline">\(x_j\)</span>, then the value <span class="math inline">\(R_j^2\)</span> will be close to 1. Hence, <span class="math display">\[\mathrm{Var}\left(\hat{\beta}_j\right) = \frac{\sigma^2}{1-R_j^2}\]</span> will be very large.</p>
<p>Furthermore, this implies that the expected Euclidean distance between <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\beta\)</span> will be quite large as well. Indeed, we have <span class="math display">\[
  \mathrm{E}\left(
    {(\hat{\beta}-\beta)}^\mathrm{T}
    {(\hat{\beta}-\beta)}
  \right)
  = \sum_{i=0}^p \mathrm{E}\left( \hat{\beta}_i-\beta_i \right)^2
  = \sum_{i=0}^p \mathrm{Var}\left( \hat{\beta}_i \right)
  = \sigma^2 \mathrm{tr}\left(({X}^\mathrm{T}X)^{-1}\right)
\]</span> where <span class="math inline">\(\mathrm{tr}\left(\cdot\right)\)</span> denotes the trace of a matrix–i.e.&nbsp;the sum of the diagonal entries. Hence, if at least one of the <span class="math inline">\(R_j^2\)</span> is close to 1, then the expected distance from our estimator to the true <span class="math inline">\(\beta\)</span> will be quite large.</p>
<p>The trace of a matrix is also equal to the sum of its eigenvalues. Hence, if we denote the eigenvalues of <span class="math inline">\({X}^\mathrm{T}X\)</span> by <span class="math inline">\(\lambda_1,\ldots,\lambda_{p+1}\)</span>, then <span class="math display">\[
  \mathrm{tr}\left(({X}^\mathrm{T}X)^{-1}\right) = \sum_{i=1}^{p+1} \lambda_{i}^{-1}.
\]</span> Hence, an equivalent condition to check for multicollinearity is the presence of eigenvalues of <span class="math inline">\({X}^\mathrm{T}X\)</span> very close to zero, which would make the above sum very large.</p>
<section id="identifying-multicollinearity" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="identifying-multicollinearity"><span class="header-section-number">3.2.1</span> Identifying Multicollinearity</h3>
<p>To identify the presence of multicollinearity in our linear regression, there are many measures to consider.</p>
<p>We already established that near linear dependencies will result in large values for the diagonal entries of <span class="math inline">\(({X}^\mathrm{T}X)^{-1}\)</span>. These values are known as the <em>Variance Inflation Factors</em> and sometimes written as $ _i = (1-R_i<sup>2)</sup>{-1}. $</p>
<p>An interesting interpretation of the VIF is in terms of confidence intervals. Recall that for <span class="math inline">\(\beta_j\)</span>, we can construct a <span class="math inline">\(1-\alpha\)</span> confidence interval as <span class="math display">\[
  -t_{\alpha/2,n-p-1}\sqrt{ ({X}^\mathrm{T}X)^{-1}_{j,j}\frac{SS_\text{res}}{n-p-1} }
  \le \beta_j - \hat{\beta}_j \le
  t_{\alpha/2,n-p-1}\sqrt{ ({X}^\mathrm{T}X)^{-1}_{j,j}\frac{SS_\text{res}}{n-p-1} }.
\]</span> If all <span class="math inline">\(i\ne j\)</span> regressors are orthogonal to the <span class="math inline">\(j\)</span>th regressor, then <span class="math inline">\(R_j^2=0\)</span> and the term <span class="math inline">\(({X}^\mathrm{T}X)^{-1}_{j,j}=1\)</span>. Under multicollinearity, <span class="math inline">\(({X}^\mathrm{T}X)^{-1}_{j,j}\gg1\)</span>. Hence, the confidence interval is expanded by a factor of <span class="math inline">\(\sqrt{({X}^\mathrm{T}X)^{-1}_{j,j}}\)</span> when the regressors are not orthogonal.</p>
<p>We can alternatively examine the eigenvalues of the matrix <span class="math inline">\({X}^\mathrm{T}X\)</span>. Recall that finding the least squares estimator is equivalent to solving a system of linear equations of the form <span class="math display">\[
  {X}^\mathrm{T}X\hat{\beta} = {X}^\mathrm{T}Y.
\]</span> To measure to stability of a solution to a system of equations to small perturbations, a term referred to as the <em>condition number</em> is used. This term arises in more generality in numerical analysis; See <a href="https://en.wikipedia.org/wiki/Condition_number">Condition Number</a>. It is <span class="math display">\[
  \kappa = \lambda_{\max}/\lambda_{\min}
\]</span> where <span class="math inline">\(\lambda_{\max}\)</span> and <span class="math inline">\(\lambda_{\min}\)</span> are the maximal and minimal eigenvalues, respectively. According to Montgomery, Peck, &amp; Vining, values of <span class="math inline">\(\kappa\)</span> less than 100 are not significant whereas values greater than 1000 indicate severe multicollinearity.</p>
<p>If the minimal eigenvalue is very small, we can use the corresponding eigenvector to understand the nature of the linear dependency. That is, consider the eigenvector <span class="math inline">\(u = (u_0,u_1,\ldots,u_p)\)</span> for the matrix <span class="math inline">\({X}^\mathrm{T}X\)</span> corresponding to the eigenvalue <span class="math inline">\(\lambda_{\min}\)</span>. Recall that this implies that <span class="math display">\[
  ({X}^\mathrm{T}X) u = \lambda_{\min} u \approx 0,
\]</span> which is approximately zero because <span class="math inline">\(\lambda_{\min}\)</span> is close to zero. Hence, for regressors <span class="math inline">\(1,x_1,\ldots,x_p\)</span>, <span class="math display">\[
  u_0 + u_1x_1 + \ldots + u_p x_p \approx 0.
\]</span> Thus, we can use the eigenvectors with small eigenvalues to get a linear relationship between the regressors.</p>
<div id="rem-svd" class="proof remark">
<p><span class="proof-title"><em>Remark 3.1</em>. </span>If you are familiar with the concept of the <a href="https://en.wikipedia.org/wiki/Singular-value_decomposition">Singular Value Decomposition</a>, then you could alternatively consider the ratio between the maximal and minimal singular values of the design matrix <span class="math inline">\(X\)</span>. Furthermore, you can also analyze the singular vectors instead of the eigen vectors.</p>
</div>
</section>
<section id="correcting-multicollinearity" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="correcting-multicollinearity"><span class="header-section-number">3.2.2</span> Correcting Multicollinearity</h3>
<p>Ideally, we would design a model such that the columns of the design matrix <span class="math inline">\(X\)</span> are linearly independent.<br>
Of course, in practise, this is often not achievable. When confronted with real world data, there are still some options available.</p>
<p>First, the regressors can be <em>respecified</em>. That is, if <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are near linearly related, then instead of including both terms in the model, we can include a single combination term like <span class="math inline">\(x_1x_2\)</span> or <span class="math inline">\((x_1+x_2)/2\)</span>. Second, one of the two variables can be dropped from the model, which will be discussed below when we consider variable selection.</p>
<p>More sophisticated solutions to this problem include penalized regression techniques, which we will discuss below. Also, principal components regression—See Montgomery, Peck, Vining Sections 9.5.4 for more on PC regression—and partial least squares are two other methods that can be applied to deal with multicollinear data.</p>
<div id="rem-biasVariance" class="proof remark">
<p><span class="proof-title"><em>Remark 3.2</em>. </span>A common thread among all of these alternatives is that they result in a biased estimate for <span class="math inline">\(\beta\)</span> unlike the usual least squares estimator. Often in statistics, we begin with unbiased estimators, but can often achieve a better estimator by adding a small amount of bias. This is the so-called <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias-variance tradeoff</a>.</p>
</div>
</section>
</section>
<section id="variable-selection" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="variable-selection"><span class="header-section-number">3.3</span> Variable Selection</h2>
<p>In general, if we have <span class="math inline">\(p\)</span> regressors, we may want to build a model consisting only of the best regressors for modelling the response variable. In some sense, we could compare all possible subset models. However, there are many issues with this, which we will address in the following subsections. First, what are the effects of removing regressors from your model? Second, how do we compare models if they are not nested? Third, there are <span class="math inline">\(2^p\)</span> possible models to consider. Exhaustively fitting and comparing all of these models may be computational impractical or impossible. Hence, how do we find a good subset of the regressors?</p>
<section id="subset-models" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="subset-models"><span class="header-section-number">3.3.1</span> Subset Models</h3>
<p>What happens to the model when we remove some regressors? Assume we have a sample of <span class="math inline">\(n\)</span> observations and <span class="math inline">\(p+q\)</span> regressors and want to remove <span class="math inline">\(q\)</span> of them.<br>
The full model would be <span class="math display">\[
  y = \beta_0 + \beta_1 x_1 + \ldots + \beta_{p+q}x_{p+q} +\varepsilon.
\]</span> This can be written in terms of the design matrix and partitioned over the two sets of regressors as <span class="math display">\[\begin{align*}
  Y &amp;= X\beta + \varepsilon\\
    &amp;= X_p\beta_p + X_q\beta_q + \varepsilon
\end{align*}\]</span> where <span class="math inline">\(X_p \in\mathbb{R}^{n\times p}\)</span>, <span class="math inline">\(X_q\in\mathbb{R}^{n\times q}\)</span>, <span class="math inline">\(\beta_p\in\mathbb{R}^p\)</span>, <span class="math inline">\(\beta_q\in\mathbb{R}^q\)</span>, and <span class="math display">\[
  X = \begin{pmatrix}
    X_p &amp; X_q
  \end{pmatrix},~~~~
  \beta = \begin{pmatrix}
    \beta_p \\ \beta_q
  \end{pmatrix}
\]</span></p>
<p>We have two models to compare. The first is the full model, <span class="math inline">\(Y = X\beta + \varepsilon\)</span>, where we denote the least squares estimator as <span class="math inline">\(\hat{\beta} = ({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T}Y\)</span> as usual with components <span class="math display">\[
  \hat{\beta} =
  \begin{pmatrix} \hat{\beta}_p \\ \hat{\beta}_q \end{pmatrix}.
\]</span> The second is the reduced model obtained by deleting <span class="math inline">\(q\)</span> regressors: <span class="math inline">\(Y = X_p\beta_p + \varepsilon\)</span>. The least squares estimator for this model will be denoted as <span class="math inline">\(\tilde{\beta}_p = ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_pY\)</span></p>
<section id="bias-may-increas" class="level4" data-number="3.3.1.1">
<h4 data-number="3.3.1.1" class="anchored" data-anchor-id="bias-may-increas"><span class="header-section-number">3.3.1.1</span> Bias may increas</h4>
<p>The first concern with the reduced model is that the estimator <span class="math inline">\(\tilde{\beta}_p\)</span> can be biased as <span class="math display">\[\begin{multline*}
  \mathrm{E}{ \tilde{\beta}_p } =
  ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p \mathrm{E}Y =
  ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p ( X_p\beta_p + X_q\beta_q ) =\\=
  \beta_p + ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p X_q\beta_q =
  \beta_p + A\beta_q.
\end{multline*}\]</span> Hence, our reduced estimator is only unbiased in two cases. Case one is when <span class="math inline">\(A=0\)</span>, which occurs if the <span class="math inline">\(p\)</span> regressors and <span class="math inline">\(q\)</span> regressors are orthogonal resulting in <span class="math inline">\({X}^\mathrm{T}_p X_q=0\)</span>. Case two is when <span class="math inline">\(\beta_q=0\)</span>, which occurs if those regressors have no effect on the given response. If neither of these cases occurs, then <span class="math inline">\(A\beta_q\ne0\)</span> and represents the bias in our estimator <span class="math inline">\(\tilde{\beta}_p\)</span>. Note that the matrix <span class="math inline">\(A\)</span> is referred to as the <em>alias</em> matrix.</p>
</section>
<section id="variance-may-decrease" class="level4" data-number="3.3.1.2">
<h4 data-number="3.3.1.2" class="anchored" data-anchor-id="variance-may-decrease"><span class="header-section-number">3.3.1.2</span> Variance may decrease</h4>
<p>While deleting regressors can result in the addition of bias to our estimate, it can also result in a reduction in the variance of our estimator. Namely, <span class="math display">\[\begin{align*}
  \mathrm{Var}\left(\tilde{\beta}_p\right) &amp;=
  \sigma^2({X}^\mathrm{T}_pX_p)^{-1}, \text{ while }\\
  \mathrm{Var}\left(\hat{\beta}_p\right) &amp;=
  \sigma^2({X}^\mathrm{T}_pX_p)^{-1} +
  \sigma^2 A[ {X_q}^\mathrm{T}( I-P_p )X_q ]^{-1}{A}^\mathrm{T},
\end{align*}\]</span> where <span class="math inline">\(P_p = X_p({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p\)</span>. This expression can be derived via the formula for <a href="https://en.wikipedia.org/wiki/Invertible_matrix#Blockwise_inversion">inverting a block matrix</a>. The matrix <span class="math inline">\(A[ {X_q}^\mathrm{T}( I-P_p )X_q ]^{-1}{A}^\mathrm{T}\)</span> is symmetric positive semi-definite, so the variance for <span class="math inline">\(\hat{\beta}_p\)</span> can only be larger than <span class="math inline">\(\tilde{\beta}_p\)</span>.</p>
</section>
<section id="mse-may-or-may-not-improve" class="level4" data-number="3.3.1.3">
<h4 data-number="3.3.1.3" class="anchored" data-anchor-id="mse-may-or-may-not-improve"><span class="header-section-number">3.3.1.3</span> MSE may or may not improve</h4>
<p>Generally in statistics, when deciding whether or not the increase in the bias is worth the decrease in the variance, we consider the change in the <em>mean squared error</em> (MSE) of our estimate.<br>
This is, <span class="math display">\[\begin{align*}
  \text{MSE}(\tilde{\beta}_p)
  &amp;= \mathrm{E}\left(
        (\tilde{\beta}_p-\beta_p)
    {(\tilde{\beta}_p-\beta_p)}^\mathrm{T} \right) \\
  &amp;= \mathrm{E}\left(
        (\tilde{\beta}_p-\mathrm{E}\tilde{\beta}_p+\mathrm{E}\tilde{\beta}_p-\beta_p)
    {(\tilde{\beta}_p-\mathrm{E}\tilde{\beta}_p+\mathrm{E}\tilde{\beta}_p-\beta_p)}^\mathrm{T} \right) \\
  &amp;= \text{var}(\tilde{\beta}_p) + \text{bias}(\tilde{\beta}_p)^2 \\
  &amp;= \sigma^2({X}^\mathrm{T}_pX_p)^{-1} + A\beta_q{\beta}^\mathrm{T}_q{A}^\mathrm{T}.
\end{align*}\]</span> For the full model, <span class="math display">\[
  \text{MSE}(\hat{\beta}_p) =
  \text{var}(\hat{\beta}_p) =
  \sigma^2({X}^\mathrm{T}_pX_p)^{-1} +
  \sigma^2 A[ {X}^\mathrm{T}_q( I-P_p )X_q ]^{-1}{A}^\mathrm{T},
\]</span> If <span class="math inline">\(\text{MSE}(\hat{\beta}_p) - \text{MSE}(\tilde{\beta}_p)\)</span> is positive semi-definite, then the mean squared error has decreased upon the removal of the regressors in <span class="math inline">\(X_q\)</span>.</p>
</section>
</section>
<section id="model-comparison" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="model-comparison"><span class="header-section-number">3.3.2</span> Model Comparison</h3>
<p>We have already compared models in Chapter 1 with the partial F-test. However, for that test to make sense, we require the models to be nested–i.e.&nbsp;the larger model must contain all of the parameters of the smaller model. But, given a model <span class="math display">\[
  y = \beta_0 + \beta_1x_1 +\ldots+ \beta_px_p + \varepsilon,
\]</span> we may want to compare two different subset models that are not nested. Hence, we have some different measures to consider.</p>
<p>Note that ideally, we would compare all possible subset models. However, given <span class="math inline">\(p\)</span> regressors, there are <span class="math inline">\(2^p\)</span> different models to consider, which will often be computationally infeasible. Hence, we will consider two approaches to model selection that avoid this combinatorial problem.</p>
<div id="rem-subsetIntercept" class="proof remark">
<p><span class="proof-title"><em>Remark 3.3</em>. </span>To avoid confusion and awkward notation, assume that all subset models will always contain the intercept term <span class="math inline">\(\beta_0\)</span></p>
</div>
<section id="residual-sum-of-squares" class="level4" data-number="3.3.2.1">
<h4 data-number="3.3.2.1" class="anchored" data-anchor-id="residual-sum-of-squares"><span class="header-section-number">3.3.2.1</span> Residual Sum of Squares</h4>
<p>For two subset models with <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> regressors, respectively, with <span class="math inline">\(p_1&lt;p\)</span> and <span class="math inline">\(p_2&lt;p\)</span>, we can compare the mean residual sum of squares for each <span class="math display">\[
  \frac{SS_\text{res}(p_1)}{n-p_1-1}
  ~~~\text{ vs }~~~
  \frac{SS_\text{res}(p_2)}{n-p_2-1}
\]</span> and choose the model with the smaller value.</p>
<p>We know from before that the mean of the residual sum of squares for the full model, <span class="math inline">\(SS_\text{res}/(n-p-1)\)</span>, is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>. Similar to the calculations in the previous section, we can show that <span class="math display">\[
  \mathrm{E}\left(\frac{SS_\text{res}(p_1)}{n-p_1-1}\right) \ge \sigma^2
  ~~~\text{ and }~~~
  \mathrm{E}\left(\frac{SS_\text{res}(p_2)}{n-p_2-1}\right) \ge \sigma^2,
\]</span> which is that these estimators for subset models are upwardly biased.</p>
</section>
<section id="mallows-c_p" class="level4" data-number="3.3.2.2">
<h4 data-number="3.3.2.2" class="anchored" data-anchor-id="mallows-c_p"><span class="header-section-number">3.3.2.2</span> Mallows’ <span class="math inline">\(C_p\)</span></h4>
<p>We can also compare different models by computing Mallows’ <span class="math inline">\(C_p\)</span>. The goal of this value is to choose the model the minimizes the mean squared prediction error, which is <span class="math display">\[
  MSPE = \sum_{i=1}^n \frac{\mathrm{E}\left(\tilde{y}_i-\mathrm{E}y_i\right)^2}{\sigma^2}
\]</span> where <span class="math inline">\(\tilde{y_i}\)</span> is the <span class="math inline">\(i\)</span>th fitted value of the submodel and <span class="math inline">\(\mathrm{E}y_i\)</span> is the <span class="math inline">\(i\)</span>th fitted value of the true model. Furthermore, let <span class="math inline">\(\hat{y}_i\)</span> be the <span class="math inline">\(i\)</span>th fitted value for the full model. This is the expected squared difference between what the submodel predicts and what the real value is. As usual with mean squared errors in statistics, we rewrite this in terms of the variance plus the squared bias, which is <span class="math display">\[\begin{align*}
  MSPE
  &amp;= \frac{1}{\sigma^2}\sum_{i=1}^n \left[
    \mathrm{E}\left(\tilde{y}_i-\mathrm{E}\tilde{y}_i+\mathrm{E}\tilde{y}_i-\mathrm{E}y_i\right)^2
  \right] \\
  &amp;= \frac{1}{\sigma^2}\sum_{i=1}^n \left[
    \mathrm{E}\left(\tilde{y}_i-\mathrm{E}\tilde{y}_i\right)^2+({\mathrm{E}\tilde{y}_i-\mathrm{E}y_i})^2
  \right] \\
  &amp;= \frac{1}{\sigma^2}\sum_{i=1}^n \left[
    \mathrm{Var}\left(\tilde{y}_i\right)+\text{bias}(\tilde{y}_i)^2
  \right]
\end{align*}\]</span></p>
<p>Recall that the variance of the fitted values for the full model is <span class="math inline">\(\mathrm{Var}\left(\hat{y}\right) = \sigma^2P_x\)</span> where <span class="math inline">\(P_x = X({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T}\)</span>. For a submodel with <span class="math inline">\(p_1&lt;p\)</span> regressors and design matrix <span class="math inline">\(X_{p_1}\)</span>, we get the similar <span class="math inline">\(\mathrm{Var}\left(\tilde{y}\right) = \sigma^2X_{p_1}({X}^\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\mathrm{T}_{p_1}\)</span>. As <span class="math inline">\(X_{p_1}({X}^\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\mathrm{T}_{p_1}\)</span> is a rank <span class="math inline">\(p_1+1\)</span> projection matrix, we have that <span class="math display">\[
  \sum_{i=1}^n \mathrm{Var}\left(\tilde{y}_i\right) =
  \sigma^2\mathrm{tr}\left(X_{p_1}({X}^\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\mathrm{T}_{p_1}\right) =
  \sigma^2(p_1+1).
\]</span></p>
<p>For the bias term, consider the expected residual sum of squares for the submodel: <span class="math display">\[\begin{align*}
  \mathrm{E}\left(SS_\text{res}(p_1)\right)
  &amp;= \mathrm{E}\sum_{i=1}^n ( y_i - \tilde{y}_i )^2 \\
  &amp;= \mathrm{E}\sum_{i=1}^n (
     y_i -
     \mathrm{E}{\tilde{y}_i} + \mathrm{E}{\tilde{y}_i} -
     \mathrm{E}{{y}_i} + \mathrm{E}{{y}_i}
     - \tilde{y}_i )^2 \\
  &amp;= \sum_{i=1}^n\left[
    \mathrm{Var}\left(\tilde{r}_i\right) + (\mathrm{E}\tilde{y}_i-\mathrm{E}y_i)^2  
  \right]\\
  &amp;=  (n-p_1-1)\sigma^2 + \sum_{i=1}^n \text{bias}(\tilde{y}_i)^2.
\end{align*}\]</span> Hence, rearranging the terms above gives <span class="math display">\[
  \sum_{i=1}^n \text{bias}(\tilde{y}_i)^2
  = \mathrm{E}\left(SS_\text{res}(p_1)\right) - (n-p_1-1).
\]</span> Combining the bias and the variance terms derived above results in Mallows’ <span class="math inline">\(C_p\)</span> statistic for a submodel with <span class="math inline">\(p_1&lt;p\)</span> regressors: %\begin{multline<em>}</em> <span class="math display">\[
  C_{p_1} =
  \frac{\mathrm{E}\left(SS_\text{res}(p_1)\right)}{\sigma^2} - n+2p_1+2 \approx%\\\approx
  \frac{SS_\text{res}(p_1)}{SS_\text{res}/(n-p-1)} - n+2p_1+2.
\]</span> %\end{multline} Here, we estimate <span class="math inline">\(\mathrm{E}\left(SS_\text{res}(p_1)\right)\)</span> by <span class="math inline">\(SS_\text{res}(p_1)\)</span> and estimate <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(SS_\text{res}/(n-p-1)\)</span>.</p>
<div id="rem-MallowsCp" class="proof remark">
<p><span class="proof-title"><em>Remark 3.4</em>. </span>Note that if we compute Mallows’ <span class="math inline">\(C_p\)</span> for the full model, we get <span class="math display">\[
    C_p  
    = \frac{SS_\text{res}}{SS_\text{res}/(n-p-1)} - n+2p+2
    = p+1.
  \]</span> Hence, Mallows’ <span class="math inline">\(C_p\)</span> in this case is just the number of parameters in the model. In general, we want to find submodels with <span class="math inline">\(C_p\)</span> value smaller than <span class="math inline">\(p+1\)</span>.</p>
</div>
</section>
<section id="information-criteria" class="level4" data-number="3.3.2.3">
<h4 data-number="3.3.2.3" class="anchored" data-anchor-id="information-criteria"><span class="header-section-number">3.3.2.3</span> Information Criteria</h4>
<p>Information criteria are concerned with quantifying the amount of information in a model. With such a measure, we can choose a model that optimizes this measurement. A main requirement for these methods is that the response <span class="math inline">\(y\)</span> is the same. Hence, we should not use the measures below when comparing transformed models–e.g.&nbsp;different linearized models–without the necessary modifications.</p>
<p>The first such measure is the Akaike Information Criterion or AIC, which is a measure of the entropy of a model. Its general definition is <span class="math display">\[
  \text{AIC} = -2\log(\text{Likelihood}) +
  2(\text{\# parameters})
\]</span> where <span class="math inline">\(p\)</span> is the number of parameters in the model. This can be thought of a measurement of how much information is lost when modelling complex data with a <span class="math inline">\(p\)</span> parameter model. Hence, the model with the minimal AIC will be optimal in some sense.</p>
<p>In our least squares regression case with normally distributed errors, <span class="math display">\[
  \text{AIC} = n\log(SS_\text{res}/n) + 2(p+1)
\]</span> where <span class="math inline">\(p+1\)</span> is for the <span class="math inline">\(p\)</span> regressors and 1 intercept term. Thus, adding more regressors will decrease <span class="math inline">\(SS_\text{res}\)</span> but will increase <span class="math inline">\(p\)</span>. The goal is to find a model with the minimal AIC. This can be shown to give the same ordering as Mallows’ <span class="math inline">\(C_p\)</span> when the errors are normally distributed.</p>
<p>The second such measure is the closely related Bayesian Information Criterion or BIC, which, in general, is <span class="math display">\[
  \text{BIC} = -2\log(\text{Likelihood}) + (\text{\# parameters})\log n.
\]</span> In the linear regression setting with normally distributed errors, <span class="math display">\[
  \text{BIC} = n\log(SS_\text{res}/n) + (p+1)\log n.
\]</span></p>
<div id="rem-AICvBIC" class="proof remark">
<p><span class="proof-title"><em>Remark 3.5</em>. </span>Using AIC versus using BIC for model selection can sometimes result in different final choices. In some cases, one may be preferred, but often both can be tried and discrepancies, if they exist, can be reported.</p>
<p>There are also other information criterion that are not as common in practise such as the Deviation Information Criterion (DIC) and the Focused Information Criterion (FIC).</p>
</div>
</section>
</section>
<section id="forward-and-backward-selection" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="forward-and-backward-selection"><span class="header-section-number">3.3.3</span> Forward and Backward Selection</h3>
<p>Ideally, we choose a measure for model selection from the previous section and then compare all possible models. However, for <span class="math inline">\(p\)</span> possible regressors, this will result in <span class="math inline">\(2^p\)</span> models to check, which may be computationally infeasible. Hence, there are iterative approaches that can be effective.</p>
<p><em>Forward selection</em> is the process of starting with the constant model <span class="math display">\[
  y = \beta_0 + \varepsilon
\]</span> and choosing the best of the <span class="math inline">\(p\)</span> regressors with respect to the model selection criterion. This gives <span class="math display">\[
  y = \beta_0 + \beta_1x_1 + \varepsilon.
\]</span> This process will continue to add terms to the model as long as it results in an improvement in the criterion. For example, computing the AIC at each step.</p>
<p><em>Backwards selection</em> is the reverse of forward selection. In this case, the algorithm begins with the full model, <span class="math display">\[
  y = \beta_0 + \beta_1x_1 + \ldots + \beta_p x_p +\varepsilon,
\]</span> and iteratively removes the regressor that gives the biggest improvement in the model selection criterion. If the best choice is to remove no regressors, then the process terminates.</p>
<p>A third option is <em>stepwise selection</em>, which incorporates both forward and backward steps. In this case, we begin with the constant model as in forward selection. However, at every step, we choose either to add a new regressor to our model or remove one that is already in the model depending on which choice improves the criterion the most.</p>
<section id="variable-selection-example" class="level4" data-number="3.3.3.1">
<h4 data-number="3.3.3.1" class="anchored" data-anchor-id="variable-selection-example"><span class="header-section-number">3.3.3.1</span> Variable Selection Example</h4>
<p>Consider the same example as in the spline section where <span class="math inline">\(x\in[0,2]\)</span> and <span class="math display">\[
  y = 2 + 3x - 4x^5 + x^7 + \varepsilon
\]</span> with a sample of <span class="math inline">\(n=41\)</span> observations. We can fit two regression models, an empty and a saturated model, respectively, <span class="math display">\[
  y = \beta_0 +\varepsilon~\text{ and }~
  y = \beta_0 + \sum_{i=1}^7 \beta_i x^i + \varepsilon.
\]</span> and use the <code>R</code> function <code>step( )</code> to choose a best model with respect to AIC.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">256</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Data from a degree-7 polynomial</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>xx  <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span>,<span class="fl">0.05</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>len <span class="ot">=</span> <span class="fu">length</span>(xx)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>yy  <span class="ot">=</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>xx <span class="sc">-</span> <span class="dv">4</span><span class="sc">*</span>xx<span class="sc">^</span><span class="dv">5</span> <span class="sc">+</span> xx<span class="sc">^</span><span class="dv">7</span> <span class="sc">+</span> <span class="fu">rnorm</span>(len,<span class="dv">0</span>,<span class="dv">4</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the null and saturated models</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: never ever fit a polynomial model</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#       like this.  We are trying to create</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#       a model with high multicollinearity</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#       for educational purposes. </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>md0 <span class="ot">=</span> <span class="fu">lm</span>(yy<span class="sc">~</span><span class="dv">1</span>);</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>md7 <span class="ot">=</span> <span class="fu">lm</span>(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  yy<span class="sc">~</span>xx<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">5</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">6</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">7</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ 1)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.4234  -3.9053   0.8976   4.0282   9.4048 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   0.6483     0.7630    0.85    0.401

Residual standard error: 4.886 on 40 degrees of freedom</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md7)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ xx + I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + 
    I(xx^6) + I(xx^7))

Residuals:
    Min      1Q  Median      3Q     Max 
-7.7630 -2.3611 -0.7164  2.2562  7.3220 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    0.529      3.554   0.149    0.883
xx           -26.747     70.159  -0.381    0.705
I(xx^2)      271.061    434.342   0.624    0.537
I(xx^3)     -799.042   1158.236  -0.690    0.495
I(xx^4)     1124.811   1557.892   0.722    0.475
I(xx^5)     -824.786   1108.239  -0.744    0.462
I(xx^6)      300.118    397.719   0.755    0.456
I(xx^7)      -42.561     56.669  -0.751    0.458

Residual standard error: 3.982 on 33 degrees of freedom
Multiple R-squared:  0.4518,    Adjusted R-squared:  0.3356 
F-statistic: 3.886 on 7 and 33 DF,  p-value: 0.003437</code></pre>
</div>
</div>
<p>First, we note that this (non-orthogonal) polynomial model is very poorly specified. That is, the estimated coefficients vary wildly and are effectively trying to counter balance each other. None of our t-statistics are significant meaning that we can remove any of these terms individually without harming the fit of the model. The F-statistic is significant indicating that, globally, this model is significantly reducing the residual sum of squares.</p>
<p>First, we apply backwards variable selection. The result is an AIC that drops from 120.42 to 113.29 and the following fitted model: <span class="math display">\[
  y = 0.56 + 20.47x^2 - 18.59 x^3 + 1.09 x^6.
\]</span> In this case, we did not recover the model that was used to generate the data. However, this one still fits the noisy data well.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a backward variable selection</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>md.bck <span class="ot">=</span> <span class="fu">step</span>(md7)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=120.42
yy ~ xx + I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6) + I(xx^7)

          Df Sum of Sq    RSS    AIC
- xx       1    2.3052 525.69 118.60
- I(xx^2)  1    6.1770 529.56 118.90
- I(xx^3)  1    7.5484 530.93 119.00
- I(xx^4)  1    8.2678 531.65 119.06
- I(xx^5)  1    8.7846 532.17 119.10
- I(xx^7)  1    8.9462 532.33 119.11
- I(xx^6)  1    9.0311 532.42 119.12
&lt;none&gt;                 523.39 120.42

Step:  AIC=118.6
yy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6) + I(xx^7)

          Df Sum of Sq    RSS    AIC
- I(xx^7)  1    7.4088 533.10 117.17
- I(xx^6)  1    7.9536 533.64 117.21
- I(xx^5)  1    8.3113 534.00 117.24
- I(xx^4)  1    8.7075 534.40 117.27
- I(xx^3)  1    9.8197 535.51 117.36
- I(xx^2)  1   13.0549 538.75 117.60
&lt;none&gt;                 525.69 118.60

Step:  AIC=117.17
yy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6)

          Df Sum of Sq    RSS    AIC
- I(xx^5)  1    1.5784 534.68 115.29
- I(xx^4)  1    1.5872 534.69 115.29
- I(xx^6)  1    2.1726 535.27 115.34
- I(xx^3)  1    2.6951 535.79 115.38
- I(xx^2)  1    6.2706 539.37 115.65
&lt;none&gt;                 533.10 117.17

Step:  AIC=115.29
yy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^6)

          Df Sum of Sq    RSS    AIC
- I(xx^4)  1    0.0096 534.69 113.29
- I(xx^3)  1    3.4790 538.16 113.56
- I(xx^6)  1    7.0789 541.76 113.83
- I(xx^2)  1   12.5541 547.23 114.24
&lt;none&gt;                 534.68 115.29

Step:  AIC=113.29
yy ~ I(xx^2) + I(xx^3) + I(xx^6)

          Df Sum of Sq    RSS    AIC
&lt;none&gt;                 534.69 113.29
- I(xx^2)  1    143.39 678.08 121.03
- I(xx^3)  1    197.82 732.50 124.20
- I(xx^6)  1    229.28 763.96 125.92</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.bck)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ I(xx^2) + I(xx^3) + I(xx^6))

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3054 -2.4655  0.0047  2.3310  6.7750 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   0.5640     1.3500   0.418 0.678528    
I(xx^2)      20.4701     6.4984   3.150 0.003226 ** 
I(xx^3)     -18.5896     5.0245  -3.700 0.000698 ***
I(xx^6)       1.0863     0.2727   3.983 0.000306 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.801 on 37 degrees of freedom
Multiple R-squared:   0.44, Adjusted R-squared:  0.3946 
F-statistic: 9.691 on 3 and 37 DF,  p-value: 7.438e-05</code></pre>
</div>
</div>
<p>Next, doing forward selection, we drop the AIC from 131.07 to 113.36, which is almost the same ending AIC as in the backwards selection performed above. In this case, the fitted model is <span class="math display">\[
  y = 0.78 + 17.19x^2 - 14.86 x^3 + 0.41 x^7.
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward variable selection</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>md.fwd <span class="ot">=</span> <span class="fu">step</span>(md0,<span class="at">direction =</span> <span class="st">"forward"</span>,<span class="at">scope =</span> <span class="fu">list</span>(<span class="at">upper=</span>md7))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=131.07
yy ~ 1

          Df Sum of Sq    RSS    AIC
+ I(xx^2)  1   189.254 765.55 124.01
+ I(xx^3)  1   178.275 776.53 124.59
+ xx       1   151.475 803.33 125.98
+ I(xx^4)  1   150.951 803.85 126.01
+ I(xx^5)  1   121.568 833.23 127.48
+ I(xx^6)  1    95.317 859.48 128.75
+ I(xx^7)  1    73.561 881.24 129.78
&lt;none&gt;                 954.80 131.06

Step:  AIC=124.01
yy ~ I(xx^2)

          Df Sum of Sq    RSS    AIC
+ I(xx^7)  1    44.476 721.07 123.55
&lt;none&gt;                 765.55 124.01
+ I(xx^6)  1    33.044 732.50 124.20
+ I(xx^5)  1    21.043 744.50 124.86
+ xx       1    15.086 750.46 125.19
+ I(xx^4)  1     9.728 755.82 125.48
+ I(xx^3)  1     1.583 763.96 125.92

Step:  AIC=123.55
yy ~ I(xx^2) + I(xx^7)

          Df Sum of Sq    RSS    AIC
+ I(xx^3)  1    185.46 535.61 113.36
+ I(xx^4)  1    178.81 542.26 113.87
+ xx       1    170.88 550.19 114.46
+ I(xx^5)  1    170.31 550.76 114.51
+ I(xx^6)  1    161.22 559.85 115.18
&lt;none&gt;                 721.07 123.55

Step:  AIC=113.36
yy ~ I(xx^2) + I(xx^7) + I(xx^3)

          Df Sum of Sq    RSS    AIC
&lt;none&gt;                 535.61 113.36
+ xx       1   2.72717 532.89 115.15
+ I(xx^4)  1   1.21296 534.40 115.27
+ I(xx^5)  1   1.01714 534.60 115.28
+ I(xx^6)  1   0.93707 534.68 115.29</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.fwd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ I(xx^2) + I(xx^7) + I(xx^3))

Residuals:
    Min      1Q  Median      3Q     Max 
-8.0984 -2.3891  0.0842  2.4374  6.7973 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   0.7795     1.3253   0.588 0.560027    
I(xx^2)      17.1905     5.7868   2.971 0.005195 ** 
I(xx^7)       0.4142     0.1043   3.972 0.000317 ***
I(xx^3)     -14.8630     4.1525  -3.579 0.000984 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.805 on 37 degrees of freedom
Multiple R-squared:  0.439, Adjusted R-squared:  0.3935 
F-statistic: 9.652 on 3 and 37 DF,  p-value: 7.672e-05</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="penalized-regressions" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="penalized-regressions"><span class="header-section-number">3.4</span> Penalized Regressions</h2>
<p>No matter how we design our model, thus far we have always computed the least squares estimator, <span class="math inline">\(\hat{\beta}\)</span>, by minimizing the sum of squared errors <span class="math display">\[
  \hat{\beta} = \underset{\beta\in\mathbb{R}^{p+1}}{\arg\min}
  \left\{
  \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2
  \right\}.
\]</span> This is an unbiased estimator for <span class="math inline">\(\beta\)</span>. However, as we have seen previously, the variance of this estimator can be quite large. Hence, we <em>shrink</em> the estimator towards zero adding bias but decreasing the variance. General idea of shrinkage is attributed to Stein (1956) and the so-called <a href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator">Stein Estimator</a>. In the context of regression, we add a penalty term to the above minimization to get a new estimator <span class="math display">\[
  \hat{\beta}^\text{pen} = \underset{\beta\in\mathbb{R}^{p+1}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2
    + \text{penalty}(\beta)
  \right\},
\]</span> which increases as <span class="math inline">\(\beta\)</span> increases thus attempting to enforce smaller choices for the estimated parameters. We will consider some different types of penalized regression. In <code>R</code>, the <a href="http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html">glmnet</a> package has a lot of functionality to fit different types of penalized general linear models ::: {#rem-penalIntercept} We generally do not want to penalize the intercept term <span class="math inline">\(\beta_0\)</span>. Often to account for this, the regressors and response are centred–i.e.&nbsp;<span class="math inline">\(Y\)</span> is replaced with <span class="math inline">\(Y - \bar{Y}\)</span> and each <span class="math inline">\(X_j\)</span> is replaced with <span class="math inline">\(X_j-\bar{X_j}\)</span> for <span class="math inline">\(j=1,\ldots,p\)</span>–in order to set the intercept term to zero. :::</p>
<section id="ridge-regression" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="ridge-regression"><span class="header-section-number">3.4.1</span> Ridge Regression</h3>
<p>The first method we consider is ridge regression, which arose in statistics in the 1970’s—see Hoerl, A.E.; R.W. Kennard (1970)—but similar techniques arise in other areas of computational mathematics. In short, a quadratic penalty is applied to the least squares estimator resulting in <span class="math display">\[
  \hat{\beta}^\text{R}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p \beta_j^2
  \right\}
\]</span> for any <span class="math inline">\(\lambda\ge0\)</span>. When <span class="math inline">\(\lambda=0\)</span>, we have the usual least squares estimator. As <span class="math inline">\(\lambda\)</span> grows, the <span class="math inline">\(\beta\)</span>’s are more strongly penalized.</p>
<p>To solve for <span class="math inline">\(\hat{\beta}^\text{R}_\lambda\)</span>, we proceed as before with the least squares estimator <span class="math inline">\(\hat{\beta}\)</span> by setting the partial derivatives equal to zero <span class="math display">\[\begin{align*}
  0 &amp;= \frac{\partial}{\partial\beta_k}
  \left\{
    \sum_{i=1}^n( y_i - \beta_1x_{i,1} -\ldots-\beta_px_{i,p} )^2 +
    \lambda\sum_{j=1}^p \beta_j^2
  \right\}.
\end{align*}\]</span> This results in the system of equations <span class="math display">\[
  {X}^\mathrm{T}Y - ({X}^\mathrm{T}X)\hat{\beta}^\text{R}_\lambda-
  \lambda\hat{\beta}^\text{R}_\lambda= 0
\]</span> with the ridge estimator being $ ^_= (X + I_n)^{-1}Y. $</p>
<p>The matrix <span class="math inline">\({X}^\mathrm{T}X\)</span> is positive semi-definite even when <span class="math inline">\(p&gt;n\)</span>–i.e.&nbsp;the number of parameters exceeds the sample size. Hence, any positive value <span class="math inline">\(\lambda\)</span> will make <span class="math inline">\({X}^\mathrm{T}X + \lambda I_n\)</span> invertible as it adds the positive constant <span class="math inline">\(\lambda\)</span> to all of the eigenvalues. Increasing the value of <span class="math inline">\(\lambda\)</span> will increase the numerical stability of the estimator–i.e.&nbsp;decrease the condition number of the matrix. Furthermore, it will decrease the variance of the estimator while increasing the bias. It can also be shown that the bias of <span class="math inline">\(\hat{\beta}^\text{R}_\lambda\)</span> is <span class="math display">\[
  \mathrm{E}{\hat{\beta}^\text{R}_\lambda} - \beta =
  %(\TT{X}X + \lmb I_n)^{-1}\TT{X}X\beta - \beta =
  %(\TT{X}X + \lmb I_n)^{-1}( \TT{X}X - \TT{X}X - \lmb I_n )\beta
  -\lambda({X}^\mathrm{T}X + \lambda I_n)^{-1}\beta,
\]</span> which implies that the estimator does, in fact, shrink towards zero as <span class="math inline">\(\lambda\)</span> increases.</p>
</section>
<section id="best-subset-regression" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="best-subset-regression"><span class="header-section-number">3.4.2</span> Best Subset Regression</h3>
<p>Another type of penalty related to the variable selection techniques from the previous section is the Best Subset Regression approach, which counts the number of non-zero <span class="math inline">\(\beta\)</span>’s and adds a larger penalty as more terms are included in the model. The optimization looks like <span class="math display">\[
  \hat{\beta}^\text{B}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p \boldsymbol{1}\!\left[\beta_j\ne 0\right]
  \right\}.
\]</span> The main problem with this method is that the optimization is non-convex and becomes severely difficult to compute in practice. This is why the forwards and backwards selection methods are used for variable selection.</p>
</section>
<section id="lasso" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="lasso"><span class="header-section-number">3.4.3</span> LASSO</h3>
<p>The last method we consider is the Least Absolute Shrinkage and Selection Operator, which is commonly referred to as just LASSO. This was introduced by Tibshirani (1996) and has since been applied to countless areas of statistics. The form is quite similar to ridge regression with one small but profound modification, <span class="math display">\[
  \hat{\beta}^\text{L}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p \lvert\beta_j\rvert
  \right\},
\]</span> which is that the penalty term is now the sum of the absolute values instead of a sum of squares.</p>
<p>The main reason for why this technique is popular is that it combines shrinkage methods like ridge regression with variable selection and still results in a convex optimization problem. Delving into the properties of this estimator requires convex analysis and will be left for future investigations.</p>
</section>
<section id="elastic-net" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="elastic-net"><span class="header-section-number">3.4.4</span> Elastic Net</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Elastic_net_regularizatio">elastic net regularization</a> method combines both ridge and lasso regression into one methodology. Here, we include a penalty term for each of the two methods: <span class="math display">\[
  \hat{\beta}^\text{EN}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda_1 \sum_{j=1}^p \lvert\beta_j\rvert +
    \lambda_2 \sum_{j=1}^p \beta_j^2
  \right\}.
\]</span> This method has two tuning parameters <span class="math inline">\(\lambda_1\ge0\)</span> and <span class="math inline">\(\lambda_2\ge0\)</span>. In the <code>R</code> library <code>glmnet</code>, a <em>mixing</em> parameter <span class="math inline">\(\alpha\)</span> and a <em>scale</em> parameter <span class="math inline">\(\lambda\)</span> is specified to get <span class="math display">\[
  \hat{\beta}^\text{EN}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p\left[
      \alpha \lvert\beta_j\rvert +
      \frac{1-\alpha}{2} \beta_j^2
    \right]
  \right\}.
\]</span> The intuition behind this approach is to combine the strengths of both ridge and lasso regression. Namely, ridge regression shrinks the coefficients towards zero reducing the variance while lasso selects a subset of the parameters to remain in the model.</p>
</section>
<section id="penalized-regression-an-example" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="penalized-regression-an-example"><span class="header-section-number">3.4.5</span> Penalized Regression: An Example</h3>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt2_ModAss.html" class="pagination-link" aria-label="Model Assumptions and Choices">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model Assumptions and Choices</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>