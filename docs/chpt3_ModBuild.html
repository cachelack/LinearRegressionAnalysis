<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Model Building – STAT 378: Linear Regression Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt4_AdvRegress.html" rel="next">
<link href="./chpt2_ModAss.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-364982630eef5352dd1537128a8ed5cb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chpt3_ModBuild.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model Building</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 378: Linear Regression Analysis</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt1_ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt2_ModAss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model Assumptions and Choices</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt3_ModBuild.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model Building</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt4_AdvRegress.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Advanced Regression Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apen1_LinearAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apen2_ProbDist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Some Useful Probability Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">3.1</span> Introduction</a></li>
  <li><a href="#multicollinearity" id="toc-multicollinearity" class="nav-link" data-scroll-target="#multicollinearity"><span class="header-section-number">3.2</span> Multicollinearity</a>
  <ul class="collapse">
  <li><a href="#identifying-multicollinearity" id="toc-identifying-multicollinearity" class="nav-link" data-scroll-target="#identifying-multicollinearity"><span class="header-section-number">3.2.1</span> Identifying Multicollinearity</a></li>
  <li><a href="#correcting-multicollinearity" id="toc-correcting-multicollinearity" class="nav-link" data-scroll-target="#correcting-multicollinearity"><span class="header-section-number">3.2.2</span> Correcting Multicollinearity</a></li>
  </ul></li>
  <li><a href="#variable-selection" id="toc-variable-selection" class="nav-link" data-scroll-target="#variable-selection"><span class="header-section-number">3.3</span> Variable Selection</a>
  <ul class="collapse">
  <li><a href="#subset-models" id="toc-subset-models" class="nav-link" data-scroll-target="#subset-models"><span class="header-section-number">3.3.1</span> Subset Models</a></li>
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison"><span class="header-section-number">3.3.2</span> Model Comparison</a></li>
  <li><a href="#forward-and-backward-selection" id="toc-forward-and-backward-selection" class="nav-link" data-scroll-target="#forward-and-backward-selection"><span class="header-section-number">3.3.3</span> Forward and Backward Selection</a></li>
  <li><a href="#hypothermic-half-marathon-data-revisited" id="toc-hypothermic-half-marathon-data-revisited" class="nav-link" data-scroll-target="#hypothermic-half-marathon-data-revisited"><span class="header-section-number">3.3.4</span> Hypothermic Half Marathon Data, Revisited</a></li>
  </ul></li>
  <li><a href="#penalized-regressions" id="toc-penalized-regressions" class="nav-link" data-scroll-target="#penalized-regressions"><span class="header-section-number">3.4</span> Penalized Regressions</a>
  <ul class="collapse">
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression"><span class="header-section-number">3.4.1</span> Ridge Regression</a></li>
  <li><a href="#best-subset-regression" id="toc-best-subset-regression" class="nav-link" data-scroll-target="#best-subset-regression"><span class="header-section-number">3.4.2</span> Best Subset Regression</a></li>
  <li><a href="#lasso" id="toc-lasso" class="nav-link" data-scroll-target="#lasso"><span class="header-section-number">3.4.3</span> LASSO</a></li>
  <li><a href="#elastic-net" id="toc-elastic-net" class="nav-link" data-scroll-target="#elastic-net"><span class="header-section-number">3.4.4</span> Elastic Net</a></li>
  <li><a href="#penalized-regression-an-example" id="toc-penalized-regression-an-example" class="nav-link" data-scroll-target="#penalized-regression-an-example"><span class="header-section-number">3.4.5</span> Penalized Regression: An Example</a></li>
  </ul></li>
  <li><a href="#us-communities-and-crime-dataset" id="toc-us-communities-and-crime-dataset" class="nav-link" data-scroll-target="#us-communities-and-crime-dataset"><span class="header-section-number">3.5</span> US Communities and Crime Dataset</a>
  <ul class="collapse">
  <li><a href="#variable-selection-1" id="toc-variable-selection-1" class="nav-link" data-scroll-target="#variable-selection-1"><span class="header-section-number">3.5.1</span> Variable Selection</a></li>
  <li><a href="#penalized-regression" id="toc-penalized-regression" class="nav-link" data-scroll-target="#penalized-regression"><span class="header-section-number">3.5.2</span> Penalized Regression</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model Building</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">3.1</span> Introduction</h2>
<p>The following sections will discuss various topics regarding constructing a good representative regression model for your data. Three main topics will be considered.</p>
<p>First, multicollinearity deals with the problem of linear relationships between your regressions. We already know that we require the columns of the design matrix to be linearly independent in order to solve for the least squares estimate. However, it is possible to have near dependencies among the columns. This can lead to numerical stability issues and unnecessary redundancy among the regressors.</p>
<p>Second, there are many different variable selection techniques in existence. Given a large number of regressors to can be included in a model, the question is, which should and which should not be included? We will discuss various techniques such as forward and backward selection as well as different tools for comparing models.</p>
<p>Third, penalized regression will be discussed. This section introduces two modern and quite powerful approaches to linear regression: ridge regression from the 1970’s and LASSO from the 1990’s. Both arise from modifying how we estimate the parameter vector <span class="math inline">\(\hat{\beta}\)</span>. Up until now, we have chosen <span class="math inline">\(\hat{\beta}\)</span> to minimize the sum of the squared error. Now, we will add a penalty term to this optimization problem, which will encourage choices of <span class="math inline">\(\hat{\beta}\)</span> with small-in-magnitude or just zero entries.</p>
</section>
<section id="multicollinearity" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="multicollinearity"><span class="header-section-number">3.2</span> Multicollinearity</h2>
<p>The concept of multicollinearity is intuitively simple. Say we have a model of the form <span class="math display">\[
  y = \beta_0 + \beta_1x_1 + \beta_2x_2 +\varepsilon.
\]</span> This results in a design matrix of the form <span class="math display">\[
  X = \begin{pmatrix}
    1 &amp; x_{1,1} &amp; x_{1,2} \\
    \vdots &amp; \vdots &amp; \vdots \\
    1 &amp; x_{n,1} &amp; x_{n,2}
  \end{pmatrix}
\]</span> Then, we can consider a new model of the form <span class="math display">\[
  x_2 = \alpha_0 + \alpha_1x_1 + \varepsilon.
\]</span> If this simple regression has a strong fit—e.g.&nbsp;A significant F-test or <span class="math inline">\(R^2\)</span> value—then the addition of the regressor <span class="math inline">\(x_2\)</span> to the original model is unnecessary as almost all of the explanatory information provided by <span class="math inline">\(x_2\)</span> with regards to predicting <span class="math inline">\(y\)</span> is already provided by <span class="math inline">\(x_1\)</span>. Hence, the inclusion of <span class="math inline">\(x_2\)</span> in our model is superfluous.</p>
<p>Taking a more mathematical approach, it can be shown that such near linear dependencies lead to a very high variance for the least squares estimator <span class="math inline">\(\hat{\beta}\)</span>. Furthermore, the magnitude of the vector is much larger than it should be.</p>
<p>Assuming that the errors have a covariance matrix <span class="math inline">\(\mathrm{Var}\left(\varepsilon\right) = \sigma^2I_n\)</span>, then we have from before that <span class="math display">\[\begin{multline*}
  \mathrm{Var}\left(\hat{\beta}\right)
  = \mathrm{Var}\left( ({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T}Y \right) = \\
  = ({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T} \mathrm{Var}\left(Y\right) X({X}^\mathrm{T}X)^{-1}
  = \sigma^2({X}^\mathrm{T}X)^{-1}.
\end{multline*}\]</span> With some effort, it can be shown that the diagonal entries of the matrix <span class="math inline">\(({X}^\mathrm{T}X)^{-1}\)</span> are equal to <span class="math inline">\((1-R^2_0)^{-1},\ldots,(1-R^2_p)^{-1}\)</span> where $ R_j^2 $ is the coefficient of determination for the model <span class="math display">\[
  x_j = \alpha_0 + \alpha_1x_1 +\ldots
  + \alpha_{j-1}x_{j-1} + \alpha_{j+1}x_{j+1} + \ldots
  + \alpha_{p}x_p + \varepsilon,
\]</span> which is trying to predict the <span class="math inline">\(j\)</span>th regressor by the other <span class="math inline">\(p-1\)</span> regressors. If the remaining regressors are good predictors for <span class="math inline">\(x_j\)</span>, then the value <span class="math inline">\(R_j^2\)</span> will be close to 1. Hence, <span class="math display">\[\mathrm{Var}\left(\hat{\beta}_j\right) = \frac{\sigma^2}{1-R_j^2}\]</span> will be very large.</p>
<p>Furthermore, this implies that the expected Euclidean distance between <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\beta\)</span> will be quite large as well. Indeed, we have <span class="math display">\[
  \mathrm{E}\left(
    {(\hat{\beta}-\beta)}^\mathrm{T}
    {(\hat{\beta}-\beta)}
  \right)
  = \sum_{i=0}^p \mathrm{E}\left( \hat{\beta}_i-\beta_i \right)^2
  = \sum_{i=0}^p \mathrm{Var}\left( \hat{\beta}_i \right)
  = \sigma^2 \mathrm{tr}\left(({X}^\mathrm{T}X)^{-1}\right)
\]</span> where <span class="math inline">\(\mathrm{tr}\left(\cdot\right)\)</span> denotes the trace of a matrix–i.e.&nbsp;the sum of the diagonal entries. Hence, if at least one of the <span class="math inline">\(R_j^2\)</span> is close to 1, then the expected distance from our estimator to the true <span class="math inline">\(\beta\)</span> will be quite large.</p>
<p>The trace of a matrix is also equal to the sum of its eigenvalues. Hence, if we denote the eigenvalues of <span class="math inline">\({X}^\mathrm{T}X\)</span> by <span class="math inline">\(\lambda_1,\ldots,\lambda_{p+1}\)</span>, then <span class="math display">\[
  \mathrm{tr}\left(({X}^\mathrm{T}X)^{-1}\right) = \sum_{i=1}^{p+1} \lambda_{i}^{-1}.
\]</span> Hence, an equivalent condition to check for multicollinearity is the presence of eigenvalues of <span class="math inline">\({X}^\mathrm{T}X\)</span> very close to zero, which would make the above sum very large.</p>
<section id="identifying-multicollinearity" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="identifying-multicollinearity"><span class="header-section-number">3.2.1</span> Identifying Multicollinearity</h3>
<p>To identify the presence of multicollinearity in our linear regression, there are many measures to consider.</p>
<p>We already established that near linear dependencies will result in large values for the diagonal entries of <span class="math inline">\(({X}^\mathrm{T}X)^{-1}\)</span>. These values are known as the <em>Variance Inflation Factors</em> and sometimes written as $ _i = (1-R_i<sup>2)</sup>{-1}. $</p>
<p>An interesting interpretation of the VIF is in terms of confidence intervals. Recall that for <span class="math inline">\(\beta_j\)</span>, we can construct a <span class="math inline">\(1-\alpha\)</span> confidence interval as <span class="math display">\[
  -t_{\alpha/2,n-p-1}\sqrt{ ({X}^\mathrm{T}X)^{-1}_{j,j}\frac{SS_\text{res}}{n-p-1} }
  \le \beta_j - \hat{\beta}_j \le
  t_{\alpha/2,n-p-1}\sqrt{ ({X}^\mathrm{T}X)^{-1}_{j,j}\frac{SS_\text{res}}{n-p-1} }.
\]</span> If all <span class="math inline">\(i\ne j\)</span> regressors are orthogonal to the <span class="math inline">\(j\)</span>th regressor, then <span class="math inline">\(R_j^2=0\)</span> and the term <span class="math inline">\(({X}^\mathrm{T}X)^{-1}_{j,j}=1\)</span>. Under multicollinearity, <span class="math inline">\(({X}^\mathrm{T}X)^{-1}_{j,j}\gg1\)</span>. Hence, the confidence interval is expanded by a factor of <span class="math inline">\(\sqrt{({X}^\mathrm{T}X)^{-1}_{j,j}}\)</span> when the regressors are not orthogonal.</p>
<p>We can alternatively examine the eigenvalues of the matrix <span class="math inline">\({X}^\mathrm{T}X\)</span>. Recall that finding the least squares estimator is equivalent to solving a system of linear equations of the form <span class="math display">\[
  {X}^\mathrm{T}X\hat{\beta} = {X}^\mathrm{T}Y.
\]</span> To measure to stability of a solution to a system of equations to small perturbations, a term referred to as the <em>condition number</em> is used. This term arises in more generality in numerical analysis; See <a href="https://en.wikipedia.org/wiki/Condition_number">Condition Number</a>. It is <span class="math display">\[
  \kappa = \lambda_{\max}/\lambda_{\min}
\]</span> where <span class="math inline">\(\lambda_{\max}\)</span> and <span class="math inline">\(\lambda_{\min}\)</span> are the maximal and minimal eigenvalues, respectively. According to Montgomery, Peck, &amp; Vining, values of <span class="math inline">\(\kappa\)</span> less than 100 are not significant whereas values greater than 1000 indicate severe multicollinearity.</p>
<p>If the minimal eigenvalue is very small, we can use the corresponding eigenvector to understand the nature of the linear dependency. That is, consider the eigenvector <span class="math inline">\(u = (u_0,u_1,\ldots,u_p)\)</span> for the matrix <span class="math inline">\({X}^\mathrm{T}X\)</span> corresponding to the eigenvalue <span class="math inline">\(\lambda_{\min}\)</span>. Recall that this implies that <span class="math display">\[
  ({X}^\mathrm{T}X) u = \lambda_{\min} u \approx 0,
\]</span> which is approximately zero because <span class="math inline">\(\lambda_{\min}\)</span> is close to zero. Hence, for regressors <span class="math inline">\(1,x_1,\ldots,x_p\)</span>, <span class="math display">\[
  u_0 + u_1x_1 + \ldots + u_p x_p \approx 0.
\]</span> Thus, we can use the eigenvectors with small eigenvalues to get a linear relationship between the regressors.</p>
<div id="rem-singluarValueDecomp" class="proof remark">
<p><span class="proof-title"><em>Remark 3.1</em>. </span>If you are familiar with the concept of the <a href="https://en.wikipedia.org/wiki/Singular-value_decomposition">Singular Value Decomposition</a>, then you could alternatively consider the ratio between the maximal and minimal singular values of the design matrix <span class="math inline">\(X\)</span>. Furthermore, you can also analyze the singular vectors instead of the eigen vectors.</p>
</div>
</section>
<section id="correcting-multicollinearity" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="correcting-multicollinearity"><span class="header-section-number">3.2.2</span> Correcting Multicollinearity</h3>
<p>Ideally, we would design a model such that the columns of the design matrix <span class="math inline">\(X\)</span> are linearly independent.<br>
Of course, in practise, this is often not achievable. When confronted with real world data, there are still some options available.</p>
<p>First, the regressors can be <em>respecified</em>. That is, if <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are near linearly related, then instead of including both terms in the model, we can include a single combination term like <span class="math inline">\(x_1x_2\)</span> or <span class="math inline">\((x_1+x_2)/2\)</span>. Second, one of the two variables can be dropped from the model, which will be discussed below when we consider variable selection.</p>
<p>More sophisticated solutions to this problem include penalized regression techniques, which we will discuss below. Also, principal components regression—See Montgomery, Peck, Vining Sections 9.5.4 for more on PC regression—and partial least squares are two other methods that can be applied to deal with multicollinear data.</p>
<div id="rem-biasVariance" class="proof remark">
<p><span class="proof-title"><em>Remark 3.2</em>. </span>A common thread among all of these alternatives is that they result in a biased estimate for <span class="math inline">\(\beta\)</span> unlike the usual least squares estimator. Often in statistics, we begin with unbiased estimators, but can often achieve a better estimator by adding a small amount of bias. This is the so-called <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias-variance tradeoff</a>.</p>
</div>
</section>
</section>
<section id="variable-selection" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="variable-selection"><span class="header-section-number">3.3</span> Variable Selection</h2>
<p>In general, if we have <span class="math inline">\(p\)</span> regressors, we may want to build a model consisting only of the best regressors for modelling the response variable. In some sense, we could compare all possible subset models. However, there are many issues with this, which we will address in the following subsections. First, what are the effects of removing regressors from your model? Second, how do we compare models if they are not nested? Third, there are <span class="math inline">\(2^p\)</span> possible models to consider. Exhaustively fitting and comparing all of these models may be computational impractical or impossible. Hence, how do we find a good subset of the regressors?</p>
<section id="subset-models" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="subset-models"><span class="header-section-number">3.3.1</span> Subset Models</h3>
<p>What happens to the model when we remove some regressors? Assume we have a sample of <span class="math inline">\(n\)</span> observations and <span class="math inline">\(p+q\)</span> regressors and want to remove <span class="math inline">\(q\)</span> of them.<br>
The full model would be <span class="math display">\[
  y = \beta_0 + \beta_1 x_1 + \ldots + \beta_{p+q}x_{p+q} +\varepsilon.
\]</span> This can be written in terms of the design matrix and partitioned over the two sets of regressors as <span class="math display">\[\begin{align*}
  Y &amp;= X\beta + \varepsilon\\
    &amp;= X_p\beta_p + X_q\beta_q + \varepsilon
\end{align*}\]</span> where <span class="math inline">\(X_p \in\mathbb{R}^{n\times p}\)</span>, <span class="math inline">\(X_q\in\mathbb{R}^{n\times q}\)</span>, <span class="math inline">\(\beta_p\in\mathbb{R}^p\)</span>, <span class="math inline">\(\beta_q\in\mathbb{R}^q\)</span>, and <span class="math display">\[
  X = \begin{pmatrix}
    X_p &amp; X_q
  \end{pmatrix},~~~~
  \beta = \begin{pmatrix}
    \beta_p \\ \beta_q
  \end{pmatrix}
\]</span></p>
<p>We have two models to compare. The first is the full model, <span class="math inline">\(Y = X\beta + \varepsilon\)</span>, where we denote the least squares estimator as <span class="math inline">\(\hat{\beta} = ({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T}Y\)</span> as usual with components <span class="math display">\[
  \hat{\beta} =
  \begin{pmatrix} \hat{\beta}_p \\ \hat{\beta}_q \end{pmatrix}.
\]</span> The second is the reduced model obtained by deleting <span class="math inline">\(q\)</span> regressors: <span class="math inline">\(Y = X_p\beta_p + \varepsilon\)</span>. The least squares estimator for this model will be denoted as <span class="math inline">\(\tilde{\beta}_p = ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_pY\)</span></p>
<section id="bias-may-increas" class="level4" data-number="3.3.1.1">
<h4 data-number="3.3.1.1" class="anchored" data-anchor-id="bias-may-increas"><span class="header-section-number">3.3.1.1</span> Bias may increas</h4>
<p>The first concern with the reduced model is that the estimator <span class="math inline">\(\tilde{\beta}_p\)</span> can be biased as <span class="math display">\[\begin{multline*}
  \mathrm{E}{ \tilde{\beta}_p } =
  ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p \mathrm{E}Y =
  ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p ( X_p\beta_p + X_q\beta_q ) =\\=
  \beta_p + ({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p X_q\beta_q =
  \beta_p + A\beta_q.
\end{multline*}\]</span> Hence, our reduced estimator is only unbiased in two cases. Case one is when <span class="math inline">\(A=0\)</span>, which occurs if the <span class="math inline">\(p\)</span> regressors and <span class="math inline">\(q\)</span> regressors are orthogonal resulting in <span class="math inline">\({X}^\mathrm{T}_p X_q=0\)</span>. Case two is when <span class="math inline">\(\beta_q=0\)</span>, which occurs if those regressors have no effect on the given response. If neither of these cases occurs, then <span class="math inline">\(A\beta_q\ne0\)</span> and represents the bias in our estimator <span class="math inline">\(\tilde{\beta}_p\)</span>. Note that the matrix <span class="math inline">\(A\)</span> is referred to as the <em>alias</em> matrix.</p>
</section>
<section id="variance-may-decrease" class="level4" data-number="3.3.1.2">
<h4 data-number="3.3.1.2" class="anchored" data-anchor-id="variance-may-decrease"><span class="header-section-number">3.3.1.2</span> Variance may decrease</h4>
<p>While deleting regressors can result in the addition of bias to our estimate, it can also result in a reduction in the variance of our estimator. Namely, <span class="math display">\[\begin{align*}
  \mathrm{Var}\left(\tilde{\beta}_p\right) &amp;=
  \sigma^2({X}^\mathrm{T}_pX_p)^{-1}, \text{ while }\\
  \mathrm{Var}\left(\hat{\beta}_p\right) &amp;=
  \sigma^2({X}^\mathrm{T}_pX_p)^{-1} +
  \sigma^2 A[ {X_q}^\mathrm{T}( I-P_p )X_q ]^{-1}{A}^\mathrm{T},
\end{align*}\]</span> where <span class="math inline">\(P_p = X_p({X}^\mathrm{T}_pX_p)^{-1}{X}^\mathrm{T}_p\)</span>. This expression can be derived via the formula for <a href="https://en.wikipedia.org/wiki/Invertible_matrix#Blockwise_inversion">inverting a block matrix</a>. The matrix <span class="math inline">\(A[ {X_q}^\mathrm{T}( I-P_p )X_q ]^{-1}{A}^\mathrm{T}\)</span> is symmetric positive semi-definite, so the variance for <span class="math inline">\(\hat{\beta}_p\)</span> can only be larger than <span class="math inline">\(\tilde{\beta}_p\)</span>.</p>
</section>
<section id="mse-may-or-may-not-improve" class="level4" data-number="3.3.1.3">
<h4 data-number="3.3.1.3" class="anchored" data-anchor-id="mse-may-or-may-not-improve"><span class="header-section-number">3.3.1.3</span> MSE may or may not improve</h4>
<p>Generally in statistics, when deciding whether or not the increase in the bias is worth the decrease in the variance, we consider the change in the <em>mean squared error</em> (MSE) of our estimate.<br>
This is, <span class="math display">\[\begin{align*}
  \text{MSE}(\tilde{\beta}_p)
  &amp;= \mathrm{E}\left(
        (\tilde{\beta}_p-\beta_p)
    {(\tilde{\beta}_p-\beta_p)}^\mathrm{T} \right) \\
  &amp;= \mathrm{E}\left(
        (\tilde{\beta}_p-\mathrm{E}\tilde{\beta}_p+\mathrm{E}\tilde{\beta}_p-\beta_p)
    {(\tilde{\beta}_p-\mathrm{E}\tilde{\beta}_p+\mathrm{E}\tilde{\beta}_p-\beta_p)}^\mathrm{T} \right) \\
  &amp;= \text{var}(\tilde{\beta}_p) + \text{bias}(\tilde{\beta}_p)^2 \\
  &amp;= \sigma^2({X}^\mathrm{T}_pX_p)^{-1} + A\beta_q{\beta}^\mathrm{T}_q{A}^\mathrm{T}.
\end{align*}\]</span> For the full model, <span class="math display">\[
  \text{MSE}(\hat{\beta}_p) =
  \text{var}(\hat{\beta}_p) =
  \sigma^2({X}^\mathrm{T}_pX_p)^{-1} +
  \sigma^2 A[ {X}^\mathrm{T}_q( I-P_p )X_q ]^{-1}{A}^\mathrm{T},
\]</span> If <span class="math inline">\(\text{MSE}(\hat{\beta}_p) - \text{MSE}(\tilde{\beta}_p)\)</span> is positive semi-definite, then the mean squared error has decreased upon the removal of the regressors in <span class="math inline">\(X_q\)</span>.</p>
</section>
</section>
<section id="model-comparison" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="model-comparison"><span class="header-section-number">3.3.2</span> Model Comparison</h3>
<p>We have already compared models in Chapter 1 with the partial F-test. However, for that test to make sense, we require the models to be nested–i.e.&nbsp;the larger model must contain all of the parameters of the smaller model. But, given a model <span class="math display">\[
  y = \beta_0 + \beta_1x_1 +\ldots+ \beta_px_p + \varepsilon,
\]</span> we may want to compare two different subset models that are not nested. Hence, we have some different measures to consider.</p>
<p>Note that ideally, we would compare all possible subset models. However, given <span class="math inline">\(p\)</span> regressors, there are <span class="math inline">\(2^p\)</span> different models to consider, which will often be computationally infeasible. Hence, we will consider two approaches to model selection that avoid this combinatorial problem.</p>
<div id="rem-subsetIntercept" class="proof remark">
<p><span class="proof-title"><em>Remark 3.3</em>. </span>To avoid confusion and awkward notation, assume that all subset models will always contain the intercept term <span class="math inline">\(\beta_0\)</span></p>
</div>
<section id="residual-sum-of-squares" class="level4" data-number="3.3.2.1">
<h4 data-number="3.3.2.1" class="anchored" data-anchor-id="residual-sum-of-squares"><span class="header-section-number">3.3.2.1</span> Residual Sum of Squares</h4>
<p>For two subset models with <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> regressors, respectively, with <span class="math inline">\(p_1&lt;p\)</span> and <span class="math inline">\(p_2&lt;p\)</span>, we can compare the mean residual sum of squares for each <span class="math display">\[
  \frac{SS_\text{res}(p_1)}{n-p_1-1}
  ~~~\text{ vs }~~~
  \frac{SS_\text{res}(p_2)}{n-p_2-1}
\]</span> and choose the model with the smaller value.</p>
<p>We know from before that the mean of the residual sum of squares for the full model, <span class="math inline">\(SS_\text{res}/(n-p-1)\)</span>, is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>. Similar to the calculations in the previous section, we can show that <span class="math display">\[
  \mathrm{E}\left(\frac{SS_\text{res}(p_1)}{n-p_1-1}\right) \ge \sigma^2
  ~~~\text{ and }~~~
  \mathrm{E}\left(\frac{SS_\text{res}(p_2)}{n-p_2-1}\right) \ge \sigma^2,
\]</span> which is that these estimators for subset models are upwardly biased.</p>
</section>
<section id="mallows-c_p" class="level4" data-number="3.3.2.2">
<h4 data-number="3.3.2.2" class="anchored" data-anchor-id="mallows-c_p"><span class="header-section-number">3.3.2.2</span> Mallows’ <span class="math inline">\(C_p\)</span></h4>
<p>We can also compare different models by computing Mallows’ <span class="math inline">\(C_p\)</span>. The goal of this value is to choose the model the minimizes the mean squared prediction error, which is <span class="math display">\[
  MSPE = \sum_{i=1}^n \frac{\mathrm{E}\left(\tilde{y}_i-\mathrm{E}y_i\right)^2}{\sigma^2}
\]</span> where <span class="math inline">\(\tilde{y_i}\)</span> is the <span class="math inline">\(i\)</span>th fitted value of the submodel and <span class="math inline">\(\mathrm{E}y_i\)</span> is the <span class="math inline">\(i\)</span>th fitted value of the true model. Furthermore, let <span class="math inline">\(\hat{y}_i\)</span> be the <span class="math inline">\(i\)</span>th fitted value for the full model. This is the expected squared difference between what the submodel predicts and what the real value is. As usual with mean squared errors in statistics, we rewrite this in terms of the variance plus the squared bias, which is <span class="math display">\[\begin{align*}
  MSPE
  &amp;= \frac{1}{\sigma^2}\sum_{i=1}^n \left[
    \mathrm{E}\left(\tilde{y}_i-\mathrm{E}\tilde{y}_i+\mathrm{E}\tilde{y}_i-\mathrm{E}y_i\right)^2
  \right] \\
  &amp;= \frac{1}{\sigma^2}\sum_{i=1}^n \left[
    \mathrm{E}\left(\tilde{y}_i-\mathrm{E}\tilde{y}_i\right)^2+({\mathrm{E}\tilde{y}_i-\mathrm{E}y_i})^2
  \right] \\
  &amp;= \frac{1}{\sigma^2}\sum_{i=1}^n \left[
    \mathrm{Var}\left(\tilde{y}_i\right)+\text{bias}(\tilde{y}_i)^2
  \right]
\end{align*}\]</span></p>
<p>Recall that the variance of the fitted values for the full model is <span class="math inline">\(\mathrm{Var}\left(\hat{y}\right) = \sigma^2P_x\)</span> where <span class="math inline">\(P_x = X({X}^\mathrm{T}X)^{-1}{X}^\mathrm{T}\)</span>. For a submodel with <span class="math inline">\(p_1&lt;p\)</span> regressors and design matrix <span class="math inline">\(X_{p_1}\)</span>, we get the similar <span class="math inline">\(\mathrm{Var}\left(\tilde{y}\right) = \sigma^2X_{p_1}({X}^\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\mathrm{T}_{p_1}\)</span>. As <span class="math inline">\(X_{p_1}({X}^\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\mathrm{T}_{p_1}\)</span> is a rank <span class="math inline">\(p_1+1\)</span> projection matrix, we have that <span class="math display">\[
  \sum_{i=1}^n \mathrm{Var}\left(\tilde{y}_i\right) =
  \sigma^2\mathrm{tr}\left(X_{p_1}({X}^\mathrm{T}_{p_1}X_{p_1})^{-1}{X}^\mathrm{T}_{p_1}\right) =
  \sigma^2(p_1+1).
\]</span></p>
<p>For the bias term, consider the expected residual sum of squares for the submodel: <span class="math display">\[\begin{align*}
  \mathrm{E}\left(SS_\text{res}(p_1)\right)
  &amp;= \mathrm{E}\sum_{i=1}^n ( y_i - \tilde{y}_i )^2 \\
  &amp;= \mathrm{E}\sum_{i=1}^n (
     y_i -
     \mathrm{E}{\tilde{y}_i} + \mathrm{E}{\tilde{y}_i} -
     \mathrm{E}{{y}_i} + \mathrm{E}{{y}_i}
     - \tilde{y}_i )^2 \\
  &amp;= \sum_{i=1}^n\left[
    \mathrm{Var}\left(\tilde{r}_i\right) + (\mathrm{E}\tilde{y}_i-\mathrm{E}y_i)^2  
  \right]\\
  &amp;=  (n-p_1-1)\sigma^2 + \sum_{i=1}^n \text{bias}(\tilde{y}_i)^2.
\end{align*}\]</span> Hence, rearranging the terms above gives <span class="math display">\[
  \sum_{i=1}^n \text{bias}(\tilde{y}_i)^2
  = \mathrm{E}\left(SS_\text{res}(p_1)\right) - (n-p_1-1).
\]</span> Combining the bias and the variance terms derived above results in Mallows’ <span class="math inline">\(C_p\)</span> statistic for a submodel with <span class="math inline">\(p_1&lt;p\)</span> regressors: %\begin{multline<em>}</em> <span class="math display">\[
  C_{p_1} =
  \frac{\mathrm{E}\left(SS_\text{res}(p_1)\right)}{\sigma^2} - n+2p_1+2 \approx%\\\approx
  \frac{SS_\text{res}(p_1)}{SS_\text{res}/(n-p-1)} - n+2p_1+2.
\]</span> %\end{multline} Here, we estimate <span class="math inline">\(\mathrm{E}\left(SS_\text{res}(p_1)\right)\)</span> by <span class="math inline">\(SS_\text{res}(p_1)\)</span> and estimate <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(SS_\text{res}/(n-p-1)\)</span>.</p>
<div id="rem-MallowsCp" class="proof remark">
<p><span class="proof-title"><em>Remark 3.4</em>. </span>Note that if we compute Mallows’ <span class="math inline">\(C_p\)</span> for the full model, we get <span class="math display">\[
    C_p  
    = \frac{SS_\text{res}}{SS_\text{res}/(n-p-1)} - n+2p+2
    = p+1.
  \]</span> Hence, Mallows’ <span class="math inline">\(C_p\)</span> in this case is just the number of parameters in the model. In general, we want to find submodels with <span class="math inline">\(C_p\)</span> value smaller than <span class="math inline">\(p+1\)</span>.</p>
</div>
</section>
<section id="information-criteria" class="level4" data-number="3.3.2.3">
<h4 data-number="3.3.2.3" class="anchored" data-anchor-id="information-criteria"><span class="header-section-number">3.3.2.3</span> Information Criteria</h4>
<p>Information criteria are concerned with quantifying the amount of information in a model. With such a measure, we can choose a model that optimizes this measurement. A main requirement for these methods is that the response <span class="math inline">\(y\)</span> is the same. Hence, we should not use the measures below when comparing transformed models–e.g.&nbsp;different linearized models–without the necessary modifications.</p>
<p>The first such measure is the Akaike Information Criterion or AIC, which is a measure of the entropy of a model. Its general definition is <span class="math display">\[
  \text{AIC} = -2\log(\text{Likelihood}) +
  2(\text{\# parameters})
\]</span> where <span class="math inline">\(p\)</span> is the number of parameters in the model. This can be thought of a measurement of how much information is lost when modelling complex data with a <span class="math inline">\(p\)</span> parameter model. Hence, the model with the minimal AIC will be optimal in some sense.</p>
<p>In our least squares regression case with normally distributed errors, <span class="math display">\[
  \text{AIC} = n\log(SS_\text{res}/n) + 2(p+1)
\]</span> where <span class="math inline">\(p+1\)</span> is for the <span class="math inline">\(p\)</span> regressors and 1 intercept term. Thus, adding more regressors will decrease <span class="math inline">\(SS_\text{res}\)</span> but will increase <span class="math inline">\(p\)</span>. The goal is to find a model with the minimal AIC. This can be shown to give the same ordering as Mallows’ <span class="math inline">\(C_p\)</span> when the errors are normally distributed.</p>
<p>The second such measure is the closely related Bayesian Information Criterion or BIC, which, in general, is <span class="math display">\[
  \text{BIC} = -2\log(\text{Likelihood}) + (\text{\# parameters})\log n.
\]</span> In the linear regression setting with normally distributed errors, <span class="math display">\[
  \text{BIC} = n\log(SS_\text{res}/n) + (p+1)\log n.
\]</span></p>
<div id="rem-AICvBIC" class="proof remark">
<p><span class="proof-title"><em>Remark 3.5</em>. </span>Using AIC versus using BIC for model selection can sometimes result in different final choices. In some cases, one may be preferred, but often both can be tried and discrepancies, if they exist, can be reported.</p>
<p>There are also other information criterion that are not as common in practise such as the Deviation Information Criterion (DIC) and the Focused Information Criterion (FIC).</p>
</div>
</section>
</section>
<section id="forward-and-backward-selection" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="forward-and-backward-selection"><span class="header-section-number">3.3.3</span> Forward and Backward Selection</h3>
<p>Ideally, we choose a measure for model selection from the previous section and then compare all possible models. However, for <span class="math inline">\(p\)</span> possible regressors, this will result in <span class="math inline">\(2^p\)</span> models to check, which may be computationally infeasible. Hence, there are iterative approaches that can be effective.</p>
<p><em>Forward selection</em> is the process of starting with the constant model <span class="math display">\[
  y = \beta_0 + \varepsilon
\]</span> and choosing the best of the <span class="math inline">\(p\)</span> regressors with respect to the model selection criterion. This gives <span class="math display">\[
  y = \beta_0 + \beta_1x_1 + \varepsilon.
\]</span> This process will continue to add terms to the model as long as it results in an improvement in the criterion. For example, computing the AIC at each step.</p>
<p><em>Backwards selection</em> is the reverse of forward selection. In this case, the algorithm begins with the full model, <span class="math display">\[
  y = \beta_0 + \beta_1x_1 + \ldots + \beta_p x_p +\varepsilon,
\]</span> and iteratively removes the regressor that gives the biggest improvement in the model selection criterion. If the best choice is to remove no regressors, then the process terminates.</p>
<p>A third option is <em>stepwise selection</em>, which incorporates both forward and backward steps. In this case, we begin with the constant model as in forward selection. However, at every step, we choose either to add a new regressor to our model or remove one that is already in the model depending on which choice improves the criterion the most.</p>
<section id="variable-selection-example" class="level4" data-number="3.3.3.1">
<h4 data-number="3.3.3.1" class="anchored" data-anchor-id="variable-selection-example"><span class="header-section-number">3.3.3.1</span> Variable Selection Example</h4>
<p>Consider the same example as in the spline section where <span class="math inline">\(x\in[0,2]\)</span> and <span class="math display">\[
  y = 2 + 3x - 4x^5 + x^7 + \varepsilon
\]</span> with a sample of <span class="math inline">\(n=41\)</span> observations. We can fit two regression models, an empty and a saturated model, respectively, <span class="math display">\[
  y = \beta_0 +\varepsilon~\text{ and }~
  y = \beta_0 + \sum_{i=1}^7 \beta_i x^i + \varepsilon.
\]</span> and use the <code>R</code> function <code>step( )</code> to choose a best model with respect to AIC.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">256</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Data from a degree-7 polynomial</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>xx  <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span>,<span class="fl">0.05</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>len <span class="ot">=</span> <span class="fu">length</span>(xx)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>yy  <span class="ot">=</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>xx <span class="sc">-</span> <span class="dv">4</span><span class="sc">*</span>xx<span class="sc">^</span><span class="dv">5</span> <span class="sc">+</span> xx<span class="sc">^</span><span class="dv">7</span> <span class="sc">+</span> <span class="fu">rnorm</span>(len,<span class="dv">0</span>,<span class="dv">4</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the null and saturated models</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: never ever fit a polynomial model</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#       like this.  We are trying to create</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#       a model with high multicollinearity</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#       for educational purposes. </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>md0 <span class="ot">=</span> <span class="fu">lm</span>(yy<span class="sc">~</span><span class="dv">1</span>);</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>md7 <span class="ot">=</span> <span class="fu">lm</span>(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  yy<span class="sc">~</span>xx<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">5</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">6</span>)<span class="sc">+</span><span class="fu">I</span>(xx<span class="sc">^</span><span class="dv">7</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ 1)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.4234  -3.9053   0.8976   4.0282   9.4048 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   0.6483     0.7630    0.85    0.401

Residual standard error: 4.886 on 40 degrees of freedom</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md7)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ xx + I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + 
    I(xx^6) + I(xx^7))

Residuals:
    Min      1Q  Median      3Q     Max 
-7.7630 -2.3611 -0.7164  2.2562  7.3220 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    0.529      3.554   0.149    0.883
xx           -26.747     70.159  -0.381    0.705
I(xx^2)      271.061    434.342   0.624    0.537
I(xx^3)     -799.042   1158.236  -0.690    0.495
I(xx^4)     1124.811   1557.892   0.722    0.475
I(xx^5)     -824.786   1108.239  -0.744    0.462
I(xx^6)      300.118    397.719   0.755    0.456
I(xx^7)      -42.561     56.669  -0.751    0.458

Residual standard error: 3.982 on 33 degrees of freedom
Multiple R-squared:  0.4518,    Adjusted R-squared:  0.3356 
F-statistic: 3.886 on 7 and 33 DF,  p-value: 0.003437</code></pre>
</div>
</div>
<p>First, we note that this (non-orthogonal) polynomial model is very poorly specified. That is, the estimated coefficients vary wildly and are effectively trying to counter balance each other. None of our t-statistics are significant meaning that we can remove any of these terms individually without harming the fit of the model. The F-statistic is significant indicating that, globally, this model is significantly reducing the residual sum of squares.</p>
<p>First, we apply backwards variable selection. The result is an AIC that drops from 120.42 to 113.29 and the following fitted model: <span class="math display">\[
  y = 0.56 + 20.47x^2 - 18.59 x^3 + 1.09 x^6.
\]</span> In this case, we did not recover the model that was used to generate the data. However, this one still fits the noisy data well.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a backward variable selection</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>md.bck <span class="ot">=</span> <span class="fu">step</span>(md7)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=120.42
yy ~ xx + I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6) + I(xx^7)

          Df Sum of Sq    RSS    AIC
- xx       1    2.3052 525.69 118.60
- I(xx^2)  1    6.1770 529.56 118.90
- I(xx^3)  1    7.5484 530.93 119.00
- I(xx^4)  1    8.2678 531.65 119.06
- I(xx^5)  1    8.7846 532.17 119.10
- I(xx^7)  1    8.9462 532.33 119.11
- I(xx^6)  1    9.0311 532.42 119.12
&lt;none&gt;                 523.39 120.42

Step:  AIC=118.6
yy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6) + I(xx^7)

          Df Sum of Sq    RSS    AIC
- I(xx^7)  1    7.4088 533.10 117.17
- I(xx^6)  1    7.9536 533.64 117.21
- I(xx^5)  1    8.3113 534.00 117.24
- I(xx^4)  1    8.7075 534.40 117.27
- I(xx^3)  1    9.8197 535.51 117.36
- I(xx^2)  1   13.0549 538.75 117.60
&lt;none&gt;                 525.69 118.60

Step:  AIC=117.17
yy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^5) + I(xx^6)

          Df Sum of Sq    RSS    AIC
- I(xx^5)  1    1.5784 534.68 115.29
- I(xx^4)  1    1.5872 534.69 115.29
- I(xx^6)  1    2.1726 535.27 115.34
- I(xx^3)  1    2.6951 535.79 115.38
- I(xx^2)  1    6.2706 539.37 115.65
&lt;none&gt;                 533.10 117.17

Step:  AIC=115.29
yy ~ I(xx^2) + I(xx^3) + I(xx^4) + I(xx^6)

          Df Sum of Sq    RSS    AIC
- I(xx^4)  1    0.0096 534.69 113.29
- I(xx^3)  1    3.4790 538.16 113.56
- I(xx^6)  1    7.0789 541.76 113.83
- I(xx^2)  1   12.5541 547.23 114.24
&lt;none&gt;                 534.68 115.29

Step:  AIC=113.29
yy ~ I(xx^2) + I(xx^3) + I(xx^6)

          Df Sum of Sq    RSS    AIC
&lt;none&gt;                 534.69 113.29
- I(xx^2)  1    143.39 678.08 121.03
- I(xx^3)  1    197.82 732.50 124.20
- I(xx^6)  1    229.28 763.96 125.92</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.bck)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ I(xx^2) + I(xx^3) + I(xx^6))

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3054 -2.4655  0.0047  2.3310  6.7750 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   0.5640     1.3500   0.418 0.678528    
I(xx^2)      20.4701     6.4984   3.150 0.003226 ** 
I(xx^3)     -18.5896     5.0245  -3.700 0.000698 ***
I(xx^6)       1.0863     0.2727   3.983 0.000306 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.801 on 37 degrees of freedom
Multiple R-squared:   0.44, Adjusted R-squared:  0.3946 
F-statistic: 9.691 on 3 and 37 DF,  p-value: 7.438e-05</code></pre>
</div>
</div>
<p>Next, doing forward selection, we drop the AIC from 131.07 to 113.36, which is almost the same ending AIC as in the backwards selection performed above. In this case, the fitted model is <span class="math display">\[
  y = 0.78 + 17.19x^2 - 14.86 x^3 + 0.41 x^7.
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward variable selection</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>md.fwd <span class="ot">=</span> <span class="fu">step</span>(md0,<span class="at">direction =</span> <span class="st">"forward"</span>,<span class="at">scope =</span> <span class="fu">list</span>(<span class="at">upper=</span>md7))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=131.07
yy ~ 1

          Df Sum of Sq    RSS    AIC
+ I(xx^2)  1   189.254 765.55 124.01
+ I(xx^3)  1   178.275 776.53 124.59
+ xx       1   151.475 803.33 125.98
+ I(xx^4)  1   150.951 803.85 126.01
+ I(xx^5)  1   121.568 833.23 127.48
+ I(xx^6)  1    95.317 859.48 128.75
+ I(xx^7)  1    73.561 881.24 129.78
&lt;none&gt;                 954.80 131.06

Step:  AIC=124.01
yy ~ I(xx^2)

          Df Sum of Sq    RSS    AIC
+ I(xx^7)  1    44.476 721.07 123.55
&lt;none&gt;                 765.55 124.01
+ I(xx^6)  1    33.044 732.50 124.20
+ I(xx^5)  1    21.043 744.50 124.86
+ xx       1    15.086 750.46 125.19
+ I(xx^4)  1     9.728 755.82 125.48
+ I(xx^3)  1     1.583 763.96 125.92

Step:  AIC=123.55
yy ~ I(xx^2) + I(xx^7)

          Df Sum of Sq    RSS    AIC
+ I(xx^3)  1    185.46 535.61 113.36
+ I(xx^4)  1    178.81 542.26 113.87
+ xx       1    170.88 550.19 114.46
+ I(xx^5)  1    170.31 550.76 114.51
+ I(xx^6)  1    161.22 559.85 115.18
&lt;none&gt;                 721.07 123.55

Step:  AIC=113.36
yy ~ I(xx^2) + I(xx^7) + I(xx^3)

          Df Sum of Sq    RSS    AIC
&lt;none&gt;                 535.61 113.36
+ xx       1   2.72717 532.89 115.15
+ I(xx^4)  1   1.21296 534.40 115.27
+ I(xx^5)  1   1.01714 534.60 115.28
+ I(xx^6)  1   0.93707 534.68 115.29</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.fwd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ I(xx^2) + I(xx^7) + I(xx^3))

Residuals:
    Min      1Q  Median      3Q     Max 
-8.0984 -2.3891  0.0842  2.4374  6.7973 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   0.7795     1.3253   0.588 0.560027    
I(xx^2)      17.1905     5.7868   2.971 0.005195 ** 
I(xx^7)       0.4142     0.1043   3.972 0.000317 ***
I(xx^3)     -14.8630     4.1525  -3.579 0.000984 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.805 on 37 degrees of freedom
Multiple R-squared:  0.439, Adjusted R-squared:  0.3935 
F-statistic: 9.652 on 3 and 37 DF,  p-value: 7.672e-05</code></pre>
</div>
</div>
</section>
</section>
<section id="hypothermic-half-marathon-data-revisited" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="hypothermic-half-marathon-data-revisited"><span class="header-section-number">3.3.4</span> Hypothermic Half Marathon Data, Revisited</h3>
<p>In this section, we will revisit the hypothermic half marathon data from the previous chapter. This time, we will consider the predictors age, sex, and date as three categorical variables. There are 6 levels for age, 2 for sex, and 2 for race date.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>hypoDat <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"data/hypoHalf.csv"</span>,)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>hypoDat<span class="sc">$</span>DIV  <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hypoDat<span class="sc">$</span>DIV)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>hypoDat<span class="sc">$</span>date <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hypoDat<span class="sc">$</span>date)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>hypoDat<span class="sc">$</span>sex  <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hypoDat<span class="sc">$</span>sex)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>hypoDat<span class="sc">$</span>ageGroup <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(hypoDat<span class="sc">$</span>ageGroup)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>( hypoDat<span class="sc">$</span>ageGroup )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "0119" "2029" "3039" "4049" "5059" "60+" </code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>( hypoDat<span class="sc">$</span>sex )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "female" "male"  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>( hypoDat<span class="sc">$</span>date )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "1" "2"</code></pre>
</div>
</div>
<p>When we fit a model taking into account age, sex, and date, we can also consider interaction terms like age<span class="math inline">\(\times\)</span>sex, which could be significant, for example, if age affects male and female runners differently with respect to their finishing time. The model that contains all pairwise interactions is fit below using the notation <code>(ageGroup + sex + date)^2</code>, which does <em>not</em> fit a quadratic polynomial model, but instead fits a model with the three main inputs and the <span class="math inline">\({3\choose2} = 3\)</span> pairwise interactions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>( time<span class="sc">~</span>(ageGroup <span class="sc">+</span> sex <span class="sc">+</span> date)<span class="sc">^</span><span class="dv">2</span>, <span class="at">data=</span>hypoDat )</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = time ~ (ageGroup + sex + date)^2, data = hypoDat)

Residuals:
    Min      1Q  Median      3Q     Max 
-51.172 -13.483  -0.944  11.212  76.509 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          137.0333    21.3140   6.429  5.7e-10 ***
ageGroup2029          -0.3606    21.8178  -0.017    0.987    
ageGroup3039           5.1578    21.6055   0.239    0.811    
ageGroup4049           7.9163    21.9381   0.361    0.718    
ageGroup5059           4.9225    23.6578   0.208    0.835    
ageGroup60+           29.6652    23.9378   1.239    0.216    
sexmale                2.4778    24.6113   0.101    0.920    
date2                -12.7856    25.2010  -0.507    0.612    
ageGroup2029:sexmale -11.1589    25.2088  -0.443    0.658    
ageGroup3039:sexmale -20.7837    25.0266  -0.830    0.407    
ageGroup4049:sexmale -14.5883    25.3120  -0.576    0.565    
ageGroup5059:sexmale   0.2885    26.9407   0.011    0.991    
ageGroup60+:sexmale  -18.2491    27.9274  -0.653    0.514    
ageGroup2029:date2     4.2395    25.4660   0.166    0.868    
ageGroup3039:date2     5.4182    25.0613   0.216    0.829    
ageGroup4049:date2     2.2504    25.2425   0.089    0.929    
ageGroup5059:date2    28.8170    26.9407   1.070    0.286    
ageGroup60+:date2     -7.9524    27.8594  -0.285    0.776    
sexmale:date2          3.7244     5.4197   0.687    0.493    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 21.31 on 273 degrees of freedom
Multiple R-squared:  0.1759,    Adjusted R-squared:  0.1216 
F-statistic: 3.238 on 18 and 273 DF,  p-value: 1.627e-05</code></pre>
</div>
</div>
<p>What we see in the summary of this regression model is that none of the t-tests for individual inputs is significant in this model except for the intercept term.<br>
However, the F-statistic returns a very significant p-value. This indicates that we likely have too many inputs in our regression model. Performing backwards variable selection with respect to AIC results in the removal of all of the pairwise interaction terms leaving only the three main inputs as predictors for this model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>md.step <span class="ot">=</span> <span class="fu">step</span>(</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>( time<span class="sc">~</span>(ageGroup <span class="sc">+</span> sex <span class="sc">+</span> date)<span class="sc">^</span><span class="dv">2</span>, <span class="at">data=</span>hypoDat )</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=1805.02
time ~ (ageGroup + sex + date)^2

                Df Sum of Sq    RSS    AIC
- ageGroup:sex   5   2287.93 126308 1800.4
- ageGroup:date  5   2866.26 126886 1801.7
- sex:date       1    214.54 124235 1803.5
&lt;none&gt;                       124020 1805.0

Step:  AIC=1800.36
time ~ ageGroup + sex + date + ageGroup:date + sex:date

                Df Sum of Sq    RSS    AIC
- ageGroup:date  5    2315.9 128624 1795.7
- sex:date       1     146.6 126455 1798.7
&lt;none&gt;                       126308 1800.4

Step:  AIC=1795.67
time ~ ageGroup + sex + date + sex:date

           Df Sum of Sq    RSS    AIC
- sex:date  1       116 128740 1793.9
&lt;none&gt;                  128624 1795.7
- ageGroup  5     11262 139886 1810.2

Step:  AIC=1793.93
time ~ ageGroup + sex + date

           Df Sum of Sq    RSS    AIC
&lt;none&gt;                  128740 1793.9
- date      1    2398.4 131138 1797.3
- ageGroup  5   11266.3 140006 1808.4
- sex       1   10116.2 138856 1814.0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = time ~ ageGroup + sex + date, data = hypoDat)

Residuals:
    Min      1Q  Median      3Q     Max 
-69.267 -14.331  -1.126  12.157  79.624 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   148.070      9.757  15.176  &lt; 2e-16 ***
ageGroup2029   -9.454      9.857  -0.959   0.3383    
ageGroup3039   -8.994      9.761  -0.921   0.3576    
ageGroup4049   -4.202      9.898  -0.425   0.6715    
ageGroup5059   11.981     10.719   1.118   0.2646    
ageGroup60+     8.672     11.159   0.777   0.4377    
sexmale       -12.069      2.555  -4.724 3.64e-06 ***
date2          -6.055      2.632  -2.300   0.0222 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 21.29 on 284 degrees of freedom
Multiple R-squared:  0.1446,    Adjusted R-squared:  0.1235 
F-statistic: 6.856 on 7 and 284 DF,  p-value: 1.545e-07</code></pre>
</div>
</div>
<p>In the final model, none of the t-tests for <code>ageGroup</code> levels are significant. These t-tests are testing the hypotheses, <em>is the average finishing time of this age group different from the 0119 category</em> after already taking the variables sex and date into account. Note that while none of the t-tests are significant, the <code>ageGroup</code> variable is still included in the regression model even after variable selection.</p>
<p>Since <code>ageGroup</code> is an ordered categorical variable, we can also re-encode it as a polynomial to identify that there is a significant linear increase in the finishing times across the age groups as well as some significance in the quadratic term. This coincides with what was seen in the previous chapter.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>hypoDat<span class="sc">$</span>ageGroup <span class="ot">&lt;-</span> <span class="fu">as.ordered</span>(hypoDat<span class="sc">$</span>ageGroup)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>( hypoDat<span class="sc">$</span>ageGroup, <span class="at">how.many=</span><span class="dv">2</span>  ) <span class="ot">&lt;-</span> contr.poly</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(  </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>( time<span class="sc">~</span>ageGroup <span class="sc">+</span> sex <span class="sc">+</span> date, <span class="at">data=</span>hypoDat )</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = time ~ ageGroup + sex + date, data = hypoDat)

Residuals:
    Min      1Q  Median      3Q     Max 
-61.057 -14.637  -1.708  11.890  78.831 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  146.443      2.645  55.360  &lt; 2e-16 ***
ageGroup.L    18.125      4.910   3.692 0.000267 ***
ageGroup.Q    10.101      4.813   2.099 0.036727 *  
sexmale      -11.921      2.557  -4.662 4.81e-06 ***
date2         -6.502      2.627  -2.475 0.013889 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 21.35 on 287 degrees of freedom
Multiple R-squared:  0.1306,    Adjusted R-squared:  0.1185 
F-statistic: 10.78 on 4 and 287 DF,  p-value: 3.739e-08</code></pre>
</div>
</div>
</section>
</section>
<section id="penalized-regressions" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="penalized-regressions"><span class="header-section-number">3.4</span> Penalized Regressions</h2>
<p>No matter how we design our model, thus far we have always computed the least squares estimator, <span class="math inline">\(\hat{\beta}\)</span>, by minimizing the sum of squared errors <span class="math display">\[
  \hat{\beta} = \underset{\beta\in\mathbb{R}^{p+1}}{\arg\min}
  \left\{
  \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2
  \right\}.
\]</span> This is an unbiased estimator for <span class="math inline">\(\beta\)</span>. However, as we have seen previously, the variance of this estimator can be quite large. Hence, we <em>shrink</em> the estimator towards zero adding bias but decreasing the variance. General idea of shrinkage is attributed to Stein (1956) and the so-called <a href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator">Stein Estimator</a>. In the context of regression, we add a penalty term to the above minimization to get a new estimator <span class="math display">\[
  \hat{\beta}^\text{pen} = \underset{\beta\in\mathbb{R}^{p+1}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2
    + \text{penalty}(\beta)
  \right\},
\]</span> which increases as <span class="math inline">\(\beta\)</span> increases thus attempting to enforce smaller choices for the estimated parameters. We will consider some different types of penalized regression. In <code>R</code>, the <a href="http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html">glmnet</a> package has a lot of functionality to fit different types of penalized general linear models ::: {#rem-penalIntercept} We generally do not want to penalize the intercept term <span class="math inline">\(\beta_0\)</span>. Often to account for this, the regressors and response are centred–i.e.&nbsp;<span class="math inline">\(Y\)</span> is replaced with <span class="math inline">\(Y - \bar{Y}\)</span> and each <span class="math inline">\(X_j\)</span> is replaced with <span class="math inline">\(X_j-\bar{X_j}\)</span> for <span class="math inline">\(j=1,\ldots,p\)</span>–in order to set the intercept term to zero. :::</p>
<section id="ridge-regression" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="ridge-regression"><span class="header-section-number">3.4.1</span> Ridge Regression</h3>
<p>The first method we consider is ridge regression, which arose in statistics in the 1970’s—see Hoerl, A.E.; R.W. Kennard (1970)—but similar techniques arise in other areas of computational mathematics. In short, a quadratic penalty is applied to the least squares estimator resulting in <span class="math display">\[
  \hat{\beta}^\text{R}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p \beta_j^2
  \right\}
\]</span> for any <span class="math inline">\(\lambda\ge0\)</span>. When <span class="math inline">\(\lambda=0\)</span>, we have the usual least squares estimator. As <span class="math inline">\(\lambda\)</span> grows, the <span class="math inline">\(\beta\)</span>’s are more strongly penalized.</p>
<p>To solve for <span class="math inline">\(\hat{\beta}^\text{R}_\lambda\)</span>, we proceed as before with the least squares estimator <span class="math inline">\(\hat{\beta}\)</span> by setting the partial derivatives equal to zero <span class="math display">\[\begin{align*}
  0 &amp;= \frac{\partial}{\partial\beta_k}
  \left\{
    \sum_{i=1}^n( y_i - \beta_1x_{i,1} -\ldots-\beta_px_{i,p} )^2 +
    \lambda\sum_{j=1}^p \beta_j^2
  \right\}.
\end{align*}\]</span> This results in the system of equations <span class="math display">\[
  {X}^\mathrm{T}Y - ({X}^\mathrm{T}X)\hat{\beta}^\text{R}_\lambda-
  \lambda\hat{\beta}^\text{R}_\lambda= 0
\]</span> with the ridge estimator being $ ^_= (X + I_n)^{-1}Y. $</p>
<p>The matrix <span class="math inline">\({X}^\mathrm{T}X\)</span> is positive semi-definite even when <span class="math inline">\(p&gt;n\)</span>–i.e.&nbsp;the number of parameters exceeds the sample size. Hence, any positive value <span class="math inline">\(\lambda\)</span> will make <span class="math inline">\({X}^\mathrm{T}X + \lambda I_n\)</span> invertible as it adds the positive constant <span class="math inline">\(\lambda\)</span> to all of the eigenvalues. Increasing the value of <span class="math inline">\(\lambda\)</span> will increase the numerical stability of the estimator–i.e.&nbsp;decrease the condition number of the matrix. Furthermore, it will decrease the variance of the estimator while increasing the bias. It can also be shown that the bias of <span class="math inline">\(\hat{\beta}^\text{R}_\lambda\)</span> is <span class="math display">\[
  \mathrm{E}{\hat{\beta}^\text{R}_\lambda} - \beta =
  %(\TT{X}X + \lmb I_n)^{-1}\TT{X}X\beta - \beta =
  %(\TT{X}X + \lmb I_n)^{-1}( \TT{X}X - \TT{X}X - \lmb I_n )\beta
  -\lambda({X}^\mathrm{T}X + \lambda I_n)^{-1}\beta,
\]</span> which implies that the estimator does, in fact, shrink towards zero as <span class="math inline">\(\lambda\)</span> increases.</p>
</section>
<section id="best-subset-regression" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="best-subset-regression"><span class="header-section-number">3.4.2</span> Best Subset Regression</h3>
<p>Another type of penalty related to the variable selection techniques from the previous section is the Best Subset Regression approach, which counts the number of non-zero <span class="math inline">\(\beta\)</span>’s and adds a larger penalty as more terms are included in the model. The optimization looks like <span class="math display">\[
  \hat{\beta}^\text{B}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p \boldsymbol{1}\!\left[\beta_j\ne 0\right]
  \right\}.
\]</span> The main problem with this method is that the optimization is non-convex and becomes severely difficult to compute in practice. This is why the forwards and backwards selection methods are used for variable selection.</p>
</section>
<section id="lasso" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="lasso"><span class="header-section-number">3.4.3</span> LASSO</h3>
<p>The last method we consider is the Least Absolute Shrinkage and Selection Operator, which is commonly referred to as just LASSO. This was introduced by Tibshirani (1996) and has since been applied to countless areas of statistics. The form is quite similar to ridge regression with one small but profound modification, <span class="math display">\[
  \hat{\beta}^\text{L}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p \lvert\beta_j\rvert
  \right\},
\]</span> which is that the penalty term is now the sum of the absolute values instead of a sum of squares.</p>
<p>The main reason for why this technique is popular is that it combines shrinkage methods like ridge regression with variable selection and still results in a convex optimization problem. Delving into the properties of this estimator requires convex analysis and will be left for future investigations.</p>
</section>
<section id="elastic-net" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="elastic-net"><span class="header-section-number">3.4.4</span> Elastic Net</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Elastic_net_regularizatio">elastic net regularization</a> method combines both ridge and lasso regression into one methodology. Here, we include a penalty term for each of the two methods: <span class="math display">\[
  \hat{\beta}^\text{EN}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda_1 \sum_{j=1}^p \lvert\beta_j\rvert +
    \lambda_2 \sum_{j=1}^p \beta_j^2
  \right\}.
\]</span> This method has two tuning parameters <span class="math inline">\(\lambda_1\ge0\)</span> and <span class="math inline">\(\lambda_2\ge0\)</span>. In the <code>R</code> library <code>glmnet</code>, a <em>mixing</em> parameter <span class="math inline">\(\alpha\)</span> and a <em>scale</em> parameter <span class="math inline">\(\lambda\)</span> is specified to get <span class="math display">\[
  \hat{\beta}^\text{EN}_\lambda= \underset{\beta\in\mathbb{R}^{p}}{\arg\min}
  \left\{
    \sum_{i=1}^n( y_i - {X}^\mathrm{T}_i\beta )^2 +
    \lambda\sum_{j=1}^p\left[
      \alpha \lvert\beta_j\rvert +
      \frac{1-\alpha}{2} \beta_j^2
    \right]
  \right\}.
\]</span> The intuition behind this approach is to combine the strengths of both ridge and lasso regression. Namely, ridge regression shrinks the coefficients towards zero reducing the variance while lasso selects a subset of the parameters to remain in the model.</p>
</section>
<section id="penalized-regression-an-example" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="penalized-regression-an-example"><span class="header-section-number">3.4.5</span> Penalized Regression: An Example</h3>
<p>Consider a sample of size <span class="math inline">\(n=100\)</span> generated by the model <span class="math display">\[
  y = \beta_0 + \beta_1x_1 +\ldots+\beta_{50}x_{50} + \varepsilon
\]</span> where <span class="math inline">\(\beta_{27}=2\)</span>, <span class="math inline">\(\beta_{34}=-2\)</span>, all other <span class="math inline">\(\beta_i=0\)</span>, and <span class="math inline">\(\varepsilon\sim\mathcal{N}\left(0,16\right)\)</span>. Even though only two of the regressors have any effect on the response <span class="math inline">\(y\)</span>, feeding all 50 regressors into <code>R</code>’s <code>lm()</code> function can result in many false positives as in the code below. In this example, we have three terms in the model that have weakly small p-values around 6% - 7%, two false positive p-values significant at the 5% level, and the two true positive results—entries 27 and 34—which both have very significant p-values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">256</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate some data</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">=</span> <span class="fu">matrix</span>( </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rnorm</span>(<span class="dv">100</span><span class="sc">*</span><span class="dv">50</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dv">100</span>, <span class="dv">50</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>yy <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>xx[,<span class="dv">27</span>] <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>xx[,<span class="dv">34</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">4</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>dat.sim <span class="ot">=</span> <span class="fu">data.frame</span>(  yy,xx )</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a least squares model with all 50 inputs</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>md.lm <span class="ot">=</span> <span class="fu">lm</span>( yy <span class="sc">~</span> ., <span class="at">data=</span>dat.sim )</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ ., data = dat.sim)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.4593 -1.7764  0.0529  1.3162  8.6453 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.114956   0.604874  -0.190  0.85006    
X1          -0.186314   0.501304  -0.372  0.71175    
X2           0.293722   0.698205   0.421  0.67583    
X3           0.846036   0.635611   1.331  0.18933    
X4          -0.617202   0.719074  -0.858  0.39489    
X5           0.947231   0.573410   1.652  0.10494    
X6           1.226520   0.530814   2.311  0.02510 *  
X7          -0.316981   0.585094  -0.542  0.59044    
X8           0.133680   0.563881   0.237  0.81359    
X9           0.583568   0.515985   1.131  0.26357    
X10          0.130990   0.472349   0.277  0.78270    
X11         -0.017902   0.469984  -0.038  0.96977    
X12         -1.269013   0.668925  -1.897  0.06372 .  
X13          0.427024   0.564001   0.757  0.45260    
X14          0.295055   0.679860   0.434  0.66620    
X15         -0.008477   0.565341  -0.015  0.98810    
X16          0.684455   0.507938   1.348  0.18401    
X17         -0.852380   0.655739  -1.300  0.19973    
X18         -1.482044   0.788183  -1.880  0.06601 .  
X19         -0.582075   0.567622  -1.025  0.31018    
X20          1.184024   0.720145   1.644  0.10655    
X21         -0.825218   0.549479  -1.502  0.13956    
X22          0.368528   0.600818   0.613  0.54246    
X23         -0.846930   0.623998  -1.357  0.18092    
X24         -0.066278   0.526772  -0.126  0.90039    
X25          0.982897   0.644828   1.524  0.13387    
X26          0.444571   0.513192   0.866  0.39056    
X27          1.733198   0.552519   3.137  0.00289 ** 
X28         -0.263400   0.510541  -0.516  0.60823    
X29          0.287366   0.623317   0.461  0.64682    
X30         -0.460872   0.505887  -0.911  0.36675    
X31         -0.057786   0.493507  -0.117  0.90727    
X32          0.522936   0.503179   1.039  0.30378    
X33          0.338087   0.581553   0.581  0.56367    
X34         -3.384722   0.537832  -6.293 8.25e-08 ***
X35          0.635258   0.530874   1.197  0.23721    
X36          0.942947   0.500341   1.885  0.06542 .  
X37          0.385363   0.565600   0.681  0.49887    
X38          0.517330   0.628746   0.823  0.41461    
X39         -0.305261   0.535436  -0.570  0.57120    
X40         -0.539736   0.532697  -1.013  0.31594    
X41         -1.357445   0.608486  -2.231  0.03030 *  
X42         -0.102478   0.657813  -0.156  0.87684    
X43          0.573256   0.681116   0.842  0.40408    
X44         -0.291538   0.591831  -0.493  0.62449    
X45          0.512879   0.571004   0.898  0.37347    
X46          0.235569   0.561770   0.419  0.67681    
X47          0.475641   0.565423   0.841  0.40432    
X48          0.308767   0.596236   0.518  0.60689    
X49          0.196569   0.535140   0.367  0.71496    
X50         -0.878138   0.578599  -1.518  0.13552    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4 on 49 degrees of freedom
Multiple R-squared:  0.7283,    Adjusted R-squared:  0.4511 
F-statistic: 2.627 on 50 and 49 DF,  p-value: 0.0004624</code></pre>
</div>
</div>
<p>We could attempt one of the stepwise variable selection procedures from the previous section. Running backwards and forwards selection results in the many terms being retained in the model, which furthermore are deemed to be statistically significant from the t-test.</p>
<p>In particular, backwards selection results in 18 of 50 terms kept in the model with 10 significant at the 5% level. Meanwhile, forward selection results in 17 of 50 terms kept in the model with 4 significant at the 5% level and another 7 terms just above the classic 5% threshold.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>md0 <span class="ot">=</span> <span class="fu">lm</span>(yy<span class="sc">~</span><span class="dv">1</span>,<span class="at">data=</span>dat.sim)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>md.bck <span class="ot">=</span> <span class="fu">step</span>( </span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  md.lm, <span class="at">direction =</span> <span class="st">"backward"</span>, <span class="at">trace=</span><span class="dv">0</span> </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.bck)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ X5 + X6 + X9 + X12 + X14 + X17 + X20 + X21 + 
    X23 + X25 + X27 + X34 + X35 + X36 + X38 + X41 + X45 + X50, 
    data = dat.sim)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.1215 -1.9121 -0.0406  1.7660 10.7895 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.02405    0.40411   0.060   0.9527    
X5           0.55510    0.36882   1.505   0.1362    
X6           0.83115    0.38887   2.137   0.0356 *  
X9           0.54329    0.35032   1.551   0.1248    
X12         -0.93968    0.45563  -2.062   0.0424 *  
X14          0.86042    0.42725   2.014   0.0473 *  
X17         -0.84374    0.39494  -2.136   0.0357 *  
X20          0.67458    0.44559   1.514   0.1339    
X21         -0.99140    0.41366  -2.397   0.0188 *  
X23         -0.81608    0.36562  -2.232   0.0284 *  
X25          0.64772    0.44846   1.444   0.1525    
X27          1.85091    0.37052   4.996 3.32e-06 ***
X34         -3.16199    0.38734  -8.163 3.58e-12 ***
X35          0.50745    0.38561   1.316   0.1919    
X36          0.91381    0.34915   2.617   0.0106 *  
X38          0.82018    0.42639   1.924   0.0579 .  
X41         -0.99945    0.41261  -2.422   0.0177 *  
X45          0.66715    0.38839   1.718   0.0897 .  
X50         -0.52955    0.39024  -1.357   0.1786    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.49 on 81 degrees of freedom
Multiple R-squared:  0.658, Adjusted R-squared:  0.582 
F-statistic: 8.659 on 18 and 81 DF,  p-value: 2.152e-12</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>md.fwd <span class="ot">=</span> <span class="fu">step</span>( </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  md0, <span class="at">direction =</span> <span class="st">"forward"</span>, <span class="at">trace=</span><span class="dv">0</span>, <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">upper=</span>md.lm) </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(md.fwd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = yy ~ X34 + X27 + X29 + X50 + X23 + X41 + X25 + X36 + 
    X45 + X21 + X5 + X17 + X12 + X6 + X48 + X8 + X9, data = dat.sim)

Residuals:
   Min     1Q Median     3Q    Max 
-7.818 -2.077  0.162  1.371 10.639 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   0.1072     0.3931   0.273  0.78584    
X34          -3.1401     0.3868  -8.117 4.10e-12 ***
X27           1.9382     0.3809   5.088 2.26e-06 ***
X29           0.5857     0.4051   1.446  0.15198    
X50          -0.6915     0.3859  -1.792  0.07679 .  
X23          -0.5109     0.3562  -1.434  0.15537    
X41          -1.1933     0.4098  -2.912  0.00463 ** 
X25           0.7984     0.4472   1.785  0.07790 .  
X36           0.6600     0.3486   1.893  0.06186 .  
X45           0.7560     0.3979   1.900  0.06091 .  
X21          -1.0932     0.4203  -2.601  0.01102 *  
X5            0.6229     0.3679   1.693  0.09425 .  
X17          -0.7021     0.4025  -1.744  0.08484 .  
X12          -0.8150     0.4439  -1.836  0.06997 .  
X6            0.5831     0.3835   1.521  0.13217    
X48           0.6162     0.3868   1.593  0.11503    
X8           -0.5647     0.3906  -1.446  0.15208    
X9            0.4483     0.3461   1.295  0.19888    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.516 on 82 degrees of freedom
Multiple R-squared:  0.6487,    Adjusted R-squared:  0.5759 
F-statistic: 8.909 on 17 and 82 DF,  p-value: 1.866e-12</code></pre>
</div>
</div>
<p>Hence, both of these procedures retain too many regressors in the final model. The stepwise selection method was also run, but returned results equivalent to forward selection.</p>
<p>Applying ridge regression to this dataset will result in all 50 of the estimated parameters being shrunk towards zero. The code and plot below demonstrate this behaviour. The vertical axis corresponds to the values of <span class="math inline">\(\beta_1,\ldots,\beta_{50}\)</span>. The horizontal axis corresponds to increasing values of the penalization parameter <span class="math inline">\(\lambda\)</span>. As <span class="math inline">\(\lambda\)</span> increases, the estimates for the <span class="math inline">\(\beta\)</span>’s tend towards zero. Hence we see all 50 of the curves bending towards the zero.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load in the glmnet package</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'glmnet' was built under R version 4.5.2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: Matrix</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded glmnet 4.1-10</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a ridge regression</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>md.ridge <span class="ot">=</span> <span class="fu">glmnet</span>( xx,yy,<span class="at">alpha=</span><span class="dv">0</span> )</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( md.ridge,<span class="at">las=</span><span class="dv">1</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="chpt3_ModBuild_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Applying LASSO to the dataset results in a different set of paths from the ridge regression. The plot below displays the LASSO paths. In this case, the horizontal axis corresponds to some <span class="math inline">\(K\)</span> such that <span class="math inline">\(\lVert\hat{\beta}^\text{L}_\lambda\rVert_1&lt;K\)</span>, which is equivalent to adding the penalty term <span class="math inline">\(\lambda\sum_{j=1}^p \lvert\beta_j\rvert\)</span>. As this bound <span class="math inline">\(K\)</span> grows, more variables will enter the model. The blue and green lines represent the regressors <span class="math inline">\(x_{34}\)</span> and <span class="math inline">\(x_{27}\)</span>, which are the first two terms to enter the model.<br>
Eventually, as the penalty is relaxed, many more terms begin to enter the model. Hence, choosing a suitable <span class="math inline">\(K\)</span>, or equivalently <span class="math inline">\(\lambda\)</span>, is a critical problem for this method.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a lasso regression</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>md.lasso <span class="ot">=</span> <span class="fu">glmnet</span>( xx,yy,<span class="at">alpha=</span><span class="dv">1</span> )</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( md.lasso,<span class="at">las=</span><span class="dv">1</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="chpt3_ModBuild_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="us-communities-and-crime-dataset" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="us-communities-and-crime-dataset"><span class="header-section-number">3.5</span> US Communities and Crime Dataset</h2>
<p>This section considers a dataset entitled “Communities and Crime”, which was downloaded from the UCI Machine Learning Repository.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> From the short description on the UCI website, <em>the data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR</em>. For our purposes, the dataset has 128 columns, but after removing those with missing values and some categorical variables, we have a dataset with <span class="math inline">\(p=99\)</span> predictors and a sample of <span class="math inline">\(n=1994\)</span> communities. The goal of the following regression models is to predict <code>ViolentCrimesPerPop</code>, which is the total number of violent crimes per 100,000 people.</p>
<p>If we fit a standard linear regression model, we see many significant predictor variables, but also have a large problem with mulitcolinearity as can be seen by computing the VIF values for each input to the model. We see some inputs with very low vif values and others with very high vif values. The three highest and lowest are summarized in the following table.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 10%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>VIF</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>indianPerCap</td>
<td>1.17</td>
<td>per capita income for native americans</td>
</tr>
<tr class="even">
<td>AsianPerCap</td>
<td>1.56</td>
<td>per capita income for people with asian heritage</td>
</tr>
<tr class="odd">
<td>LemasPctOfficDrugUn</td>
<td>1.58</td>
<td>percent of officers assigned to drug units</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>PctPersOwnOccup</td>
<td>568.98</td>
<td>percent of people in owner occupied households</td>
</tr>
<tr class="even">
<td>OwnOccMedVal</td>
<td>577.52</td>
<td>owner occupied housing - median value</td>
</tr>
<tr class="odd">
<td>TotalPctDiv</td>
<td>1037.33</td>
<td>percentage of population who are divorced</td>
</tr>
</tbody>
</table>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load car package for vif()</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'car' was built under R version 4.5.2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: carData</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'carData' was built under R version 4.5.2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read in data</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>dat.cc <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/crimeDat.csv"</span>)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fit OLS model</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>md.ols <span class="ot">&lt;-</span> <span class="fu">lm</span>( ViolentCrimesPerPop<span class="sc">~</span>., <span class="at">data=</span>dat.cc )</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># compute vif values</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">vif</span>(md.ols))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         indianPerCap           AsianPerCap   LemasPctOfficDrugUn 
             1.170277              1.561501              1.578085 
         pctWFarmSelf        PctWOFullPlumb            HispPerCap 
             1.931995              1.994314              2.162989 
          blackPerCap      PctVacantBoarded MedOwnCostPctIncNoMtg 
             2.175619              2.485693              2.542223 
            NumStreet        PctVacMore6Mos              MedNumBR 
             2.559441              2.573746              2.812721 
       PctUsePubTrans              LandArea     MedRentPctHousInc 
             3.204282              3.300431              3.478678 
         PctHousOccup               PopDens            pctWRetire 
             4.126627              4.348160              4.354717 
     MedOwnCostPctInc           PctEmplManu         NumInShelters 
             4.757836              4.816572              4.966596 
       MedYrHousBuilt              NumImmig              pctUrban 
             5.180590              5.284101              5.529765 
      PctEmplProfServ          racePctAsian         PctSameCity85 
             5.877700              5.886797              6.667340 
  PctWorkMomYoungKids           PctTeen2Par         PctUnemployed 
             7.177116              7.629905              7.768671 
       PctSameState85      PctBornSameState        PctHousNoPhone 
             8.194955              8.262952              8.332114 
       PctImmigRecent            PctWorkMom        PctHousLess3BR 
             9.238551             10.180738             11.702095 
          pctWPubAsst        PctSameHouse85      PctYoungKids2Par 
            11.964125             12.469516             12.727784 
         PctOccupManu              PctIlleg            HousVacant 
            13.635199             13.658336             13.797805 
        PctImmigRec10              NumIlleg        MalePctNevMarr 
            15.470409             15.910242             16.239444 
           pctWInvInc           racePctHisp          racepctblack 
            16.550890             17.666785             19.219132 
            PctEmploy          PctImmigRec5         householdsize 
            21.581210             22.424146             22.825206 
       PctPopUnderPov          racePctWhite       PctLess9thGrade 
            23.469586             23.518855             23.945792 
             RentLowQ   PctNotSpeakEnglWell    PersPerRentOccHous 
            24.716495             25.768707             26.795712 
         PctImmigRec8      PctPersDenseHous      PctSpeakEnglOnly 
            27.654191             28.838007             29.127245 
     PctOccupMgmtProf           PctBSorMore              pctWWage 
            29.586770             30.012558             30.517782 
          agePct12t21           NumUnderPov            agePct65up 
            30.826378             35.694002             39.295561 
           pctWSocSec          PctNotHSGrad        PctForeignBorn 
            39.545215             43.166972             49.297286 
            RentHighQ           agePct12t29            PersPerFam 
            52.265807             57.632357             77.639751 
    PersPerOwnOccHous           agePct16t24               MedRent 
            80.189980             85.484471             87.214942 
          whitePerCap        PctRecentImmig             medFamInc 
            92.712971             94.964943            115.656168 
          PctKids2Par            PctFam2Par            RentMedian 
           117.310433            118.913596            122.773952 
            perCapInc             medIncome         OwnOccHiQuart 
           148.700025            149.478200            171.793014 
     PersPerOccupHous       PctLargHouseFam     PctLargHouseOccup 
           205.076410            225.969285            233.094839 
       MalePctDivorce        OwnOccLowQuart             numbUrban 
           233.733489            237.789803            281.558920 
           population         PctRecImmig10          PctRecImmig5 
           290.456635            301.610537            312.448675 
         FemalePctDiv          PctRecImmig8         PctHousOwnOcc 
           336.318105            478.170602            550.685258 
      PctPersOwnOccup          OwnOccMedVal           TotalPctDiv 
           569.984167            577.523898           1037.327141 </code></pre>
</div>
</div>
<section id="variable-selection-1" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="variable-selection-1"><span class="header-section-number">3.5.1</span> Variable Selection</h3>
<p>We can try to fix this problem of multicolinearity by using forwards or backwards variable selection, which will remove terms from our model if removing those terms improves the AIC. Note that with <span class="math inline">\(p=99\)</span>, there are <span class="math inline">\(2^{99} \approx 6.3\times 10^{29}\)</span> possible regression models to consider, which is much too many to exhaustively search through.</p>
<p>Backwards variable selection finished with <span class="math inline">\(p=53\)</span> inputs remaining in the model with the largest VIF now at 202.03. Forwards variable selection finished with <span class="math inline">\(p=37\)</span> inputs in the model with the largest VIF only at 88.14.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Backwards Variable Selection</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>md.back <span class="ot">&lt;-</span> <span class="fu">step</span>(md.ols,<span class="at">trace=</span><span class="dv">0</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">vif</span>(md.back))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         indianPerCap   LemasPctOfficDrugUn          pctWFarmSelf 
             1.141616              1.496937              1.769121 
             pctUrban            HispPerCap        PctVacMore6Mos 
             1.893758              2.025846              2.198193 
MedOwnCostPctIncNoMtg      PctVacantBoarded             NumStreet 
             2.212207              2.296319              2.445900 
       PctUsePubTrans              MedNumBR            PctWorkMom 
             2.613058              2.685736              2.767703 
    MedRentPctHousInc          PctHousOccup            pctWRetire 
             2.960188              3.032502              3.534648 
             NumImmig      MedOwnCostPctInc           PctEmplManu 
             3.607630              3.787314              4.110466 
        NumInShelters            HousVacant       PctLess9thGrade 
             4.679666              5.940583              7.018809 
         racepctblack              NumIlleg           racePctHisp 
             7.165975              7.234419              8.506237 
       PctHousLess3BR        MalePctNevMarr              PctIlleg 
            10.236929             11.029312             11.405678 
          agePct12t29          PctOccupManu        PctForeignBorn 
            11.603578             11.846101             11.995279 
     PctOccupMgmtProf             PctEmploy            pctWInvInc 
            12.075506             12.579787             13.266811 
       PctPopUnderPov     PctLargHouseOccup              RentLowQ 
            15.050398             16.063152             16.758133 
           pctWSocSec   PctNotSpeakEnglWell    PersPerRentOccHous 
            19.202997             19.642101             21.943281 
          whitePerCap      PctPersDenseHous              pctWWage 
            22.255426             22.628408             22.755048 
          PctKids2Par        MalePctDivorce      PersPerOccupHous 
            27.162936             32.780138             33.099839 
            medFamInc           TotalPctDiv             RentHighQ 
            39.285294             40.667122             43.859596 
              MedRent        OwnOccLowQuart          OwnOccMedVal 
            48.267116            154.292084            156.122860 
        PctHousOwnOcc       PctPersOwnOccup 
           173.515817            202.028631 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Forwards Variable Selection</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>md<span class="fl">.0</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>( ViolentCrimesPerPop<span class="sc">~</span><span class="dv">1</span>, <span class="at">data=</span>dat.cc )</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>md.forw <span class="ot">&lt;-</span> <span class="fu">step</span>(</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>  md<span class="fl">.0</span>,<span class="at">trace=</span><span class="dv">0</span>,<span class="at">direction=</span><span class="st">"forward"</span>,</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower=</span>md<span class="fl">.0</span>,<span class="at">upper=</span>md.ols)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">vif</span>(md.forw))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         indianPerCap   LemasPctOfficDrugUn           AsianPerCap 
             1.124833              1.457492              1.470653 
          PctEmplManu          pctWFarmSelf MedOwnCostPctIncNoMtg 
             1.642768              1.699459              1.817426 
            NumStreet              pctUrban            HispPerCap 
             1.853679              1.890667              1.991065 
       PctVacMore6Mos      PctVacantBoarded            PctWorkMom 
             2.014917              2.147307              2.361651 
    MedRentPctHousInc      MedOwnCostPctInc            pctWRetire 
             2.697115              2.986114              3.097814 
      PctLess9thGrade            HousVacant       PctLargHouseFam 
             4.955276              6.893231              6.942874 
          agePct12t21        MalePctNevMarr             numbUrban 
             6.954019              7.616776              7.719742 
             pctWWage              PctIlleg            pctWInvInc 
             9.862223              9.937225             10.821914 
            PctEmploy          racepctblack        PctPopUnderPov 
            11.118442             11.519250             11.860735 
     PctPersDenseHous          racePctWhite           agePct12t29 
            12.485013             13.838907             14.560154 
             RentLowQ               MedRent           PctKids2Par 
            15.681439             17.993276             24.552975 
       MalePctDivorce           TotalPctDiv           whitePerCap 
            28.494673             36.357173             68.199024 
            perCapInc 
            88.142344 </code></pre>
</div>
</div>
</section>
<section id="penalized-regression" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="penalized-regression"><span class="header-section-number">3.5.2</span> Penalized Regression</h3>
<p>Another approach to this data is to use penalized regression methods discussed about like LASSO and Ridge Regression. In the plots below, from left to right, we see <span class="math inline">\(\lambda\)</span> going to zero or <span class="math inline">\(-\log\lambda\)</span> increasing. As this happens, more input variables enter the regression model and the value of these coefficients change.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load in the glmnet package</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a lasso model</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>md.la <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">x=</span>dat.cc[,<span class="sc">-</span><span class="dv">100</span>], <span class="at">y=</span>dat.cc[,<span class="dv">100</span>],<span class="at">alpha=</span><span class="dv">1</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a ridge model</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>md.rd <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">x=</span>dat.cc[,<span class="sc">-</span><span class="dv">100</span>], <span class="at">y=</span>dat.cc[,<span class="dv">100</span>],<span class="at">alpha=</span><span class="dv">0</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="co"># fit an elastic net model</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>md.en <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">x=</span>dat.cc[,<span class="sc">-</span><span class="dv">100</span>], <span class="at">y=</span>dat.cc[,<span class="dv">100</span>],<span class="at">alpha=</span><span class="fl">0.5</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the variable paths</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(md.la,<span class="at">las=</span><span class="dv">1</span>,<span class="at">main=</span><span class="st">"Lasso"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="chpt3_ModBuild_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(md.rd,<span class="at">las=</span><span class="dv">1</span>,<span class="at">main=</span><span class="st">"Ridge"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="chpt3_ModBuild_files/figure-html/unnamed-chunk-14-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(md.en,<span class="at">las=</span><span class="dv">1</span>,<span class="at">main=</span><span class="st">"Elastic Net"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="chpt3_ModBuild_files/figure-html/unnamed-chunk-14-3.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>As an example, when <span class="math inline">\(-\log \lambda = 2\)</span>, the lasso model retains <span class="math inline">\(p=2\)</span> parameters whereas the elastic net model retains <span class="math inline">\(p=6\)</span> parameters. When <span class="math inline">\(-\log \lambda = 4\)</span>, the lasso model retains <span class="math inline">\(p=10\)</span> parameters whereas the elastic net model retains <span class="math inline">\(p=18\)</span> parameters. In ridge regression, all variables are included in the model, but the coefficient values are shrunken towards zero.</p>
<p>Lasso selects the two variables <code>PctIlleg</code> (percentage of kids born to never married) and <code>PctKids2Par</code> (percentage of kids in family housing with two parents) as the most important predictors when highly penalizing the model. When relaxing the penalization a bit, we get the following additional variables:</p>
<ul>
<li><code>racePctWhite</code>, percentage of population that is caucasian</li>
<li><code>pctUrban</code>, percentage of people living in areas classified as urban</li>
<li><code>MalePctDivorce</code>, percentage of males who are divorced</li>
<li><code>PctWorkMom</code>, percentage of moms of kids under 18 in labor force</li>
<li><code>PctPersDenseHous</code>, percent of persons in dense housing</li>
<li><code>HousVacant</code>, number of vacant households</li>
<li><code>PctVacantBoarded</code>, percent of vacant housing that is boarded up</li>
<li><code>NumStreet</code>, number of homeless people counted in the street</li>
</ul>
<div id="rem-causality" class="proof remark">
<p><span class="proof-title"><em>Remark 3.6</em>. </span>With this (and really any) dataset, it is worth emphasizing that correlation does not imply causation. If some of these inputs have strong predictive power on the output (total number of violent crimes per 100K popuation), it only implies that these variables are correlated and not that there is a direct causal link between the inputs and outputs.</p>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Look at the parameters when -log(Lambda) is approx 2 and 4</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>md.la<span class="sc">$</span>lambda[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">26</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.13011052 0.01680442</code></pre>
</div>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>md.la<span class="sc">$</span>beta[,<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">26</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>99 x 2 sparse Matrix of class "dgCMatrix"
                               s3          s25
population             .           .          
householdsize          .           .          
racepctblack           .           .          
racePctWhite           .          -0.177643427
racePctAsian           .           .          
racePctHisp            .           .          
agePct12t21            .           .          
agePct12t29            .           .          
agePct16t24            .           .          
agePct65up             .           .          
numbUrban              .           .          
pctUrban               .           0.002480816
medIncome              .           .          
pctWWage               .           .          
pctWFarmSelf           .           .          
pctWInvInc             .           .          
pctWSocSec             .           .          
pctWPubAsst            .           .          
pctWRetire             .           .          
medFamInc              .           .          
perCapInc              .           .          
whitePerCap            .           .          
blackPerCap            .           .          
indianPerCap           .           .          
AsianPerCap            .           .          
HispPerCap             .           .          
NumUnderPov            .           .          
PctPopUnderPov         .           .          
PctLess9thGrade        .           .          
PctNotHSGrad           .           .          
PctBSorMore            .           .          
PctUnemployed          .           .          
PctEmploy              .           .          
PctEmplManu            .           .          
PctEmplProfServ        .           .          
PctOccupManu           .           .          
PctOccupMgmtProf       .           .          
MalePctDivorce         .           0.081181639
MalePctNevMarr         .           .          
FemalePctDiv           .           .          
TotalPctDiv            .           .          
PersPerFam             .           .          
PctFam2Par             .           .          
PctKids2Par           -0.11118010 -0.300107242
PctYoungKids2Par       .           .          
PctTeen2Par            .           .          
PctWorkMomYoungKids    .           .          
PctWorkMom             .          -0.003477488
NumIlleg               .           .          
PctIlleg               0.09491414  0.195131076
NumImmig               .           .          
PctImmigRecent         .           .          
PctImmigRec5           .           .          
PctImmigRec8           .           .          
PctImmigRec10          .           .          
PctRecentImmig         .           .          
PctRecImmig5           .           .          
PctRecImmig8           .           .          
PctRecImmig10          .           .          
PctSpeakEnglOnly       .           .          
PctNotSpeakEnglWell    .           .          
PctLargHouseFam        .           .          
PctLargHouseOccup      .           .          
PersPerOccupHous       .           .          
PersPerOwnOccHous      .           .          
PersPerRentOccHous     .           .          
PctPersOwnOccup        .           .          
PctPersDenseHous       .           0.048592093
PctHousLess3BR         .           .          
MedNumBR               .           .          
HousVacant             .           0.133518706
PctHousOccup           .           .          
PctHousOwnOcc          .           .          
PctVacantBoarded       .           0.005459467
PctVacMore6Mos         .           .          
MedYrHousBuilt         .           .          
PctHousNoPhone         .           .          
PctWOFullPlumb         .           .          
OwnOccLowQuart         .           .          
OwnOccMedVal           .           .          
OwnOccHiQuart          .           .          
RentLowQ               .           .          
RentMedian             .           .          
RentHighQ              .           .          
MedRent                .           .          
MedRentPctHousInc      .           .          
MedOwnCostPctInc       .           .          
MedOwnCostPctIncNoMtg  .           .          
NumInShelters          .           .          
NumStreet              .           0.080640084
PctForeignBorn         .           .          
PctBornSameState       .           .          
PctSameHouse85         .           .          
PctSameCity85          .           .          
PctSameState85         .           .          
LandArea               .           .          
PopDens                .           .          
PctUsePubTrans         .           .          
LemasPctOfficDrugUn    .           .          </code></pre>
</div>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Look at the parameters when -log(Lambda) is approx 2 and 4</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>md.en<span class="sc">$</span>lambda[<span class="fu">c</span>(<span class="dv">12</span>,<span class="dv">33</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.12362608 0.01752368</code></pre>
</div>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>md.en<span class="sc">$</span>beta[,<span class="fu">c</span>(<span class="dv">12</span>,<span class="dv">33</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>99 x 2 sparse Matrix of class "dgCMatrix"
                               s11           s32
population             .            .           
householdsize          .            .           
racepctblack           .            0.0687186851
racePctWhite          -0.112296761 -0.1252486854
racePctAsian           .            .           
racePctHisp            .            .           
agePct12t21            .            .           
agePct12t29            .           -0.0027706673
agePct16t24            .            .           
agePct65up             .            .           
numbUrban              .            .           
pctUrban               .            0.0194212436
medIncome              .            .           
pctWWage               .            .           
pctWFarmSelf           .            .           
pctWInvInc             .            .           
pctWSocSec             .            .           
pctWPubAsst            .            .           
pctWRetire             .            .           
medFamInc              .            .           
perCapInc              .            .           
whitePerCap            .            .           
blackPerCap            .            .           
indianPerCap           .            .           
AsianPerCap            .            .           
HispPerCap             .            .           
NumUnderPov            .            .           
PctPopUnderPov         .            .           
PctLess9thGrade        .            .           
PctNotHSGrad           .            .           
PctBSorMore            .            .           
PctUnemployed          .            .           
PctEmploy              .            .           
PctEmplManu            .            .           
PctEmplProfServ        .            .           
PctOccupManu           .            .           
PctOccupMgmtProf       .            .           
MalePctDivorce         .            0.0982854492
MalePctNevMarr         .            .           
FemalePctDiv           .            .           
TotalPctDiv            .            0.0240915372
PersPerFam             .            .           
PctFam2Par            -0.084158719 -0.0306809156
PctKids2Par           -0.165607053 -0.2154189468
PctYoungKids2Par      -0.004186120 -0.0091135229
PctTeen2Par           -0.005214765  .           
PctWorkMomYoungKids    .            .           
PctWorkMom             .           -0.0396100426
NumIlleg               .            .           
PctIlleg               0.145972531  0.1837625182
NumImmig               .            .           
PctImmigRecent         .            .           
PctImmigRec5           .            .           
PctImmigRec8           .            .           
PctImmigRec10          .            .           
PctRecentImmig         .            .           
PctRecImmig5           .            .           
PctRecImmig8           .            .           
PctRecImmig10          .            .           
PctSpeakEnglOnly       .            .           
PctNotSpeakEnglWell    .            .           
PctLargHouseFam        .            .           
PctLargHouseOccup      .            .           
PersPerOccupHous       .            .           
PersPerOwnOccHous      .            .           
PersPerRentOccHous     .            .           
PctPersOwnOccup        .            .           
PctPersDenseHous       .            0.1017035006
PctHousLess3BR         .            .           
MedNumBR               .            .           
HousVacant             .            0.1093486938
PctHousOccup           .           -0.0328292382
PctHousOwnOcc          .            .           
PctVacantBoarded       .            0.0211353681
PctVacMore6Mos         .            .           
MedYrHousBuilt         .            .           
PctHousNoPhone         .            .           
PctWOFullPlumb         .            .           
OwnOccLowQuart         .            .           
OwnOccMedVal           .            .           
OwnOccHiQuart          .            .           
RentLowQ               .            .           
RentMedian             .            .           
RentHighQ              .            .           
MedRent                .            .           
MedRentPctHousInc      .            .           
MedOwnCostPctInc       .            .           
MedOwnCostPctIncNoMtg  .            .           
NumInShelters          .            .           
NumStreet              .            0.1308971121
PctForeignBorn         .            0.0009972388
PctBornSameState       .            .           
PctSameHouse85         .            .           
PctSameCity85          .            .           
PctSameState85         .            .           
LandArea               .            .           
PopDens                .            .           
PctUsePubTrans         .            .           
LemasPctOfficDrugUn    .            0.0125378275</code></pre>
</div>
</div>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Redmond M. Communities and Crime [dataset]. 2002. UCI Machine Learning Repository. Available from: https://doi.org/10.24432/C53W3X.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt2_ModAss.html" class="pagination-link" aria-label="Model Assumptions and Choices">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model Assumptions and Choices</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt4_AdvRegress.html" class="pagination-link" aria-label="Advanced Regression Methods">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Advanced Regression Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>